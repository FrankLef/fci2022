[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "",
    "text": "Preface\nThis is a study project of the book Fundamentals of Causal Inference With R by Babette A. Brumback. Ms Brumback uses base R for all the code. This study project is coding with the tidyverse way. The motivation stems from the opinion that, in practice, the code is as important as the theory and learning better coding practice should be started as early as possible."
  },
  {
    "objectID": "index.html#where-to-find",
    "href": "index.html#where-to-find",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Where-to-find",
    "text": "Where-to-find\n\nThe online version of this document can be found at FCI.\nThe companion package for this project is fciR and can be found at fciR.\nThe exercises can be found at FCI exercises. This repo could be set as a private to avoid frustrating the publisher."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany thanks to Babette A. Brumback for a book full of amazing observations, tricks and tips."
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Packages",
    "text": "Packages\nFor data processing and analysis the following packages are used\n\n\n\nPackage\nComment\n\n\n\n\nfciR\nCompanion R package for this book\n\n\nconflicted\nManage conflict resolution amongst packages\n\n\ntidyverse\nTidyverse is the favored coding way\n\n\nskimr\nSummary statistics\n\n\nmodelr\nCreate elegant pipelines when modelling\n\n\nassertthat\nEasy pre and post assertions\n\n\nsimpr\nGenerate simulated data\n\n\nrsample\nResampling and bootstraps\n\n\nMonteCarlo\nMonte Carlo simulation\n\n\ngeepack\nGeneralized estimating equations solver\n\n\nMatching\nMultivariate and Propensity Score Matching Software for Causal Inference. See chapter 10.\n\n\n\nFor plotting, graphs and tables these packages are used\n\n\n\nPackage\nComment\n\n\n\n\nggplot\nCreate graphics based on the grammar of graphics\n\n\nggdag\nCausal directed acyclic graphs\n\n\ntidygraph\nGraphs and networks manipulation\n\n\nggvenn\nVenn diagrams by ggplot2. Used in chap. 4\n\n\ngt\nNice-looking tables"
  },
  {
    "objectID": "part01.html",
    "href": "part01.html",
    "title": "Part I Basics",
    "section": "",
    "text": "This part covers the basics of probability theory, effect-measure and causal directed acyclic graphs"
  },
  {
    "objectID": "ch01_intro.html#a-brief-history",
    "href": "ch01_intro.html#a-brief-history",
    "title": "1  Introduction",
    "section": "1.1 A Brief History",
    "text": "1.1 A Brief History"
  },
  {
    "objectID": "ch01_intro.html#data-examples",
    "href": "ch01_intro.html#data-examples",
    "title": "1  Introduction",
    "section": "1.2 Data Examples",
    "text": "1.2 Data Examples\n\n1.2.1 Mortality Rates by Country\nThis dataset is available with fciR::mortality. The summary table is\n\ndata(\"mortality\", package = \"fciR\")\nmortality |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.1:  Mortality Rates by Age and Country \n  \n    \n    \n      T\n      H\n      deaths\n      population\n      Y\n    \n  \n  \n    TRUE\nFALSE\n756340\n282305227\n0.002679157\n    TRUE\nTRUE\n2152660\n48262955\n0.044602739\n    FALSE\nFALSE\n2923480\n1297258493\n0.002253583\n    FALSE\nTRUE\n7517520\n133015479\n0.056516129\n  \n  \n  \n\n\n\n\n\n\n\n1.2.2 National Center for Education Statistics\nThis dataset is available with fciR::nces.\nThe statistical summary is\n\ndata(\"nces\", package = \"fciR\")\nnces |&gt;\n  skimr::skim()\n\n\nTable 1.2: ?(caption)\n\n\n\n\n(a) Data summary\n\n\nName\nnces\n\n\nNumber of rows\n1217\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nselective\n0\n1\n0.20\n0.40\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nfemale\n0\n1\n0.44\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▆\n\n\nhighmathsat\n0\n1\n0.21\n0.41\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\n\n\n\n\n\nand the frequency table is\n\nnces |&gt;\n  dplyr::count(selective, female, highmathsat) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.3:  NCES Data \n  \n    \n    \n      selective\n      female\n      highmathsat\n      n\n    \n  \n  \n    0\n0\n0\n435\n    0\n0\n1\n87\n    0\n1\n0\n420\n    0\n1\n1\n37\n    1\n0\n0\n50\n    1\n0\n1\n104\n    1\n1\n0\n55\n    1\n1\n1\n29\n  \n  \n  \n\n\n\n\n\n\n\n1.2.3 Reducing Alcohol Consumption\n\n1.2.3.1 The What-If? Study\nThis dataset is available with fciR::whatifdat.\nThe statistical summary is\n\ndata(\"whatifdat\", package = \"fciR\")\nwhatifdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\nwhatifdat\n\n\nNumber of rows\n165\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nT\n0\n1\n0.48\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nA\n0\n1\n0.64\n0.48\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nH\n0\n1\n0.36\n0.48\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nY\n0\n1\n0.32\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\n\n\n\nThe frequency table is\n\nwhatifdat |&gt;\n  dplyr::count(`T`, A, H, Y) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.4:  The What-If? Study \n  \n    \n    \n      T\n      A\n      H\n      Y\n      n\n    \n  \n  \n    0\n0\n0\n0\n15\n    0\n0\n0\n1\n3\n    0\n0\n1\n0\n3\n    0\n0\n1\n1\n11\n    0\n1\n0\n0\n36\n    0\n1\n0\n1\n4\n    0\n1\n1\n0\n4\n    0\n1\n1\n1\n9\n    1\n0\n0\n0\n15\n    1\n0\n0\n1\n3\n    1\n0\n1\n0\n3\n    1\n0\n1\n1\n7\n    1\n1\n0\n0\n27\n    1\n1\n0\n1\n3\n    1\n1\n1\n0\n9\n    1\n1\n1\n1\n13\n  \n  \n  \n\n\n\n\n\n\n1.2.3.1.1 The Double What-If? Study\nThis dataset is available with fciR::doublewhatifdat.\nThe statistical summary is\n\ndata(\"doublewhatifdat\", package = \"fciR\")\ndoublewhatifdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ndoublewhatifdat\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAD0\n0\n1\n0.50\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nVL0\n0\n1\n0.62\n0.49\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nU\n0\n1\n0.49\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nT\n0\n1\n0.50\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nA\n0\n1\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nAD1\n0\n1\n0.28\n0.45\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\nVL1\n0\n1\n0.58\n0.49\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\nThe DAG for the Double What-If? study in the dagitty version is\n\nscm &lt;- list()\nscm &lt;- within(scm, {\n  the_nodes &lt;- c(\"U\" = \"Unmeasured, healthy behavior (U=1)\", \n                 \"AD0\" = \"Adherence time 0\", \n                 \"VL0\" = \"Viral Load time 0\", \n                 \"T\" = \"Naltrexone (T=1)\", \n                 \"A\" = \"Reduced drinking (A=1)\", \n                 \"AD1\" = \"Adherence time 1\", \n                 \"VL1\" = \"Viral Load time 1\")\n  coords &lt;- data.frame(\n    name = names(the_nodes),\n    x = c(2, 3, 4, 1, 2, 3, 4),\n    y = c(2, 2, 2, 1, 1, 1, 1)\n  )\n  dag &lt;- dagify(\n    AD0 ~ U,\n    VL0 ~ AD0,\n    A ~ `T` + U,\n    AD1 ~ A,\n    VL1 ~ AD0 + AD1 + U,\n  outcome = \"VL1\",\n  exposure = \"T\",\n  latent = \"U\",\n  coords = coords,\n  labels = the_nodes)\n  \n  # this is the only technique known to have a subscript in a DAG\n  # IMPORTANT: the expression must be exactly in alphabetical order\n  the_text_labels &lt;- c(\n    expression(bold(A)), expression(bold(AD[0])),expression(bold(AD[1])),\n    expression(bold(T)), expression(bold(U)), expression(bold(VL[0])), \n    expression(bold(VL[1])))\n  \n  # status' colors\n  colrs &lt;- c(\"latent\" = \"palevioletred\", \"exposure\" = \"mediumspringgreen\", \n             \"outcome\" = \"cornflowerblue\")\n  # plot the DAG\n  plot &lt;- dag |&gt; \n    tidy_dagitty() |&gt;\n    ggdag_status(color = status, text = FALSE) +\n    geom_dag_text(size = 5, color = \"white\", fontface = \"bold\",\n      parse = TRUE, label = the_text_labels) +\n    scale_color_manual(values = colrs, na.value = \"honeydew3\") +\n    scale_fill_manual(values = colrs, na.value = \"honeydew3\") +\n    ggdag::theme_dag_blank(panel.background = \n                             element_rect(fill=\"snow\", color=\"snow\")) +\n    theme(title = element_text(color = \"darkblue\"),\n          legend.position = \"bottom\",\n          legend.title = element_blank()) +\n    labs(title = \"The Double What-If? Study\")\n})\nscm$plot\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\nFigure 1.1: The Double What-If? Study\n\n\n\n\nand the code for doublewhatifsim.R is\n\n#' \\code{doublewhatifsim} script rewritten\n#' \n#' \\code{doublewhatifsim} script rewritten.\n#' \n#' Simulate the What-If study data.\n#'\n#' @param n Nb of observations.\n#' @param seed Integer, the seed used for random numbers.\n#'\n#' @return Dataframe\ndoublewhatifsim &lt;- function(n = 1000, seed = 444) {\n  \n  set.seed(seed)\n  \n  # variables each with probability 0.5\n  U &lt;- rbinom(n, size = 1, prob = 0.5)\n  # probability of AD0 depends on U\n  AD0prob &lt;- 0.2 + 0.6 * U\n  # generate independent bernoulli variables with varying probabilities\n  AD0 &lt;- rbinom(n, size = 1, prob = AD0prob)\n  VL0prob &lt;- 0.8 - 0.4 * AD0\n  VL0 &lt;- rbinom(n, size = 1, prob = VL0prob)\n  `T` &lt;- rbinom(n, size = 1, prob = 0.5)\n  Aprob &lt;- 0.05 + `T` * U * 0.8\n  A &lt;- rbinom(n, size = 1, prob = Aprob)\n  AD1prob &lt;- 0.1 + 0.8 * A\n  AD1 &lt;- rbinom(n, size = 1, prob = AD1prob)\n  VL1prob &lt;- VL0prob + 0.1 - 0.45 * AD1\n  VL1 &lt;- rbinom(n, size =1 , prob = VL1prob)\n  \n  data.frame(\n    \"AD0\" = AD0,\n    \"VL0\" = VL0,\n    \"T\" = `T`,\n    \"A\" = A,\n    \"AD1\" = AD1,\n    \"VL1\" = VL1\n  )\n}\n\n\n\n\n\n1.2.4 General Social Survey\nThis dataset is available with fciR::gss.\nThe statistical summary is\n\ndata(\"gss\", package = \"fciR\")\ngss |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ngss\n\n\nNumber of rows\n2348\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n0\n1.00\n49.12\n18.24\n18\n34\n48\n63\n99\n▇▇▇▅▁\n\n\ngt65\n0\n1.00\n0.22\n0.41\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nattend\n16\n0.99\n0.34\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\ngthsedu\n0\n1.00\n0.39\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nmagthsedu\n180\n0.92\n0.24\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\npagthsedu\n583\n0.75\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nfair\n798\n0.66\n0.49\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nowngun\n818\n0.65\n0.35\n0.48\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nconservative\n101\n0.96\n0.33\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\ntrump\n0\n1.00\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nwhite\n0\n1.00\n0.72\n0.45\n0\n0\n1\n1\n1\n▃▁▁▁▇\n\n\nfemale\n0\n1.00\n0.55\n0.50\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\n\n\n1.2.5 A Cancer Clinical Trial\nThis dataset is available with fciR::cogdat.\nThe statistical summary is\n\ndata(\"cogdat\", package = \"fciR\")\ncogdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ncogdat\n\n\nNumber of rows\n1190\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nA1\n0\n1\n0.47\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nH2\n0\n1\n0.38\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nA2\n0\n1\n0.13\n0.33\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nY\n0\n1\n0.26\n0.44\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\n\n\n\nand the frequency table is\n\ndf &lt;- cogdat |&gt;\n  filter(Y == 1) |&gt;\n  count(A1, H2, A2, name = \"nY\")\ncogdat |&gt;\n  count(A1, H2, A2) |&gt;\n  left_join(df) |&gt;\n  mutate(nY = dplyr::if_else(is.na(nY), 0, nY),\n         prop = round(nY / n, 2)) |&gt;\n  gt::gt()\n\nJoining with `by = join_by(A1, H2, A2)`\n\n\n\n\n\n\nTable 1.5:  A Hypothetical Cancer Clinical Trial \n  \n    \n    \n      A1\n      H2\n      A2\n      n\n      nY\n      prop\n    \n  \n  \n    0\n0\n0\n410\n120\n0.29\n    0\n0\n1\n30\n0\n0.00\n    0\n1\n0\n160\n30\n0.19\n    0\n1\n1\n30\n20\n0.67\n    1\n0\n0\n280\n30\n0.11\n    1\n0\n1\n20\n10\n0.50\n    1\n1\n0\n190\n80\n0.42\n    1\n1\n1\n70\n20\n0.29\n  \n  \n  \n\n\n\n\nrm(df)"
  },
  {
    "objectID": "ch01_intro.html#exercises",
    "href": "ch01_intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.3 Exercises",
    "text": "1.3 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch02_probability.html#conditional-probability",
    "href": "ch02_probability.html#conditional-probability",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.1 Conditional Probability",
    "text": "2.1 Conditional Probability\n\n2.1.1 Law of total probability\nIt is important to note that \\(\\sum_i{H_i} = H\\), that is \\(H\\) can be partitioned in \\(i\\) non-overlapping partitions.\nThen the law of total probabilities is\n\\[\n\\begin{align*}\nP(A) &= \\sum_i{P(A \\cap H_i)}= \\sum_i{P(A \\mid H_i) P(H_i)} \\\\\n&\\text{and we condition the whole expression with B} \\\\\nP(A \\mid B) &= \\sum_i{P(A \\cap H_i \\mid B)}= \\sum_i{P(A \\mid B, H_i) P(B,H_i)} \\\\\n\\end{align*}\n\\]\nand the multiplication rule is\n\\[\n\\begin{align*}\nP(A, B \\mid C) &= \\frac{P(A, B, C)}{P(C)} \\\\\n&= \\frac{P(A \\mid B, C) P(B, C)}{P(C)} \\\\\n&= \\frac{P(A \\mid B, C) P(B \\mid C) P(C)}{P(C)} \\\\\n&= P(A \\mid B, C) P(B \\mid C)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch02_probability.html#conditional-expectation-and-the-law-of-total-expectation",
    "href": "ch02_probability.html#conditional-expectation-and-the-law-of-total-expectation",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.2 Conditional Expectation and the Law of Total expectation",
    "text": "2.2 Conditional Expectation and the Law of Total expectation\nThe conditional expectation is defined as\n\\[\nE(Y \\mid T) = \\sum_y y P(Y=y \\mid T)\n\\]\nand one way that helps me usually understand it well is that conditioning is the same as filtering the data.\nFor example the conditional expectation of mortality of US resident at the beginning of 2019 \\(E(Y \\mid T = 1) = 0.0088\\)\n\ndata(\"mortality_long\", package = \"fciR\")\nmortality_long |&gt;\n  # condition on T = 1\n  filter(`T` == 1) |&gt;\n  # compute probabilities\n  mutate(prob = n / sum(n)) |&gt;\n  # compute expectation for each possible value of Y\n  group_by(Y) |&gt;\n  summarize(EYT1 = sum(Y * prob)) |&gt;\n  # output results in a named vector\n  pull() |&gt;\n  setNames(nm = c(\"EY0T1\", \"EY1T1\"))\n\n EY0T1  EY1T1 \n0.0000 0.0088 \n\n\nwhere \\(E(Y=0 \\mid T=1) = 0\\) because when \\(Y=0 \\implies 0 \\cdot P(Y=0) = 0\\) and since \\(Y\\) is binary \\(E(Y=1 \\mid T=1) = P(Y=1 \\mid T=1)\\).\n\nAnalogous to the law of total probability is the law of ttal expectation, also called double expectation theorem.\n\nThis law is used extensively in this textbook.\n\\[\n\\begin{align*}\nE(Y \\mid T) &= E_{H \\mid T}(E(Y \\mid H, T)) \\\\\n&= \\sum_h \\left[ \\sum_y y P(Y=y \\mid H=h, T) \\right] P(H=h \\mid T)\n\\end{align*}\n\\]\nAlso the following equivalence is used very often in this book.\n\\[\n\\begin{align*}\nE(H \\mid T) = \\sum_h h P(H=h \\mid T) = E_{H \\mid T} (H)\n\\end{align*}\n\\]\n\n2.2.1 Mean independence and conditional mean independence\n\nThe random variable \\(Y\\) is mean independent of \\(T\\) if\n\n\\[\nE(Y \\mid T) = E(Y)\n\\]\n\nand is conditionally mean independent of \\(T\\) given \\(H\\) is\n\n\\[\nE(Y \\mid T, H) = E(Y \\mid H)\n\\]\nand the conditional uncorrelation and uncorrelation are\n\\[\nE(YT \\mid H) = E(Y \\mid H)E(T \\mid H) \\implies \\text{conditionally uncorrelated} \\\\\nE(YT) = E(Y)E(T) \\implies \\text{uncorrelated}\n\\]\n\nIt happens that conditional mean independence implies conditional uncorrelation, but not the other way around.\n\nWe prove it as follows\n\\[\n\\begin{align*}\n\\text{assume Y is conditionally independent of T given H then} \\\\\nE(Y \\mid T, H) &= E(Y \\mid H) \\\\ \\\\\n\\text{using double expectation theorem} \\\\\nE(TY \\mid H) &= E_{T \\mid H}(E(TY \\mid H, T)) \\\\\n\\text{expectation is a linear operator} \\\\\n&= E_{T \\mid H}(TE(Y \\mid H, T)) \\\\\n\\text{by conditional mean independence from above} \\\\\n&= E_{T \\mid H}(TE(Y \\mid H)) \\\\\n\\text{expectation is a linear operator} \\\\\n&= E_{T \\mid H}(T)E_{T \\mid H}(E(Y \\mid H)) \\\\\n\\text{which proves the conditional uncorrelation} \\\\\n&= E(T \\mid H)E(Y \\mid H)\n\\end{align*}\n\\]\n\n\n2.2.2 Regression model\n\nA statistical model for a conditional expectation is called a regression model.\n\nFor a binary dataset the regression model is said to be saturated or nonparametric because the 4 proportions, or coefficients, cover all possibilities.\n\\[\nE(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T + \\beta_3 H * T\n\\]\nWhen unsaturated or parametric the model makes an assumption. For example the following model assumes no interaction.\n\\[\nE(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T\n\\]\n\n\n2.2.3 Nonlinear parametric models\nThe three parametric models, also the most well-known, used in the book are\n\\[\n\\begin{align*}\n\\text{linear: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p \\\\\n\\text{loglinear: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\exp{(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)} \\\\\n\\text{logistic: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\text{expit}(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)\n\\end{align*}\n\\]\nThe function expit() used by the author is actually the same as gtools::inv.logit(), boot::inv.logit() or stats::plogis(). In this project we use stats::plogis() to minimize dependencies since it is in base R."
  },
  {
    "objectID": "ch02_probability.html#estimation",
    "href": "ch02_probability.html#estimation",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.3 Estimation",
    "text": "2.3 Estimation\nThis section is extremely important as it is used extensively, especially from the chapter 6 on.\nLets use the What-if example to illustrate the mathematics of it. In that case, we have 3 covariates, \\(T\\) for naltrexone, \\(A\\) for reduced drinking and \\(H\\) for unsuppressed viral load. In the data set we have 165 observations, therefore \\(i=165\\).\nLet \\(X_i\\) denote the collection of \\(X_{ij}\\) for \\(j=1, \\ldots, p\\) where \\(p\\) is the number of covariates which is 3 in the What-if dataset. \\(X_i\\) is a horizontal vector.\nSo for the What-if study\n\\[\nX_i = \\begin{bmatrix}A_i & T_i & H_1 \\end{bmatrix}\n\\]\nand \\(\\beta\\) is a vertical vector.\n\\[\n\\beta = \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix}\n\\]\ntherefore\n\\[\n\\begin{align*}\nX_i \\beta &= \\begin{bmatrix}A_i & T_i & H_1 \\end{bmatrix} \\times \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} \\\\\n&= \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i\n\\end{align*}\n\\]\nwe also have\n\\[\n\\begin{align*}\nX_i^T (Y_i - X_i \\beta) &= \\begin{bmatrix}A_i \\\\ T_i \\\\ H_1 \\end{bmatrix}(Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\n&= \\begin{bmatrix}\nA_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\nT_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\nH_1 (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i)\n\\end{bmatrix}\n\\end{align*}\n\\]\nAlso, we can estimate the unconditional expectation of the binary outcome \\(Y\\) as\n\\[\n\\hat{E}(Y) = \\frac{1}{n} \\sum_{i=1}^n Y_i\n\\]\nwhich returns the proportion of participants with unsuppressed viral load after 4 months\n\n#label: ch02_whatifdat\ndata(\"whatifdat\", package = \"fciR\")\nmean(whatifdat$Y)\n\n[1] 0.3212121\n\n\nThis is an unbiased estimator for \\(E(Y)\\) because \\(E(\\hat{E}(Y)) = E(Y)\\)\nNow let the saturated model for \\(E(Y)\\) be\n\\[\nE(Y) = \\beta\n\\]\nwith the following estimating equation\n\\[\nU(\\beta) = \\sum_{i=1}^n (Y_i - \\beta) = 0\n\\]\nand simple algebra, using the estimate of \\(\\hat{E}(Y)\\) from above gives \\(\\beta = \\hat{E}(Y)\\) and we use the notation \\(\\hat{E}(Y) = E(Y \\mid X, \\beta)\\) to show its dependency on the data and \\(\\beta\\).\n\\[\n\\begin{align*}\nU(\\beta) &= \\sum_{i=1}^n (Y_i - \\beta) \\\\\n&= \\sum_{i=1}^n \\left[ Y_i - E(Y \\mid X, \\beta) \\right] \\\\\n&= 0\n\\end{align*}\n\\]\nand to do the actual calculations we use a variation of the estimating equation as follows\n\\[\n\\begin{align*}\nU(\\beta) = \\sum_{i=1}^n X_i^T \\left[ Y_i - E(Y \\mid X, \\beta) \\right] = 0\n\\end{align*}\n\\]\nAs an example, we fit the logistic regression model with the What-if dataset\n\\[\nE(Y \\mid A,T,H) = expit(\\beta_0 + \\beta_A A + \\beta_T T + \\beta_H H)\n\\]\n\nwhatif.mod &lt;- glm(Y ~ A + `T` + H, family = \"binomial\", data = whatifdat)\nsummary(whatif.mod)\n\n\nCall:\nglm(formula = Y ~ A + T + H, family = \"binomial\", data = whatifdat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.5234     0.4125  -3.693 0.000222 ***\nA            -0.5647     0.4214  -1.340 0.180248    \nT            -0.2254     0.4147  -0.543 0.586790    \nH             2.7438     0.4158   6.600 4.12e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.17  on 164  degrees of freedom\nResidual deviance: 150.83  on 161  degrees of freedom\nAIC: 158.83\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe coefficients, \\(\\beta_0, \\beta_A, \\beta_T, \\beta_H\\) are obtained with coef() which is an alias for coefficient(). The example in the book uses lmod$coef which is not a recommended coding practice. The data should be obtained with an extractor function such as coef().\n\ncoef(whatif.mod)\n\n(Intercept)           A           T           H \n -1.5233530  -0.5647068  -0.2254112   2.7438245 \n\n\nand since the model is binomial then the inverse function to convert from the logit scale to the natural scale is stats::plogis().\nTherefore the parametric estimate is\n\\[\nE(Y \\mid A=1, T=1, H=1) = \\beta_0 + \\beta_A \\cdot 1 + \\beta_T \\cdot 1 + \\beta_H \\cdot 1\n\\]\n\nstats::plogis(sum(coef(whatif.mod)))\n\n[1] 0.6059581\n\n\nwe can also use the linear model directly, thus avoiding a call to plogis and making the code a little more robust. This technique is used quite often in the book, especially starting from chapter 6 on.\n\nwhatif.newdata &lt;- whatifdat |&gt;\n  filter(A == 1, `T` == 1, H == 1)\nwhatif.EYT1 &lt;- predict(whatif.mod, newdata = whatif.newdata, \n                            type = \"response\") |&gt;\n  mean()\nwhatif.EYT1\n\n[1] 0.6059581\n\n\nand we can validate the result directly using the dataset\n\nwhatifdat.est &lt;- whatifdat |&gt;\n  filter(A == 1, `T` == 1, H == 1) |&gt;\n  pull(Y) |&gt;\n  mean()\nwhatifdat.est\n\n[1] 0.5909091\n\n\nwhich is close to the parametric estimate."
  },
  {
    "objectID": "ch02_probability.html#sampling-distributions-and-the-bootstrap",
    "href": "ch02_probability.html#sampling-distributions-and-the-bootstrap",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.4 Sampling Distributions and the Bootstrap",
    "text": "2.4 Sampling Distributions and the Bootstrap\nThe previous section prived and estimate. But how variable is this estimator? One way would be to report the standard error \\(\\sigma_{\\hat{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Therefore, since \\(\\sigma^2 = p(1-p)\\) for a binomial distribution\n\n# the sample size\nwhatifdat.n &lt;- sum(whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H == 1)\n# whatifdat.n\n\n# the standard deviation\nwhatifdat.sd &lt;- sqrt(whatifdat.est * (1- whatifdat.est))\n# whatifdat.sd\n\n# the standard error\nwhatifdat.se &lt;- whatifdat.sd / sqrt(whatifdat.n)\nwhatifdat.se\n\n[1] 0.1048236\n\n\nNote that the formula used by B. Brumback is using the standard deviation of the population \\(p(1-p)\\) which, maybe, is not entirely the right way since the standard error of the mean is defined using the standard deviation of the sample.\nThat is Brumback uses\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) * sqrt(n-1) / n\n\n[1] 0.1048236\n\n\nwhen the correct definition is\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H == 1]\nsd(x) / sqrt(length(x))\n\n[1] 0.1072903\n\n\n\nAnother way would be by reporting the sampling distribution of our estimator.\n\n\nthe sampling distribution is centered at the true value of our estimand and it has a standard deviation equal to the true standard error of our estimator.\n\nThe section 2.4 will not be repeated here, yet it is very instructive and cover a topic that we end to forget. We will just repeat the calculations below. They\nThe sim() function in section 2.4, p. 31 is coded in fciR::sim_intervals() and its alias fciR::sim().\nRun and verify with author’s results on p. 32\n\nd &lt;- fciR::sim_intervals(nsim = 500, n = 500, seed = 1013)\nstopifnot(abs(d$bad - 0.8374) &lt; 0.01, abs(d$good - 0.948) &lt; 0.01)\nd\n\n$bad\n[1] 0.84\n\n$good\n[1] 0.946\n\n\n\n2.4.1 Bootstrapping\n\nAs we have seen, replacing \\(\\mu\\) and \\(\\sigma\\) with estimates leads to some problems, but we can still form an interpretable 95% confidence interval. In many situations, we do not have a good estimate of \\(\\sigma\\). In those cases, we often can turn to bootstrap.\n\nBootstrapping in base R is done with the boot from the boot package. Another option is the rsample package which isthe tidyverse way of doing it. Both methods will be used in this project. In the package fciR, the functions boot_run and boot_est use the classic R package boot. The functions bootidy_run and bootidy_est are similar but use the rsample package.\n\n2.4.1.1 boot package\nWe use boot::boot() for boostrapping and boot::boot.ci() to compute the confidence interval of the estimand using the logistic model as done by the lmodboot function in section 2.4. We only run 100 boot samples since it is only an example. The default value is 1000.\nThe first 3 arguments data, statistic and R are specific to boot, the ... is for extra arguments used by lmodboot. The details of lmdboots can be found in the package fciR or in its alias fciR::prob_lmod.\n\nprob_lmod(whatifdat, formula = Y ~ `T` + A + H)\n\n    term  estimate std.err\n1 logitP 0.4303535      NA\n\n\nand we show the details on how to do the bootstrapping with the boot package as follows.\n\n# define the function used by boot\nwhatifdat.fnc &lt;- function(data, ids, ...) {\n  dat &lt;- data[ids, ]\n  df &lt;- prob_lmod(data = dat, ...)\n  # create the named vector\n  out &lt;- c(df$estimate)\n  names(out) &lt;- df$term\n  out\n}\n# run the bootstrapping\nwhatifdat.boot &lt;- boot::boot(data = whatifdat, statistic = whatifdat.fnc, \n                             R = 100, formula = Y ~ `T` + A + H)\n# get the confidence interval for every term in the bootstrap object\nwhatifdat.out &lt;- lapply(X = seq_along(whatifdat.boot$t0), FUN = function(i, alpha = 0.05) {\n  est &lt;- whatifdat.boot$t0[i]\n  # the method used to find the interval\n  the_method &lt;- \"norm\"\n  # extract the interval from the boot object\n  ci &lt;- boot::boot.ci(whatifdat.boot, conf = 1 - alpha, type = the_method, \n                      index = i)$normal\n  # the dataframe of results, follow the format from rsample package\n  data.frame(\"term\" = names(est),\n    \".lower\" = plogis(ci[2]),\n    \".estimate\" = plogis(unname(est)),\n    \".upper\" = plogis(ci[3]),\n    \".alpha\" = alpha,\n    \".method\" = the_method)\n  })\n# bind the data.frame in one.\n# In this case it makes no difference as there is only one.\n# and we just want to show how it works in case of several items\nwhatifdat.out &lt;- do.call(rbind, whatifdat.out)\nwhatifdat.out\n\n    term    .lower .estimate    .upper .alpha .method\n1 logitP 0.4023362 0.6059581 0.7694102   0.05    norm\n\n\n\n\n2.4.1.2 rsample package\nAnother way package that could be used for boostrapping is rsample::bootstrap which uses the tidyverse way of R programming.\nTo be fully tidyverse-like we recode the prob_lmod function from above and call it prob_lmod_td where the td suffix stands for tidyverse. Note the use of a formula as an argument to the function and the rlang package which is a great tool to learn for quality code in R.\n\nprob_lmod_td &lt;- function(data, formula = Y ~ `T` + A + H,\n                         condition.names = NULL) {\n  # independent variables from the formula\n  f_vars &lt;- all.vars(rlang::f_rhs(formula))\n  # if condition.names is NULL then use all independent variables\n  # which is the same as saying there is no condition\n  if (is.null(condition.names)) condition.names &lt;- f_vars\n  stopifnot(all(condition.names %in% f_vars))\n\n  # add intercept to conditions\n  x0 &lt;- \"(Intercept)\"  # name of intercept used by lm, glm, etc.\n  condition.names &lt;- c(x0, condition.names)\n  \n  \n  fit &lt;- glm(formula = formula, family = \"binomial\", data = data) |&gt;\n    tidy()\n  fit |&gt;\n    filter(term %in% condition.names) |&gt;\n    summarize(term = \"logitP\",\n              estimate = sum(estimate),\n              # don't know the std.err so no t-intervals\n              std.err = NA_real_)\n}\n\n\nprob_lmod_td(whatifdat, formula = Y ~ `T` + A + H)\n\n# A tibble: 1 × 3\n  term   estimate std.err\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 logitP    0.430      NA\n\n\nThe following function is inspired from rsample bootstrap.\nWe obtain the confidence interval using rsample::int_pctl which uses a percentile interval instead of assuming a normal distribution as Ms Brumback does. The result are not materially different in the current case.\n\nwhatifdat |&gt;\n  rsample::bootstraps(times = 1000, apparent = FALSE) |&gt;\n  mutate(results = map(splits, function(x) {\n    dat &lt;- analysis(x)\n    prob_lmod_td(dat, formula = Y ~ `T` + A + H)})) |&gt;\n  rsample::int_pctl(statistics = results, alpha = 0.05) |&gt;\n  mutate(term = \"P\",\n         .lower = plogis(.lower),\n         .estimate = plogis(.estimate),\n         .upper = plogis(.upper))\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 P      0.440     0.610  0.771   0.05 percentile\n\n\nand lmodboot() is fciR::boot_lmod() with the alias fciR::lmodboot()\nFinally we run fciR::boot_lmod() and verify against the author’s results on p.34\n\nfciR::boot_est(whatifdat, func = fciR::prob_lmod, times = 500,\n                 alpha = 0.05, seed = 123, transf = \"expit\", terms = NULL,\n                 formula = Y ~ `T` + A + H)\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 P      0.427     0.608  0.750   0.05 percentile"
  },
  {
    "objectID": "ch02_probability.html#exercises",
    "href": "ch02_probability.html#exercises",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch03_outcomes.html#potential-outcomes-and-the-consistency-assumption",
    "href": "ch03_outcomes.html#potential-outcomes-and-the-consistency-assumption",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.1 Potential Outcomes and the Consistency Assumption",
    "text": "3.1 Potential Outcomes and the Consistency Assumption\n\nThe utility of the potential outcome framework hinges on the validity of a consistency assumption that links potential outcome to obe=served outcomes.\n\n\nIt is worth emphasizing that the consistency assumption requires the potential outcomes to be well-defined.\n\n\nA major problem for causal inference is that we can only observe one potential outcome per participant. This is the Fundamental Problem of Causal Inference (FPCI).\n\nWe can therefore classify 4 causal types\n\\[\n\\text{Doomed:} \\: Y(0) = Y(1) = 0 \\\\\n\\text{Responsive:} \\: Y(0) = 0,  Y(1) = 1 \\\\\n\\text{Harmed:} \\: Y(0) = 1,  Y(1) = 0 \\\\\n\\text{Immune:} \\: Y(0) = Y(1) = 1 \\\\\n\\]\nIf we consider that the treatment cannot be harmful, in that case a patient with \\(Y=0\\) must be doomed and an untreated patient with \\(Y=0\\) must be immune.\n\\[\n\\text{Assuming the treatment cannot be harmful} \\\\ \\\\\n\\text{Doomed:} \\: Y(0) = Y(1) = 0 \\\\\n\\text{Responsive:} \\: Y(0) = 0,  Y(1) = 1 \\\\\n\\text{Immune:} \\: Y(0) = 1 \\\\\n\\]"
  },
  {
    "objectID": "ch03_outcomes.html#circumventing-the-fundamental-problem-of-causal-inference",
    "href": "ch03_outcomes.html#circumventing-the-fundamental-problem-of-causal-inference",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.2 Circumventing the Fundamental Problem of Causal Inference",
    "text": "3.2 Circumventing the Fundamental Problem of Causal Inference\nThere are 2 solutions to the FPCI\n\nScientific solution: We use scientific theory to measure both potential outcomes. For example by using 2 different participants that are considered identical for the purpose of the scientific study.\nStatistical solution: The treatment is randomized independently of any existing data. As a result we obtain mean independence and the mean can be used to draw inference. This approach has some issues\n\nThe difference between the outcome with or without treatment can still be caused by chance.\nThe study populaiton may not be relevant anymore.\nExtrapolating an average result to an individdual might be problematic."
  },
  {
    "objectID": "ch03_outcomes.html#effect-measures",
    "href": "ch03_outcomes.html#effect-measures",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.3 Effect Measures",
    "text": "3.3 Effect Measures\nTo estimate the association measures we use the parametric method with the glm function.\n\nWe certainly do not believe that our binary outcomes the distributional assumtptions for the gaussian and poisson models, but we are only using the glm function to solve estimating equations.\n\n\ndata(\"gss\", package = \"fciR\")\ngssrcc &lt;- gss[, c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")]\ngssrcc &lt;- gss[complete.cases(gssrcc), ]\nstopifnot(nrow(gssrcc) == 2348 - 180)  # see comment on p. 46\n\n\nuncond &lt;- fciR::boot_est(gssrcc, func = fciR::meas_effect_uncond, times = 500,\n                 alpha = 0.05, transf = \"exp\",\n                 terms = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"), \n                 formula = trump ~ gthsedu)\n\n\ngt_measures(uncond, \n            title = \"Table 3.2\", \n            subtitle = paste(\"4 Association Measures Relating\", \n            \"&lt;em&gt;More than High School Education&lt;/em&gt; to &lt;em&gt;Voting for Trump&lt;/em&gt;\", \n            sep = \"&lt;br&gt;\"))\n\n\n\n\n\n  \n    \n      Table 3.2\n    \n    \n      4 Association Measures RelatingMore than High School Education to Voting for Trump\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    P0\n0.234\n(0.213, 0.259)\n    P1\n0.272\n(0.239, 0.302)\n    RD\n0.038\n(0.001, 0.077)\n    RR\n1.162\n(1.005, 1.358)\n    RR*\n1.052\n(1.002, 1.109)\n    OR\n1.222\n(1.007, 1.504)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nverify with author’s results, p. 48-49\n\nbb &lt;- data.frame(\n  term = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n  .estimate = c(0.23338, 0.27117, 0.037782, 1.1619, 1.0518, 1.221),\n  .lower = c(0.21030, 0.24095, 0.00078085, 1.0047, 1.0006, 1.0056),\n  .upper = c(0.25647, 0.30139, 0.07478354, 1.3437, 1.1057, 1.4853))\nstopifnot(\n  all((uncond$.estimate - bb$.estimate) &lt; 0.01),\n  all((uncond$.lower - bb$.lower) &lt; 0.1),\n  all((uncond$.upper - bb$.upper) &lt; 0.1))\n\nNow we estimate the four effect measures using conditional association measures.\nTo use the function with conditioning on variables, i.e. filtering on variables, we note that\n\\[\n\\begin{align*}\nY &= \\text{trump} \\\\\nT &= \\text{gthsedu} \\\\\nH_1 &= \\text{magthsedu} \\\\\nH_2 &= \\text{white} \\\\\nH_3 &= \\text{female} \\\\\nH_4 &= \\text{gt65} \\\\\n\\end{align*}\n\\]\nTherefore\n\\[\n\\begin{align*}\nE(Y &\\mid T = 0, H_1 = 1, H_2 = 1, H_3 = 1, H_4 = 1) \\\\\n&\\therefore \\text{because T = 0} \\\\\nY &\\sim H_1 + H_2 + H_3 + H_4 \\\\\n\\text{trump} &\\sim \\text{magthsedu} + \\text{white} + \\text{female} + \\text{gt65} \\\\\n\\end{align*}\n\\]\n\na_formula &lt;- trump ~ gthsedu + magthsedu + white + female + gt65\ncondit &lt;- fciR::boot_est(gssrcc, func = fciR::meas_effect_cond,\n                 times = 500, alpha = 0.05, seed = 1234, transf = \"exp\",\n                 terms = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n                 formula = a_formula, exposure.name = \"gthsedu\",\n                 confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"))\n\n\ngt_measures(condit, \n            title = \"Table 3.3\", \n            subtitle = paste(\"4 Conditional Association or Effect Measures\",\n            \"Relating &lt;em&gt;More than High School Education&lt;/em&gt; \n            to &lt;em&gt;Voting for Trump&lt;/em&gt;\", sep = \"&lt;br&gt;\"))\n\n\n\n\n\n  \n    \n      Table 3.3\n    \n    \n      4 Conditional Association or Effect MeasuresRelating More than High School Education \n            to Voting for Trump\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    P0\n0.257\n(0.191, 0.332)\n    P1\n0.302\n(0.234, 0.372)\n    RD\n0.045\n(0.006, 0.092)\n    RR\n1.180\n(1.023, 1.425)\n    RR*\n1.065\n(1.008, 1.138)\n    OR\n1.257\n(1.031, 1.618)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nverify with author’s results, p.52\n\nbb &lt;- data.frame(\n  term = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n  .estimate = c(0.25458, 0.30101, 0.046438, 1.1824, 1.0664, 1.261),\n  .lower = c(0.18518, 0.22995, 0.0022706, 1.0039, 1.0018, 1.0091),\n  .upper = c(0.32397, 0.37208, 0.0906057, 1.3927, 1.1352, 1.5758))\nstopifnot(\n  all(abs(condit$.estimate - bb$.estimate) &lt; 0.01),\n  all(abs(condit$.lower - bb$.lower) &lt; 0.1),\n  all(abs(condit$.upper - bb$.upper) &lt; 0.1))"
  },
  {
    "objectID": "ch03_outcomes.html#exercises",
    "href": "ch03_outcomes.html#exercises",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch04_measures.html#effect-measure-modification-and-statistical-interaction",
    "href": "ch04_measures.html#effect-measure-modification-and-statistical-interaction",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.1 Effect-Measure Modification and Statistical Interaction",
    "text": "4.1 Effect-Measure Modification and Statistical Interaction\n\n4.1.1 RECOVERY trial\nGet the data.frame for RECOVERY trial\n\ndata(\"recovery\", package = \"fciR\")\n\nrun fciR::boot(), alias meas_effect_modif() with the RECOVERY data set\n\nthe_terms &lt;- c(\n  \"EYT0.M0\", \"EYT0.M1\", \"EYT0.diff\", \"EYT1.M0\", \"EYT1.M1\", \"EYT1.diff\", \n  \"RD.M0\", \"RD.M1\", \"RD.diff\", \"RR.M0\", \"RR.M1\", \"RR.diff\",\n  \"RR*.M0\", \"RR*.M1\", \"RR*.diff\", \"OR.diff\", \"OR.M0\", \"OR.M1\")\nrecovery.out &lt;- fciR::boot_est(data = recovery, func = meas_effect_modif,\n                         times = 100, alpha = 0.05, \n                         terms = the_terms, transf = \"exp\",\n                         formula = Y ~ `T` + M, exposure.name = \"T\",\n                         modifier.name = \"M\")\nrecovery.out\n\n# A tibble: 18 × 6\n   term       .lower .estimate  .upper .alpha .method   \n   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1 EYT0.M0    0.770     0.783   0.797    0.05 percentile\n 2 EYT0.M1    0.553     0.593   0.624    0.05 percentile\n 3 EYT0.diff -0.226    -0.191  -0.155    0.05 percentile\n 4 EYT1.M0    0.776     0.794   0.812    0.05 percentile\n 5 EYT1.M1    0.691     0.731   0.777    0.05 percentile\n 6 EYT1.diff -0.114    -0.0624 -0.0246   0.05 percentile\n 7 RD.M0     -0.0142    0.0101  0.0316   0.05 percentile\n 8 RD.M1      0.0858    0.138   0.193    0.05 percentile\n 9 RD.diff    0.0692    0.128   0.188    0.05 percentile\n10 RR.M0      0.982     1.01    1.04     0.05 percentile\n11 RR.M1      1.14      1.23    1.34     0.05 percentile\n12 RR.diff    1.12      1.22    1.32     0.05 percentile\n13 RR*.M0     0.937     1.05    1.16     0.05 percentile\n14 RR*.M1     1.29      1.52    1.86     0.05 percentile\n15 RR*.diff   1.19      1.45    1.78     0.05 percentile\n16 OR.diff    1.33      1.76    2.38     0.05 percentile\n17 OR.M0      0.921     1.06    1.21     0.05 percentile\n18 OR.M1      1.46      1.87    2.47     0.05 percentile\n\n\nverify the results with the author’s on p. 65.\n\nbb &lt;- data.frame(\n  term = c(\"EYT0\", \"EYT0\", \"EYT1\", \"EYT1\", \"RD\", \"RD\",\n           \"EYT0\", \"EYT1\", \"RD\", \"RR\", \"RR\", \"RR\",\n           \"RR*\", \"RR*\", \"RR*\", \"OR\", \"OR\", \"OR\"),\n  group = c(\"M0\", \"M1\", \"M0\", \"M1\", \"M0\", \"M1\",\n           \"diff\", \"diff\", \"diff\", \"M0\", \"M1\", \"diff\",\n           \"M0\", \"M1\", \"diff\", \"M0\", \"M1\", \"diff\"),\n  estimate = c(0.784, 0.593, 0.793, 0.735, 0.01, 0.142,\n          -0.191, -0.059, 0.132, 1.012, 1.239, 1.224,\n          1.046, 1.533, 1.466, 1.059, 1.9, 1.794)\n  )\nrecovery.out$term\n\n [1] \"EYT0.M0\"   \"EYT0.M1\"   \"EYT0.diff\" \"EYT1.M0\"   \"EYT1.M1\"   \"EYT1.diff\"\n [7] \"RD.M0\"     \"RD.M1\"     \"RD.diff\"   \"RR.M0\"     \"RR.M1\"     \"RR.diff\"  \n[13] \"RR*.M0\"    \"RR*.M1\"    \"RR*.diff\"  \"OR.diff\"   \"OR.M0\"     \"OR.M1\"    \n\nids &lt;- match(paste(bb$term, bb$group, sep = \".\"), recovery.out$term)\n# ids\ncomp &lt;- data.frame(bb = bb,\n                   d = recovery.out$.estimate[ids])\n# comp\ncomp$dev &lt;- abs(comp$bb.estimate - comp$d)\n# comp$dev\nstopifnot(all(comp$dev &lt; 0.1))\n\nand we communicate the results in a table\n\n# reformat to use for table and plot\nrecovery.out &lt;- recovery.out %&gt;%\n  separate(col = \"term\", into = c(\"term\", \"group\"), sep = \"[.]\", remove = TRUE)\nfciR::gt_measures_colgrp(recovery.out, var_grp = \"group\",\n                   title = \"Table 4.2 RECOVERY Trial\",\n                   subtitle = \"Effect-measure Modification\")\n\n\n\n\n\n  \n    \n      Table 4.2 RECOVERY Trial\n    \n    \n      Effect-measure Modification\n    \n    \n      Estimator\n      \n        M0\n      \n      \n        M1\n      \n      \n        diff\n      \n    \n    \n      Estimate\n      CI1\n      Estimate\n      CI1\n      Estimate\n      CI1\n    \n  \n  \n    EYT0\n0.783\n(0.770, 0.797)\n0.593\n(0.553, 0.624)\n-0.191\n(-0.226, -0.155)\n    EYT1\n0.794\n(0.776, 0.812)\n0.731\n(0.691, 0.777)\n-0.062\n(-0.114, -0.025)\n    RD\n0.010\n(-0.014, 0.032)\n0.138\n(0.086, 0.193)\n0.128\n(0.069, 0.188)\n    RR\n1.013\n(0.982, 1.041)\n1.233\n(1.141, 1.343)\n1.218\n(1.116, 1.323)\n    RR*\n1.049\n(0.937, 1.164)\n1.519\n(1.289, 1.856)\n1.447\n(1.188, 1.778)\n    OR\n1.063\n(0.921, 1.212)\n1.873\n(1.465, 2.470)\n1.762\n(1.327, 2.384)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nplotting the results makes it easier to see the measures vary among the strata. We can clearly see here significant difference in effect measures between the 2 strata.\nIt supports the observation in the text concerning the lack of effect of dexamethasone without intrusive mechanical ventilation (M0) vs its use with intrusive mechanical ventilation (M1) which is significant.\n\nfciR::ggp_measures_modif(recovery.out, title = \"RECOVERY trial\")\n\n\n\n\nThe gee::gee() function is used to find information on the coefficients and see if they are statistically significant. The same could be done withe glm::glm() but gee offers results with robust statistics which is very useful in practical terms.\nLuckily, the gee() works exactly like the glm() functions, with the same extractor functions coefficients(), effects(), etc. See the documentation of lm with ?lm for more details.\nWe are going through an example just below to illustrate how tthe extractor functions are used which is not shown in the textbook.\n\nlinmod &lt;- gee::gee(Y ~ `T` + M + `T` * M,\n                   id = id,\n                   data = recovery,\n                   family = gaussian)\n\nBeginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n\n\nrunning glm to get initial regression estimate\n\n\n(Intercept)           T           M         T:M \n 0.78367235  0.00958608 -0.19070017  0.13200964 \n\nsummary(linmod)\n\n\n GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n gee S-function, version 4.13 modified 98/01/27 (1998) \n\nModel:\n Link:                      Identity \n Variance to Mean Relation: Gaussian \n Correlation Structure:     Independent \n\nCall:\ngee::gee(formula = Y ~ T + M + T * M, id = id, data = recovery, \n    family = gaussian)\n\nSummary of Residuals:\n       Min         1Q     Median         3Q        Max \n-0.7932584  0.2067416  0.2163277  0.2163277  0.4070278 \n\n\nCoefficients:\n               Estimate  Naive S.E.     Naive z Robust S.E.    Robust z\n(Intercept)  0.78367235 0.006975704 112.3431253 0.006826403 114.8001855\nT            0.00958608 0.012170195   0.7876686 0.011778554   0.8138588\nM           -0.19070017 0.017545664 -10.8687918 0.019999408  -9.5352906\nT:M          0.13200964 0.030881745   4.2746820 0.033074076   3.9913327\n\nEstimated Scale Parameter:  0.1770267\nNumber of Iterations:  1\n\nWorking Correlation\n     [,1]\n[1,]    1\n\n\nTo extract the coefficients from the gee object we use the extractor function coefficients() or its alias coef()\n\ncoef(linmod)\n\n(Intercept)           T           M         T:M \n 0.78367235  0.00958608 -0.19070017  0.13200964 \n\n\nand to extract the entire coefficient data to work with it, just use coefficients() with summary()\n\ncoef(summary(linmod))\n\n               Estimate  Naive S.E.     Naive z Robust S.E.    Robust z\n(Intercept)  0.78367235 0.006975704 112.3431253 0.006826403 114.8001855\nT            0.00958608 0.012170195   0.7876686 0.011778554   0.8138588\nM           -0.19070017 0.017545664 -10.8687918 0.019999408  -9.5352906\nT:M          0.13200964 0.030881745   4.2746820 0.033074076   3.9913327\n\n\nand in this case we are concerned about how significant the interaction is. Therefore the Robust z is extracted with coefficients() alias coef()\n\ncoef(summary(linmod))[, \"Robust z\"]\n\n(Intercept)           T           M         T:M \n114.8001855   0.8138588  -9.5352906   3.9913327 \n\n\nwe see that \\(T:M\\) is 3.99 standard deviations away from zero which will give us the 2-sided p-value that is significant\n\nz &lt;- coef(summary(linmod))[\"T:M\", \"Robust z\"]\n2 * (1 - pnorm(z))\n\n[1] 6.570304e-05\n\n\n\n\n4.1.2 NCES\nWe process the NCES data the same way we did for the RECOVERY trial.\nRun the bootsrap\n\nnces.out &lt;- fciR::boot_est(data = nces, func = meas_effect_modif,\n                     times = 100, alpha = 0.05, transf = \"exp\",\n                     terms = the_terms,\n                     formula = highmathsat ~ female + selective, \n                     exposure.name = \"female\", modifier.name = \"selective\")\n\nverify the results with the author’s on p. 70.\n\nbb &lt;- data.frame(\n  term = c(\"EYT0\", \"EYT0\", \"EYT1\", \"EYT1\", \"RD\", \"RD\",\n           \"EYT0\", \"EYT1\", \"RD\", \"RR\", \"RR\", \"RR\",\n           \"RR*\", \"RR*\", \"RR*\", \"OR\", \"OR\", \"OR\"),\n  group = c(\"M0\", \"M1\", \"M0\", \"M1\", \"M0\", \"M1\",\n           \"diff\", \"diff\", \"diff\", \"M0\", \"M1\", \"diff\",\n           \"M0\", \"M1\", \"diff\", \"M0\", \"M1\", \"diff\"),\n  estimate = c(0.167, 0.675, 0.081, 0.345, -0.086, -0.33,\n          0.509, 0.264, -0.244, 0.486, 0.511, 1.052,\n          0.907, 0.496, 0.547, 0.44, 0.254, 0.576)\n  )\nids &lt;- match(paste(bb$term, bb$group, sep = \".\"), nces.out$term)\ncomp &lt;- data.frame(bb = bb, d = nces.out$.estimate[ids])\ncomp$dev &lt;- abs(comp$bb.estimate - comp$d)\n# comp$dev\nstopifnot(all(comp$dev &lt; 0.1))\n\nand the table is\n\n# reformat to use for table and plot\nnces.out &lt;- nces.out %&gt;%\n  separate(col = \"term\", into = c(\"term\", \"group\"), sep = \"[.]\", remove = TRUE)\nfciR::gt_measures_colgrp(nces.out, var_grp = \"group\",\n                   title = \"Table 4.3 NCES data\",\n                   subtitle = \"Effect-measure Modification\")\n\n\n\n\n\n  \n    \n      Table 4.3 NCES data\n    \n    \n      Effect-measure Modification\n    \n    \n      Estimator\n      \n        M0\n      \n      \n        M1\n      \n      \n        diff\n      \n    \n    \n      Estimate\n      CI1\n      Estimate\n      CI1\n      Estimate\n      CI1\n    \n  \n  \n    EYT0\n0.165\n(0.137, 0.198)\n0.673\n(0.596, 0.744)\n0.508\n(0.428, 0.577)\n    EYT1\n0.080\n(0.056, 0.106)\n0.355\n(0.265, 0.453)\n0.275\n(0.181, 0.364)\n    RD\n-0.084\n(-0.120, -0.053)\n-0.318\n(-0.430, -0.199)\n-0.234\n(-0.365, -0.124)\n    RR\n0.484\n(0.345, 0.657)\n0.523\n(0.396, 0.699)\n1.081\n(0.681, 1.572)\n    RR*\n0.908\n(0.872, 0.941)\n0.506\n(0.405, 0.630)\n0.557\n(0.445, 0.692)\n    OR\n0.440\n(0.305, 0.619)\n0.265\n(0.159, 0.441)\n0.602\n(0.295, 0.986)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nand we plot the results\n\nfciR::ggp_measures_modif(nces.out, title = \"NCES data\")\n\n\n\n\nWe observe that\n\nRD: Risk difference shows that using selection to accept more women seem to decrease the % of school with hogh math SAT\nOR and RRstar: Show the same results as RD\nRR: Indicates that selection, in relative terms has no significant effect"
  },
  {
    "objectID": "ch04_measures.html#qualitative-agreement-of-effect-measures-in-modification",
    "href": "ch04_measures.html#qualitative-agreement-of-effect-measures-in-modification",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.2 Qualitative Agreement of Effect Measures in Modification",
    "text": "4.2 Qualitative Agreement of Effect Measures in Modification\nThis section relies heavily on the paper from Shannin and Brumback (2021) Jake Shannin (2021). It used a Monte-Carlo simulation in java by jake running 1000000 times for six effect measures (the 4 in this chapter, the hazard ratio HR and the recovery ration HR*).\nFor the purpose of this project we only simulate the 4 effect measures discussed so far (RD, RR, RR* and OR). We use R package MonteCarlo with 5000 repetitions. The distribution used for simulation is the beta distribution which is generally used for values in [0,1]. It is also used as a prior of binomial regression in Bayes analysis which is the subject covered a little later in this section. Regardless if the 6 effects measures from Jake Shannin (2021) or the 4 from Brumback (2022) are used, the process and conclusion are the same.\nWe point out that using the distribution \\(Beta(1, 1) \\sim Uniform(0, 1)\\) is equivalent to running a grid search. It is also equivalent to the uniform distribution used in Jake Shannin (2021).\n\n4.2.1 Simulate the effect measures\nWe run the Monte-Carlo simulation without constraint and with \\(Beta(1,1)\\) which is equivalent to \\(Uniform(0,1)\\) used by Jake Shannin (2021).\n\ngridsim &lt;- fciR::mc_beta_effect_measures(shape1 = 1, shape2 = 1, nrep = 5000)\n\nGrid of  1  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n# gridsim\n\nwhich gives the vector of percentages\n\nunlist(gridsim)\n\n             RD_RR          RD_RRstar              RD_OR          RR_RRstar \n            0.0000             0.0000             0.0000             0.0000 \n             RR_OR          RRstar_OR RD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR \n            0.0000             0.0000             0.0258             0.0246 \nRD_OR_vs_RR_RRstar       RD_RR_RRstar           RD_RR_OR       RD_RRstar_OR \n            0.0000             0.0000             0.0534             0.0518 \n      RR_RRstar_OR    RD_RR_RRstar_OR               NONE \n            0.0000             0.8444             0.0000 \n\n\nFor the following discussion, we must note the following about the vector of percentages returned by the simulation.\n\nPairwise events: Some measures move as 1 pair in the same direction while all the other pairs move in different direction between each other. These are 6 possibilities named RD_RR,RD_RRstar, RD_OR, RR_RRstar, RR_OR, RRstar_OR.\nOpposite pairwise events: Some measures move as 2 pairs but each of the 2 pairs does not move in the same direction. There are 3 possibilities, called RD_RR_vs_RRstar_OR, RD_RRstar_vs_RR_OR and RD_OR_vs_RR_RRstar. These are the problematic ones as they cannot be represented in the Venn diagram of section 4.2. However we can distribute them to ensure probabilities add up to 1. For example RD_RR_vs_RRstar_OR will be split 50% between pairwise events RD_RR and 50% to pairwise eventRRstar_OR. This enforces the very important rule that probabilities must add up to 1 without consequences on the conclusions reached.\n3-wise events: Some 3 measures move in the same direction together There are 4 possibilities called RD_RR_RRstar, RD_RR_OR, RD_RRstar_OR and RR_RRstar_OR.\nAll events: Sometimes all measures move together. This is the possibility of interest discussed by Shannin and Brumback (2021). This possibility is called ALL.\nNo event: The possibility NONE concerns the event that no pair of measures move in the same direction. It is impossible and represents the empty set \\(\\emptyset\\) which is one of the 3 conditions of a \\(\\sigma-field\\).\nValidation: The sum of the vector’s elements must be one.\n\nThe event definitions above ensure that the sample space is actually a \\(\\sigma-field\\). See Geoffrey R. Grimmet (2001), section 1.2.\nthen we compare with the author’s results\n\nbb &lt;- c(\"RD_RR\" = 0.026, \"RD_RRstar\" = 0.026, \"RD_OR\" = 0, \n        \"RR_RRstar\" = 0, \"RR_OR\" = 0.026, \"RRstar_OR\" = 0.026,\n        \"RD_RR_vs_RRstar_OR\" = 0,\n        \"RD_RRstar_vs_RR_OR\" = 0,\n        \"RD_OR_vs_RR_RRstar\" = 0,\n        \"RD_RR_RRstar\" = 0, \"RD_RR_OR\" = 0.057, \n        \"RD_RRstar_OR\" = 0.057, \"RR_RRstar_OR\" = 0,\n        \"RD_RR_RRstar_OR\" = 0.833, \"NONE\" = 0)\ncomp &lt;- data.frame(bb = round(bb, 4), sim = round(unlist(gridsim), 4))\ncomp\n\n                      bb    sim\nRD_RR              0.026 0.0000\nRD_RRstar          0.026 0.0000\nRD_OR              0.000 0.0000\nRR_RRstar          0.000 0.0000\nRR_OR              0.026 0.0000\nRRstar_OR          0.026 0.0000\nRD_RR_vs_RRstar_OR 0.000 0.0258\nRD_RRstar_vs_RR_OR 0.000 0.0246\nRD_OR_vs_RR_RRstar 0.000 0.0000\nRD_RR_RRstar       0.000 0.0000\nRD_RR_OR           0.057 0.0534\nRD_RRstar_OR       0.057 0.0518\nRR_RRstar_OR       0.000 0.0000\nRD_RR_RRstar_OR    0.833 0.8444\nNONE               0.000 0.0000\n\n\nThe results from the Monte Carlo simulation above confirm the main conclusion from Jake Shannin (2021) that all effect measures move together 84% of the time.\n\nc(\"simulation\" = unname(unlist(gridsim[\"RD_RR_RRstar_OR\"])), \n  \"author's\" = unname(bb[\"RD_RR_RRstar_OR\"]))\n\nsimulation   author's \n    0.8444     0.8330 \n\n\nWe note that the sim adds up correctly to 1 but not the author’s which adds up to 1.051. This is explained on p. 72 as a bit of arbitrary allocations. The Jake Shannin (2021) paper (caption figure 1) mentions that they do not add to 1 because they include events that are not mutually exclusive.\n\nc(\"simulation\" = sum(unlist(gridsim)), \"author's\" = sum(bb))\n\nsimulation   author's \n     1.000      1.051 \n\n\nActually, this is caused by the Opposite pairwise events which cannot be represented in the 4-set Venn diagram. For the purpose of this books we will simply split them between the events that makes them up without consequence on the conclusion.\nThe Opposite pairwise events are the following\n\n# The events that pair of measures move together but in opposite\n# direction of another pair who also move together\nunlist(gridsim[c(\"RD_RR_vs_RRstar_OR\", \"RD_RRstar_vs_RR_OR\", \"RD_OR_vs_RR_RRstar\")])\n\nRD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR RD_OR_vs_RR_RRstar \n            0.0258             0.0246             0.0000 \n\n\nand when we split them between their 2 sub-events we can generate the Venn diagram as follows\n\nggp_venn_sim(gridsim, n = 1000, \n         fill_colr = c(\"blue\", \"yellow\", \"green\", \"red\"),\n         title = \"Venn diagram of effect measure modifications\")\n\n\n\n\nnow for constrained data\n\ngridsim_const &lt;- fciR::mc_beta_effect_measures(shape1 = 1, shape2 = 1, nrep = 5000,\n                                constrained = TRUE)\n\nGrid of  1  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n\nwhich gives the vector of percentages\n\nunlist(gridsim_const)\n\n             RD_RR          RD_RRstar              RD_OR          RR_RRstar \n        0.00000000         0.00000000         0.00000000         0.00000000 \n             RR_OR          RRstar_OR RD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR \n        0.00000000         0.00000000         0.05117790         0.05199025 \nRD_OR_vs_RR_RRstar       RD_RR_RRstar           RD_RR_OR       RD_RRstar_OR \n        0.00000000         0.00000000         0.11291633         0.11129163 \n      RR_RRstar_OR    RD_RR_RRstar_OR               NONE \n        0.00000000         0.67262388         0.00000000 \n\n\nand comparing to the author’s results\n\nbb_const &lt;- c(\"RD_RR\" = 0.053, \"RD_RRstar\" = 0.053, \"RD_OR\" = 0.053, \n        \"RR_RRstar\" = 0, \"RR_OR\" = 0.053, \"RRstar_OR\" = 0.0,\n        \"RD_RR_vs_RRstar_OR\" = 0,\n        \"RD_RRstar_vs_RR_OR\" = 0,\n        \"RD_OR_vs_RR_RRstar\" = 0,\n        \"RD_RR_RRstar\" = 0, \"RD_RR_OR\" = 0.114, \n        \"RD_RRstar_OR\" = 0.114, \"RR_RRstar_OR\" = 0,\n        \"RD_RR_RRstar_OR\" = 0.667, \"NONE\" = 0)\nsum(bb_const)\n\n[1] 1.107\n\ncomp &lt;- data.frame(bb = round(bb_const, 4), \n                   sim = round(unlist(gridsim_const), 4))\ncomp\n\n                      bb    sim\nRD_RR              0.053 0.0000\nRD_RRstar          0.053 0.0000\nRD_OR              0.053 0.0000\nRR_RRstar          0.000 0.0000\nRR_OR              0.053 0.0000\nRRstar_OR          0.000 0.0000\nRD_RR_vs_RRstar_OR 0.000 0.0512\nRD_RRstar_vs_RR_OR 0.000 0.0520\nRD_OR_vs_RR_RRstar 0.000 0.0000\nRD_RR_RRstar       0.000 0.0000\nRD_RR_OR           0.114 0.1129\nRD_RRstar_OR       0.114 0.1113\nRR_RRstar_OR       0.000 0.0000\nRD_RR_RRstar_OR    0.667 0.6726\nNONE               0.000 0.0000\n\n\nThe results from the Monte Carlo simulation with the constrained data agree the author’s result.\n\nc(\"simulation\" = unname(unlist(gridsim_const[\"RD_RR_RRstar_OR\"])),\n  \"author's\" = unname(bb_const[\"RD_RR_RRstar_OR\"]))\n\nsimulation   author's \n 0.6726239  0.6670000 \n\n\nAgain the Opposite pairwise events cannot be represented in the 4-set Venn diagram.\n\n# The events that pair of measures move together but in opposite\n# direction of another pair who also move together\nunlist(gridsim_const[c(\"RD_RR_vs_RRstar_OR\", \"RD_RRstar_vs_RR_OR\", \"RD_OR_vs_RR_RRstar\")])\n\nRD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR RD_OR_vs_RR_RRstar \n        0.05117790         0.05199025         0.00000000 \n\n\nbut if we do it as mentioned above then we can show a Venn diagram as follows\n\n# This is a custom function using the venn package\nggp_venn_sim(gridsim_const,\n         fill_colr = c(\"cyan\", \"gold\", \"springgreen\", \"hotpink\"),\n         title = \"Venn diagram for constrained data\")\n\n\n\n\n\n\n4.2.2 Applications\n\n4.2.2.1 Simulation of distribution of effect measures\nWe use a parametric Monte Carlo simulation using the beta distribution to evaluate the effect of the distribution assumption on the effect-measure modifications.\nWe run the Monte-Carlo simulation with 5000 repetitions, a grid of shape1 and shape2 parameters for the Beta distribution and no constraint. Namely mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7), shape2 = c(0.5, 1, 3, 5, 7), nrep = 5000)\n\npriorsim &lt;- fciR::mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7),\n                                  shape2 = c(0.5, 1, 3, 5, 7),\n                                  nrep = 5000)\n\nGrid of  25  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===                                                                   |   4%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======                                                                |   8%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========                                                              |  12%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========                                                           |  16%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============                                                        |  20%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=================                                                     |  24%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================                                                  |  28%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================                                                |  32%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=========================                                             |  36%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |============================                                          |  40%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===============================                                       |  44%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================                                    |  48%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================================                                  |  52%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=======================================                               |  56%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==========================================                            |  60%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=============================================                         |  64%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================                      |  68%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================================                    |  72%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=====================================================                 |  76%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========================================================              |  80%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========================================================           |  84%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============================================================        |  88%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================================      |  92%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===================================================================   |  96%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n\nand we look at the matrix for the event RD_RR_RRstar_OR which represents the event that all effect measures move in the same direction.\nThe matrix elements correspond to the percentage frequency of the event given the beta distribution with shape parameters shape1 = s1 with s1 indicated as column names, and shape2 = s1 parameter with s2 indicated in the row names.\nThe beta distribution with shape1 = 1 and shape2 = 1 is similar to the uniform distribution on \\([0, 1]\\). Therefore this element is the one simulated by Shannon and Brumback, see Jake Shannin (2021) and, in fact, with 5000 repetitions nrep = 5000the measure is almost always very close to what is mentioned in Jake Shannin (2021).\nAlso note that for large \\(shape1 = shape2 = \\text{large number}\\) the beta distribution is similar to the normal distribution with a mean of \\(shape1 / (shape1 + shape2)\\). The simulation shows that in that case the percentage of measures moving in the same directions is nearing 100%. See for example when \\(shape1 = shape2 = 7\\) in the matrix of results from the sim.\nhere is the matrix of percentage of times that all measures move in the same direction, i.e. the event RD_RR_RRstar_OR\n\nround(priorsim[[\"RD_RR_RRstar_OR\"]], 2)\n\n       s1=0.5 s1=1 s1=3 s1=5 s1=7\ns2=0.5   0.78 0.81 0.83 0.83 0.83\ns2=1     0.80 0.84 0.85 0.87 0.87\ns2=3     0.82 0.86 0.90 0.91 0.92\ns2=5     0.83 0.87 0.92 0.92 0.93\ns2=7     0.83 0.86 0.91 0.93 0.93\n\n\nand to show as a heatmap\n\nggp_betasim(priorsim, var = \"RD_RR_RRstar_OR\", \n             colr = list(\"low\" = \"deepskyblue1\", \"high\" = \"deepskyblue4\"))\n\n\n\n\nand comparing with the author\n\nround(c(\"simulation\" = priorsim[[\"RD_RR_RRstar_OR\"]][\"s2=1\", \"s1=1\"], \n  \"author's\" = unname(bb[\"RD_RR_RRstar_OR\"])), 3)\n\nsimulation   author's \n     0.837      0.833 \n\n\nWe can see that the range is wide and should be considered.\n\nrange(priorsim[[\"RD_RR_RRstar_OR\"]])\n\n[1] 0.7848 0.9302\n\n\nNow we do it with constrained data\n\npriorsim_const &lt;- fciR::mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7),\n                                        shape2 = c(0.5, 1, 3, 5, 7),\n                                        nrep = 5000, constrained = TRUE)\n\nGrid of  25  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===                                                                   |   4%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======                                                                |   8%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========                                                              |  12%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========                                                           |  16%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============                                                        |  20%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=================                                                     |  24%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================                                                  |  28%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================                                                |  32%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=========================                                             |  36%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |============================                                          |  40%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===============================                                       |  44%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================                                    |  48%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================================                                  |  52%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=======================================                               |  56%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==========================================                            |  60%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=============================================                         |  64%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================                      |  68%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================================                    |  72%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=====================================================                 |  76%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========================================================              |  80%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========================================================           |  84%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============================================================        |  88%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================================      |  92%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===================================================================   |  96%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n\n\nggp_betasim(priorsim_const, var = \"RD_RR_RRstar_OR\",\n             colr = list(\"low\" = \"lightsalmon1\", \"high\" = \"lightsalmon4\"),\n             title = \"Monte Carlo simulation. Constrained data.\")\n\n\n\n\nand comparing with the author\n\nround(c(\"simulation\" = priorsim_const[[\"RD_RR_RRstar_OR\"]][\"s2=1\", \"s1=1\"], \n  \"author's\" = unname(bb_const[\"RD_RR_RRstar_OR\"])), 3)\n\nsimulation   author's \n     0.670      0.667 \n\n\nand the range is even larger and therefore more significant.\n\nrange(priorsim_const[[\"RD_RR_RRstar_OR\"]])\n\n[1] 0.5811623 0.8613429\n\n\n\n\n4.2.2.2 Application: Data pre-processing (data cleaning)\nUnless the \\(p_0, p_1\\) obtained are uniformly distributed (not the most common scenario for sure) it seems from the results above that the likelihood of not having all effect measures moving in the same direction is low. Thus, it could be a good hint to uncover hidden processes in a data pre-processing routine.\nWe note the important rule mentioned at the beginning of section 4.2\n\nwhen relative risk \\(RR\\) and other relative risk \\(RR^*\\) both change in the same direction […] then so must the difference the risk difference and odds ratio.\n\nAnother rule from section 4.2 can help in data cleaning\n\nWhen \\(RR_0\\) and \\(RR_1\\) are on opposite side of 1, that is, when in one stratum the treatment is helpful and in the other it is harmful, then all measures will automatically change together.\n\nThus a quick, easy way to clean up potential data problems and obtain relevant details on outliers and hidden processes might be\n\nExclude cases when \\(RR\\) and \\(RR^*\\) change in the same direction to reduce the data load.\nExclude cases when \\(RR_0\\) and \\(RR_1\\) are on opposite side of 1\nInvestigate the remining cases asthey are good candidates for hidden processes\n\n\n\n4.2.2.3 Application: Bayesian prior in Beta-binomial model\nIf we use information from the population, or from expert knowledge, that effect measures should move in the same direction then a beta distribution with \\(shape1 &gt; 1, shape2 &gt; 1\\) would make sense and provide a better prior for the Beta-binomial model.\nOn the contrary, if we wish that the model look into the unlikely events that effect measure do not move in the same directions, then a beta distribution with \\(shape1 \\leq 1, shape2 \\leq 1\\) could be supported by the results from above."
  },
  {
    "objectID": "ch04_measures.html#causal-interaction",
    "href": "ch04_measures.html#causal-interaction",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.3 Causal Interaction",
    "text": "4.3 Causal Interaction"
  },
  {
    "objectID": "ch04_measures.html#exercises",
    "href": "ch04_measures.html#exercises",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\nThe exercises are located in a separate project.\n\n\n\n\nBrumback, Babette A. 2022. Fundamentals of Causal Inference with r. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and Random Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP: Oxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "ch05_dag.html#theory",
    "href": "ch05_dag.html#theory",
    "title": "5  Causal Directed Acyclic Graphs",
    "section": "5.1 Theory",
    "text": "5.1 Theory\n\n5.1.1 Conditional Indepence and DAG\n\nCausal directed acyclic graphs provide a convenient and efficient way to represent statistical ans causal dependence.\n\nFor example\n\nscm_5.1 &lt;- list()\nscm_5.1 &lt;- within(scm_5.1, {\n  the_nodes &lt;- c(\"X1\" = \"\", \n                 \"X2\" = \"\", \n                 \"X3\" = \"\", \n                 \"X4\" = \"\")\n  dag &lt;- dagify(\n    X2 ~ X1,\n    X3 ~ X2,\n    X4 ~ X1 + X2,\n    labels = the_nodes)\n  \n  text_labels &lt;- c(expression(X[1]), expression(X[2]),\n                   expression(X[3]), expression(X[4]))\n  plot &lt;- fciR::ggp_dag(dag, text_labels = text_labels)\n})\n\n\n\n\n\n\nFigure 5.1: Example of a Directed Acyclic Graph\n\n\n\n\ncan be expressed in terms of conditional probabilities as\n\\[\n\\begin{align*}\nP(X_1, X_2, X_3, X_4) = P(X_4 \\mid X_2, X_1)P(X_3 \\mid X_2)P(X_2 \\mid X_1)P(X_1)\n\\end{align*}\n\\]\n\n\n5.1.2 D-Separation\n\nA path is said to be d-separated, or blocked,, by a set of variables \\(C\\) if and only if the path i) containis a chain as in figure 5.2a such that the midlle variable \\(Z\\) is in \\(C\\) or ii) contains a fork as in figure 5.2b such that such that the middle variable \\(Z\\) is not in \\(C\\) or iii) contains an inverted fork, or collider, as in figure 5.2c such that the midlle variable \\(Z\\) is not in \\(C\\) and such that no descendant of a collider is in \\(C\\).\n\nand we illustrate the 3 basic graphical structures as follows\n\nscm_5.2 &lt;- list()\nscm_5.2 &lt;- within(scm_5.2, {\n  \n  # CHAIN\n  dagChain &lt;- dagify(\n    Z ~ X1,\n    X2 ~ Z)\n  \n  text_labels &lt;- c(expression(X[1]), expression(X[2]), \"Z\")\n  plotChain &lt;- ggp_dag(dagChain, text_size = 5, text_labels = text_labels) +\n    theme(title = element_text(size = 10)) +\n    labs(title = \"Chain: Z is an intermediate variable\")\n\n\n  # FORK\n  dagFork &lt;- dagify(\n    X1 ~ Z,\n    X2 ~ Z)\n  \n  plotFork &lt;- ggp_dag(dagFork, text_size = 5, text_labels = text_labels) +\n    theme(title = element_text(size = 10)) +\n    labs(title = \"Fork: Z is a common cause\")\n\n  \n  # COLLIDER\n  dagColl &lt;- dagify(\n    Z ~ X1,\n    Z ~ X2)\n\n  plotColl &lt;- ggp_dag(dagColl, text_size = 5, text_labels = text_labels) +\n    theme(title = element_text(size = 10)) +\n    labs(title = \"Collider: Z is a common effect\")\n\n  \n  # DESCENDANT\n  dagDesc &lt;- dagify(\n    W ~ X1,\n    W ~ X2,\n    Z ~ W)\n\n  text_labels &lt;- c(expression(X[1]), expression(X[2]), expression(X[3]), \"Z\")\n  plotDesc &lt;- ggp_dag(dagDesc, text_size = 5, text_labels = text_labels) +\n    theme(title = element_text(size = 10)) +\n    labs(title = \"Descendant: Z is an effect of a common effect\")\n})\n\n\ngridExtra::grid.arrange(scm_5.2$plotChain, scm_5.2$plotFork, scm_5.2$plotColl, \n                        scm_5.2$plotDesc, \n                        nrow = 2, ncol = 2)\n\n\n\n\nFigure 5.2: Graphical structures\n\n\n\n\n\n\n5.1.3 Causal DAG with collider\nand for our first practice of d-separation we have a DAG with a collider\n\nscm_5.3 &lt;- list()\nscm_5.3 &lt;- within(scm_5.3, {\n  dag &lt;- dagify(\n    A ~ W1,\n    Y ~ W3,\n    W2 ~ W1 + W3,\n    W4 ~ W2)\n\n  text_labels &lt;- c(\"A\", expression(W[1]), expression(W[2]),\n                   expression(W[3]), expression(W[4], \"Y\"))\n  plot &lt;- fciR::ggp_dag(dag, text_labels = text_labels)\n})\n\n\n\n\n\n\nFigure 5.3: Causal DAG With Collider\n\n\n\n\nThe conditional independences can be obtained using impliedConditionalIndependencies from thedagitty package. You must ensure that you use the parameter type = \"all.pairs explicitly. The default is type = \"missing.edge. We will not list all of them, just their number, 93.\n\n# use type = \"all.pairs\" to get everything.\n# impliedConditionalIndependencies uses type \"missing.edge\" by default.\nlength(impliedConditionalIndependencies(scm_5.3$dag, type = \"all.pairs\"))\n\n[1] 93\n\n\nsome of those independence are\n\nimpliedConditionalIndependencies(scm_5.3$dag, type = \"missing.edge\")\n\nA _||_ W2 | W1\nA _||_ W3\nA _||_ W4 | W2\nA _||_ W4 | W1\nA _||_ Y\nW1 _||_ W3\nW1 _||_ W4 | W2\nW1 _||_ Y\nW2 _||_ Y | W3\nW3 _||_ W4 | W2\nW4 _||_ Y | W3\nW4 _||_ Y | W2"
  },
  {
    "objectID": "ch05_dag.html#examples",
    "href": "ch05_dag.html#examples",
    "title": "5  Causal Directed Acyclic Graphs",
    "section": "5.2 Examples",
    "text": "5.2 Examples\n\n5.2.1 Causal DAG With Intermediate Value\nHere we have \\(A \\not\\!\\perp\\!\\!\\!\\perp Y \\mid M\\) because \\(M\\) is a collider and conditioning on \\(M\\) would open up the path.\n\nscm_5.4 &lt;- list()\nscm_5.4 &lt;- within(scm_5.4, {\n  dag &lt;- dagify(\n    M ~ A + H,\n    Y ~ A + M + H)\n\n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\n\n\n\n\nFigure 5.4: Causal DAG With Intermediate Variable\n\n\n\n\n\n\n5.2.2 A counfounder may occur after the exposure\n\nFigure 5.5 illustrates and unmeasured true confounder \\(U\\) of the effect of \\(A\\) on \\(Y\\) that can be handled by conditioning on the measured variable \\(Z\\), even if \\(Z\\) occur after \\(A\\).\n\n\nscm_5.5 &lt;- list()\nscm_5.5 &lt;- within(scm_5.5, {\n\n  dag &lt;- dagify(\n    A ~ U,\n    Z ~ U,\n    Y ~ A + Z,\n    latent = \"U\",\n    exposure = \"A\",\n    outcome = \"Y\")\n  \n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\n\n\n\n\nFigure 5.5: Confounder May Occur After the Exposure\n\n\n\n\n\n\n5.2.3 Potential Outcomes are the Ultimate Counfounder\n\nThe collection of potential outcomes \\(\\{Y(a)\\}_{a \\in A}\\) can be viewed as the ultimate confounder, even if it is not a tue confounder. The unmeasured counfounder \\(U = \\{Y(a)\\}_{a \\in A}\\) will always block all backdoor paths from \\(A\\) to \\(Y\\), because \\(Y\\) is a deterministic function of \\(A\\) and \\(\\{Y(a)\\}_{a \\in A}\\); that is \\(Y = Y(A)\\), or, for binary \\(Y\\), \\(Y = AY(1) + (1-A)Y(0)\\)\n\n\nscm_5.6 &lt;- list()\nscm_5.6 &lt;- within(scm_5.6, {\n  the_nodes &lt;- c(\"A\" = \"\", \n                 \"Y\" = \"\", \n                 \"U\" = \"(Y(0), y(1))\")\n  dag &lt;- dagify(\n    A ~ U,\n    Y ~ A + U,\n    latent = \"U\",\n    labels = the_nodes)\n  \n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\n\n\n\n\nFigure 5.6: One way to generate confounding\n\n\n\n\nand lets do the simulation of figure 5.6 on p. 88 with simpr\n\nscm_5.6 &lt;- within(scm_5.6, {\n  set.seed(111)\n  data &lt;- simpr::specify(\n    Y0 = ~ rbinom(1000, size = 1, 0.42),\n    Y1 = ~ rbinom(1000, size = 1, 0.62),\n    probA = ~ (1-Y0) * (1- Y1) * 0.6307 + (1 - Y0) * Y1 * 0.4867 +\n      Y0 * (1 - Y1) * 0.4699 + Y0 * Y1 * 0.4263,\n    A = ~ rbinom(1000, size = 1, prob = probA),\n    # Y must depend on A, Y1, Y0 in this way\n    Y = ~ A * Y1 + (1 - A) * Y0) |&gt;\n    generate(1) |&gt;\n    unnest()\n  \n  # create the tabletbl &lt;- sim_5.6$data |&gt;\n  tbl &lt;- data |&gt;\n    group_by(A, Y0, Y1, Y) |&gt;\n    count(name = \"prob\") |&gt;\n    ungroup() |&gt;\n    mutate(prob = prob / sum(prob))\n  stopifnot(near(sum(tbl$prob), 1))\n\n  # format the tableau\n  summ &lt;- fciR::gt_probs(tbl, title = \"Table 5.1\",\n                   subtitle = \"Simulation Probabilities for `sim1.r`\")\n  \n})\n\nWarning: `cols` is now required when using `unnest()`.\nℹ Please use `cols = c(sim)`.\n\nscm_5.6$summ\n\n\n\n\n\nTable 5.1:  Simulation Probabilities for sim1.r \n  \n    \n      Table 5.1\n    \n    \n      Simulation Probabilities for `sim1.r`\n    \n    \n      A\n      Y0\n      Y1\n      Y\n      prob\n    \n  \n  \n    0\n0\n0\n0\n0.082\n    0\n0\n1\n0\n0.176\n    0\n1\n0\n1\n0.091\n    0\n1\n1\n1\n0.156\n    1\n0\n0\n0\n0.142\n    1\n0\n1\n1\n0.170\n    1\n1\n0\n0\n0.071\n    1\n1\n1\n1\n0.112\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n\n\n\n\n\nbut when compare with the author’s\n\nbb_5.6 &lt;- data.frame(\n  A = c(0, 0, 0, 0, 1, 1, 1, 1),\n  Y0 = c(0, 0, 1, 1, 0, 0, 1, 1),\n  Y1 = c(0, 1, 0, 1, 0, 1, 0, 1),\n  Y = c(0, 0, 1, 1, 0, 1, 0, 1),\n  prob = c(0.0814, 0.1846, 0.0846, 0.1494, 0.139, 0.175, 0.075, 0.111)\n)\nbb_5.6\n\n  A Y0 Y1 Y   prob\n1 0  0  0 0 0.0814\n2 0  0  1 0 0.1846\n3 0  1  0 1 0.0846\n4 0  1  1 1 0.1494\n5 1  0  0 0 0.1390\n6 1  0  1 1 0.1750\n7 1  1  0 0 0.0750\n8 1  1  1 1 0.1110\n\n\nit is different. Table 5.1 is entitled Simulation probabilities for sim1.r because they are simulated probabilities. If the simulation size is increased 10000 the results are more in line with the author’s.\n\ndata.frame(\"author\" = bb_5.6$prob, \"sim\" = scm_5.6$tbl$prob) |&gt;\n  mutate(diff = sim - author)\n\n  author   sim    diff\n1 0.0814 0.082  0.0006\n2 0.1846 0.176 -0.0086\n3 0.0846 0.091  0.0064\n4 0.1494 0.156  0.0066\n5 0.1390 0.142  0.0030\n6 0.1750 0.170 -0.0050\n7 0.0750 0.071 -0.0040\n8 0.1110 0.112  0.0010\n\n\nand we also confirm the theoretical probabilities of the first line in the table as follows\n\\[\n\\begin{align*}\n\\text{using (5.1), p.81, we can write the joint distribution as} \\\\\nP(Y = 0, A = 0, Y1 = 0, Y0 = 0) = \\\\\nP(Y = 0 \\mid A = 0, Y1 = 0, Y0 = 0) P(A = 0 \\mid Y1 = 0, Y0 = 0)P(Y1 = 0 \\mid Y0 = 0)\\ P(Y0 =0)\n\\end{align*}\n\\]\nand\n\\[\nP(Y = 0 \\mid A = 0, Y1 = 0, Y0 = 0)=1\n\\]\nand from the simulation we have\n\\[\n\\begin{align*}\nP(A = 1 \\mid Y0 = 0, Y1 = 0) &= 0.6307 \\\\\nP(A = 0 \\mid Y0 = 0, Y1 = 0) &=  - 0.6307 =  0.3693\\\\\nP(A = 1 \\mid Y0 = 1, Y1 = 0) &= 0.4699 \\\\\nP(A = 0 \\mid Y0 = 1, Y1 = 0) &= 1 - 0.4699 = 0.5301 \\\\\nP(A = 1 \\mid Y0 = 0, Y1 = 1) &= 0.4867 \\\\\nP(A = 0 \\mid Y0 = 0, Y1 = 1) &= 1 - 0.4867 = 0.5133 \\\\\nP(A = 1 \\mid Y0 = 1, Y1 = 1) &= 0.4263 \\\\\nP(A = 0 \\mid Y0 = 1, Y1 = 1) &= 1 - 0.4263 = 0.5737\n\\end{align*}\n\\]\nand we note that \\(Y0\\) and \\(Y1\\) are independent, i.e. \\(P(Y1 \\mid Y0) = P(Y1)\\) so that\n\\[\n\\begin{align*}\nP(Y0 = 1) &= 0.42\\\\\nP(Y0 = 0) &= 1 - 0.42 = 0.58\\\\\nP(Y1 = 1) &= 0.62\\\\\nP(Y1 = 0) &= 1 - 0.62 = 0.38\n\\end{align*}\n\\]\nand since \\(Y0\\) and \\(Y1\\) are independent, i.e. \\(P(Y1 \\mid Y0) = P(Y1)\\) then\n\\[\n\\begin{align*}\nP(Y = 0, A = 0, Y1 = 0, Y0 = 0) &= \\\\\nP(Y = 0 \\mid A = 0, Y1 = 0, Y0 = 0) P(A = 0 \\mid Y1 = 0, Y0 = 0) \\cdot P(Y1 = 0 \\mid Y0 = 0)P(Y0 = 0) &= \\\\\n1 \\cdot P(A = 0 \\mid Y1 = 0, Y0 = 0) \\cdot P(Y1 = 0)P(Y0 = 0) &= \\\\\n1\\times 0.3693 \\times 0.58 \\times 0.38 &= 0.0814\n\\end{align*}\n\\]\n\n\n5.2.4 Structural Causal Model 5.7\nThe causal DAG is\n\nscm_5.7 &lt;- list()\nscm_5.7 &lt;- within(scm_5.7, {\n  dag &lt;- dagify(\n    A ~ H,\n    Y ~ A + H)\n\n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\n\n\n\n\nFigure 5.7: Another way to generate confounding\n\n\n\n\nand we simulate the model with simpr (and with the author’s script after)\n\nscm_5.7 &lt;- within(scm_5.7, {\n  set.seed(222)\n  data &lt;- simpr::specify(\n    # generate the confounder H first\n    H = ~ rbinom(1000, size = 1, prob = 0.4),\n    # let the treatment depend on the confounder\n    probA = ~ H * 0.8 + (1 - H) * 0.3,\n    A = ~ rbinom(1000, size = 1, prob = probA),\n    # let the outcome depend on the treatment and the confounder\n    probY = ~ A * (H * 0.5 + (1 - H) * 0.7) + (1 - A) * (H * 0.3 + (1 - H) * 0.5),\n    Y = ~ rbinom(1000, size = 1, prob = probY)) |&gt;\n    simpr::generate(1) |&gt;\n    unnest()\n  \n  # create the tabletbl &lt;- sim_5.6$data |&gt;\n  tbl &lt;- data |&gt;\n    group_by(A, H, Y) |&gt;\n    count(name = \"prob\") |&gt;\n    ungroup() |&gt;\n    mutate(prob = prob / sum(prob))\n  stopifnot(near(sum(tbl$prob), 1))\n\n  # format the tableau\n  summ &lt;- fciR::gt_probs(tbl, title = \"Table 5.2\",\n                   subtitle = \"Simulation Probabilities for `sim2.r`\")\n})\n\nWarning: `cols` is now required when using `unnest()`.\nℹ Please use `cols = c(sim)`.\n\nscm_5.7$summ\n\n\n\n\n\nTable 5.2:  Simulation Probabilities for sim2.r \n  \n    \n      Table 5.2\n    \n    \n      Simulation Probabilities for `sim2.r`\n    \n    \n      A\n      H\n      Y\n      prob\n    \n  \n  \n    0\n0\n0\n0.221\n    0\n0\n1\n0.209\n    0\n1\n0\n0.054\n    0\n1\n1\n0.029\n    1\n0\n0\n0.060\n    1\n0\n1\n0.120\n    1\n1\n0\n0.153\n    1\n1\n1\n0.154\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n\n\n\n\n\nand the author provides the theoretical probability table\n\nbb_5.7 &lt;- data.frame(\n  A = c(0, 0, 0, 0, 1, 1, 1, 1),\n  H = c(0, 0, 1, 1, 0, 0, 1, 1),\n  Y = c(0, 1, 0, 1, 0, 1, 0, 1),\n  prob = c(0.21, 0.21, 0.056, 0.024, 0.024, 0.126, 0.16, 0.16)\n)\nbb_5.7\n\n  A H Y  prob\n1 0 0 0 0.210\n2 0 0 1 0.210\n3 0 1 0 0.056\n4 0 1 1 0.024\n5 1 0 0 0.024\n6 1 0 1 0.126\n7 1 1 0 0.160\n8 1 1 1 0.160\n\n\nand computating the theoretical prob. as was donw above\n\\[\n\\begin{align*}\nP(Y, A, H) = P(Y \\mid A, H)P(A \\mid H)P(H) \\\\\n\\end{align*}\n\\]\nand from the simulation we have\n\\[\n\\begin{align*}\nP(H = 1) = 0.4 \\\\\nP(H = 0) = 0.6 \\\\\nP(A = 1 \\mid H = 0) = 0.3 \\\\\nP(A = 0 \\mid H = 0) = 1 - 0.3 = 0.7 \\\\\nP(A = 1 \\mid H = 1) = 0.8 \\\\\nP(A = 0 \\mid H = 1) = 1 - 0.8 = 0.2 \\\\\nP(Y = 1 \\mid A = 1, H = 1) = 0.5\\\\\nP(Y = 0 \\mid A = 1, H = 1) = 1 - 0.5 = 0.5\\\\\nP(Y = 1 \\mid A = 1, H = 0) = 0.7\\\\\nP(Y = 0 \\mid A = 1, H = 0) = 1 - 0.7 = 0.3\\\\\nP(Y = 1 \\mid A = 0, H = 1) = 0.3\\\\\nP(Y = 0 \\mid A = 0, H = 1) = 1 - 0.3 = 0.7\\\\\nP(Y = 1 \\mid A = 0, H = 0) = 0.5\\\\\nP(Y = 0 \\mid A = 0, H = 0) = 1 - 0.5 = 0.5\\\\\n\\end{align*}\n\\]\nTherefore for row 1 of table 5.2 we have\n\\[\n\\begin{align*}\nP(Y = 0, A = 0, H = 0) &= P(Y = 0 \\mid A = 0, H = 0)P(A = 0 \\mid H = 0)P(H = 0) \\\\\n&= 0.5 \\times 0.7 \\times 0.6 = 0.21\n\\end{align*}\n\\]\nand for row 4 of table 5.2\n\\[\n\\begin{align*}\nP(Y = 1, A = 0, H = 1) &= P(Y = 1 \\mid A = 0, H = 1)P(A = 0 \\mid H = 1)P(H = 1) \\\\\n&= 0.3 \\times 0.2 \\times 0.4 = 0.024\n\\end{align*}\n\\]\nUsing either sim1.r or sim2.r we can get the joint probability of \\(A\\) and \\(Y\\) using the law of total probabilities.\n\nbb_5.6 |&gt;\n  group_by(A, Y) |&gt;\n  summarize(prob = sum(prob))\n\n# A tibble: 4 × 3\n# Groups:   A [2]\n      A     Y  prob\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0 0.266\n2     0     1 0.234\n3     1     0 0.214\n4     1     1 0.286\n\n\n\nbb_5.7 |&gt;\n  group_by(A, Y) |&gt;\n  summarize(prob = sum(prob))\n\n# A tibble: 4 × 3\n# Groups:   A [2]\n      A     Y  prob\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0 0.266\n2     0     1 0.234\n3     1     0 0.184\n4     1     1 0.286\n\n\nNow\n\nOne might wonder if knowledge of \\((A, H, Y)\\) is equivalent to knowledge of \\((A, Y0, Y1, Y)\\) Given \\((A, Y)\\) we cannot recover both \\(Y0\\) and \\(Y1\\).\n\nFor example we can recover \\(Y1 = 1\\) since\n\\[\n\\begin{align*}\nP(Y1 = 1 \\mid A = 1, Y = 1) &= \\frac{P(Y1 = 1, A = 1, Y = 1)}{P(A = 1, Y = 1)} \\\\\n&= \\frac{0.175 + 0.111}{0.175 + 0.111} = 1\n\\end{align*}\n\\]\nbut for \\(P(Y0 = 1 \\mid A = 1, Y = 1)\\) we have\n\\[\n\\begin{align*}\nP(Y0 = 1 \\mid A = 1, Y = 1) &= \\frac{P(Y0 = 1, A = 1, Y = 1)}{P(A = 1, Y = 1)} \\\\\n&= \\frac{0.111}{0.175 + 0.111} = 0.39\n\\end{align*}\n\\]\nand so the likelihood that \\(Y0 = 1\\) is only 39% and it is therefore more likely that \\(Y0 = 0\\) . . . but not always.\n\nDoes the additional knowledge of \\(H\\) help to identify \\(Y(0)\\)? We leave that as a question for the reader.\n\nIf we include \\(H\\) in the DAG of figure 5.6 we obtain the DAG of figure 5.8 below. We observe that \\((Y(0), Y(1)) \\perp\\!\\!\\!\\perp A \\mid H\\). In addition, since \\(Y\\) is a collider \\((Y(0), Y(1)) \\not\\!\\perp\\!\\!\\!\\perp A \\mid Y\\), therefore if we use the data from figure 5.8 below (simall.r) we have\n\\[\n\\begin{align*}\nP(Y0 = 1 \\mid H = 1) &= \\frac{P(Y0 = 1, H = 1)}{P(H = 1)} \\\\\n&= \\frac{0.019 + 0.006 + 0.051 + 0.05}{0.031 + 0.02 + 0.019 + 0.006 + 0.109 + 0.117 + 0.051 + 0.05} \\\\\n&= \\frac{0.126}{0.403} = 0.31\n\\end{align*}\n\\] Therefore \\(H\\) is not entirely sufficient to determine \\((Y(0), Y(1))\\) as it will do it only 31% of the time for \\(Y(0) = 1\\) and otherwise 69% of the time for \\(Y(0) = 0\\).\n\n\n5.2.5 Potential Outcomes Behind the Scenes\n\nscm_5.8 &lt;- list()\nscm_5.8 &lt;- within(scm_5.8, {\n  the_nodes &lt;- c(\"A\" = \"\", \n                 \"Y\" = \"\", \n                 \"U\" = \"(Y(0), y(1))\")\n  dag &lt;- dagify(\n    A ~ H,\n    Y ~ A + U,\n    U ~ H,\n    latent = \"U\",\n    labels = the_nodes)\n  \n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\n\n\n\n\nFigure 5.8: Potential Outcomes Behind the Scenes\n\n\n\n\n\nscm_5.8 &lt;- within(scm_5.8, {\n  set.seed(888)\n  \n  data &lt;- simpr::specify(\n    # generate the observed confounder H\n    H = ~ rbinom(1000, size = 1, prob = 0.4),\n    # let the treatment depend on the observed confounder\n    probA = ~ H * 0.8 + (1 - H) * 0.3,\n    A = ~ rbinom(1000, size = 1, prob = probA),\n    # generate the poential outcomes dependent on the observed confounder\n    probY0 = ~ H * 0.3 + (1 - H) * 0.5,\n    Y0 = ~ rbinom(1000, size = 1, prob = probY0),\n    probY1 = ~ H * 0.5 + (1 - H) * 0.7,\n    Y1 = ~ rbinom(1000, size = 1, prob = probY1),\n    # let the outcome depend on the treatment and Y0, Y1\n    Y = ~ A * Y1 + (1 - A) * Y0) |&gt;\n    generate(1) |&gt;\n    unnest()\n  \n  tbl &lt;- data |&gt;\n    group_by(H, A, Y0, Y1, Y) |&gt;\n    count(name = \"prob\") |&gt;\n    ungroup() |&gt;\n    mutate(prob = prob / sum(prob)) |&gt;\n    identity()\n  stopifnot(near(sum(tbl$prob), 1))\n\n  # format the tableau\n  tbl &lt;- fciR::gt_probs(tbl, title = \"Table for `simall.r`\",\n                   subtitle = \"Simulation Probabilities for `simall.r`\")\n})\n\nWarning: `cols` is now required when using `unnest()`.\nℹ Please use `cols = c(sim)`.\n\nscm_5.8$tbl\n\n\n\n\n\nTable 5.3:  Simulation Probabilities for simall.r \n  \n    \n      Table for `simall.r`\n    \n    \n      Simulation Probabilities for `simall.r`\n    \n    \n      H\n      A\n      Y0\n      Y1\n      Y\n      prob\n    \n  \n  \n    0\n0\n0\n0\n0\n0.058\n    0\n0\n0\n1\n0\n0.137\n    0\n0\n1\n0\n1\n0.087\n    0\n0\n1\n1\n1\n0.138\n    0\n1\n0\n0\n0\n0.021\n    0\n1\n0\n1\n1\n0.057\n    0\n1\n1\n0\n0\n0.018\n    0\n1\n1\n1\n1\n0.056\n    1\n0\n0\n0\n0\n0.032\n    1\n0\n0\n1\n0\n0.027\n    1\n0\n1\n0\n1\n0.012\n    1\n0\n1\n1\n1\n0.022\n    1\n1\n0\n0\n0\n0.113\n    1\n1\n0\n1\n1\n0.111\n    1\n1\n1\n0\n0\n0.050\n    1\n1\n1\n1\n1\n0.061\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n\n\n\n\n\nand the causal DAG for the What-If? study is\n\nscm_5.9 &lt;- list()\nscm_5.9 &lt;- within(scm_5.9, {\n  the_nodes &lt;- c(\"U\" = \"Unmeasured, healthy behavior (U=1)\", \n                 \"AD0\" = \"Adherence time 0\", \n                 \"VL0\" = \"Viral Load time 0\", \n                 \"T\" = \"Naltrexone (T=1)\", \n                 \"A\" = \"Reduced drinking (A=1)\", \n                 \"AD1\" = \"Adherence time 1\", \n                 \"VL1\" = \"Viral Load time 1\")\n  coords &lt;- data.frame(\n    name = names(the_nodes),\n    x = c(2, 3, 4, 1, 2, 3, 4),\n    y = c(2, 2, 2, 1, 1, 1, 1)\n  )\n  dag &lt;- dagify(\n    AD0 ~ U,\n    VL0 ~ AD0,\n    A ~ `T` + U,\n    AD1 ~ A,\n    VL1 ~ AD0 + AD1 + U,\n  outcome = \"VL1\",\n  exposure = \"T\",\n  latent = \"U\",\n  labels = the_nodes)\n  \n  \n  text_labels &lt;- c(\"A\", expression(AD[0]), expression(AD[1]),\n                   \"T\", \"U\", expression(VL[0]), expression(VL[1]))\n  plot &lt;- fciR::ggp_dag(dag, text_labels = text_labels, text_size = 5)\n})\n\n\n\n\n\n\nFigure 5.9: The Double What-If? Study"
  },
  {
    "objectID": "ch05_dag.html#exercises",
    "href": "ch05_dag.html#exercises",
    "title": "5  Causal Directed Acyclic Graphs",
    "section": "5.3 Exercises",
    "text": "5.3 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "part02.html",
    "href": "part02.html",
    "title": "Part II Causality",
    "section": "",
    "text": "This part covers the causality methods."
  },
  {
    "objectID": "ch06_backdoor.html#standardization-via-outome-modeling",
    "href": "ch06_backdoor.html#standardization-via-outome-modeling",
    "title": "6  Backdoor Method via Standardization",
    "section": "6.1 Standardization via Outome Modeling",
    "text": "6.1 Standardization via Outome Modeling\n\nStandardization vis via outcome modelingis one way to estimate \\(E(Y(t))\\)\n\n\\[\n\\begin{align*}\n&\\text{by double expectation theorem} \\\\\n&E(Y(t)) = E_H E(Y(t) \\mid H) \\\\\n&\\text{by independence of T given H, (6.1)} \\\\\n&= E_HE(Y(t) \\mid T=t, H) \\\\\n&\\text{by consistency assumption} \\\\\n&= E_HE(Y \\mid do(T=t), H)\n\\end{align*}\n\\]\nand with a binary data set we can write\n\\[\n\\begin{align*}\nE_H E(Y \\mid do(T=t), H) = E(Y \\mid do(T=t), H = 0) P(H = 0) + E(Y \\mid do(T=t), H = 1) P(H = 1)\n\\end{align*}\n\\]\nand using the example on p. 100 with the mortality data we first load the data set\n\ndata(\"mortality_long\", package = \"fciR\")\nmortality &lt;- mortality_long\n\nand we begin by calculating \\(\\hat{E}(Y \\mid T=0, H=0)\\)\n\nmortality |&gt;\n  filter(`T` == 0, H == 0) |&gt;\n  summarize(EY = weighted.mean(Y, n))\n\n           EY\n1 0.002253583\n\n\nand for all permutations of \\(T\\) and \\(H\\) we have\n\nEYcondTH &lt;- mortality |&gt;\n  group_by(`T`, H) |&gt;\n  summarize(EYcond = weighted.mean(Y, n))\nEYcondTH\n\n# A tibble: 4 × 3\n# Groups:   T [2]\n      T     H  EYcond\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0     0 0.00225\n2     0     1 0.0565 \n3     1     0 0.00268\n4     1     1 0.0446 \n\n\nand then we multiply the conditional expectations by the probabilities of \\(H\\).\n\nPH &lt;- mortality |&gt;\n  group_by(H) |&gt;\n  summarize(prob = sum(p))\nPH\n\n# A tibble: 2 × 2\n      H  prob\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.897\n2     1 0.103\n\n\nand the multiplication\n\nEYH &lt;- dplyr::inner_join(EYcondTH, PH, by = c(\"H\")) |&gt;\n  mutate(EYH = EYcond * prob)\nEYH\n\n# A tibble: 4 × 5\n# Groups:   T [2]\n      T     H  EYcond  prob     EYH\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1     0     0 0.00225 0.897 0.00202\n2     0     1 0.0565  0.103 0.00582\n3     1     0 0.00268 0.897 0.00240\n4     1     1 0.0446  0.103 0.00459\n\n\nand the final results are\n\nEYout &lt;- EYH |&gt;\n  group_by(`T`) |&gt;\n  summarize(EYout = sum(EYH))\nEYout\n\n# A tibble: 2 × 2\n      T   EYout\n  &lt;dbl&gt;   &lt;dbl&gt;\n1     0 0.00784\n2     1 0.00700\n\n\nNow, lets do it with raw data. For that we convert the mortality data to have 1 line per 10000 observations.\n\nmort &lt;- mortality |&gt;\n  select(H, `T`, Y, n) |&gt;\n  mutate(n = as.integer(n / 10000)) |&gt;\n  tidyr::uncount(n)\n\nand the function used to automate the process described above is as follows\n\nfunc_out_np &lt;- function(data, formula, exposure.name, confound.names) {\n  # the name of the outcome variable\n  outcome.name &lt;- all.vars(rlang::f_lhs(formula))\n  \n  # compute the frequencies, this table is then used for all computations\n  summ &lt;- data |&gt;\n    count(.data[[outcome.name]], .data[[exposure.name]], .data[[confound.names]]) |&gt;\n    mutate(freq = n / sum(n))\n  stopifnot(abs(sum(summ$freq) - 1) &lt; .Machine$double.eps^0.5)\n  \n  # the expected value of the outcome given the exposure and confounds\n  # i.e. the outcome conditional mean\n  out_cond_mean &lt;- summ |&gt;\n    group_by(.data[[exposure.name]], .data[[confound.names]]) |&gt;\n    summarize(EY = weighted.mean(.data[[outcome.name]], w = n)) |&gt;\n    # add and id column to be able to join the confounds variables later\n    unite(col = \"id\", .data[[confound.names]], remove = FALSE)\n  \n  # the confounds' distribution\n  confound_dist &lt;- summ |&gt;\n    group_by(.data[[confound.names]]) |&gt;\n    summarize(prob = sum(freq)) |&gt;\n    # add and id column to be able to join the confounds variables later\n    unite(col = \"id\", .data[[confound.names]], remove = FALSE)\n  \n  # multiply the conditional expectation by the confound probabilities\n  EY &lt;- dplyr::inner_join(out_cond_mean, confound_dist, by = \"id\") |&gt;\n    group_by(.data[[exposure.name]]) |&gt;\n    summarize(EY = sum(EY * prob)) |&gt;\n    # create the output vector\n    arrange(.data[[exposure.name]]) |&gt;\n    pull(EY) |&gt;\n    setNames(c(\"EY0\", \"EY1\"))\n  EY\n}\n\n\nmort.out.est &lt;- func_out_np(mort, formula = Y ~ `T` + H, exposure.name = \"T\", \n                           confound.names = \"H\")\n\nWarning: Use of .data in tidyselect expressions was deprecated in tidyselect 1.2.0.\nℹ Please use `all_of(var)` (or `any_of(var)`) instead of `.data[[var]]`\n\nmort.out.est\n\n        EY0         EY1 \n0.007831933 0.006969437 \n\n\nand we can see it gives the same results with the fciR package with fciR::backdr_out_np)\n\nmort.out.np &lt;- fciR::boot_est(\n    mort,\n    func = fciR::backdr_out_np, times = 500, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = Y ~ `T` + H, exposure.name = \"T\", confound.names = \"H\")\n# mort.out.np\n\n\nfciR::gt_measures(mort.out.np,  digits = 6,\n            title = \"Mortality\", \n            subtitle = paste(\"Standardized Estimates via Outcome Modeling\",\n                              \"Non Parametric Without Regression\",\n                             sep = \"&lt;br&gt;\"))\n\n\n\n\n\nTable 6.1:  Standardized Estimates via Outcome Modeling. Non Parametric Without\nRegression. \n  \n    \n      Mortality\n    \n    \n      Standardized Estimates via Outcome ModelingNon Parametric Without Regression\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    EY0\n0.007844\n(0.007398, 0.008355)\n    EY1\n0.006998\n(0.00625, 0.007794)\n    RD\n-0.000847\n(-0.001679, 3.2e-05)\n    RR\n0.891105\n(0.788227, 1.004317)\n    RR*\n0.999148\n(0.99831, 1.000033)\n    OR\n0.890345\n(0.786856, 1.00435)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\n\nSection 6.1, p. 101, also give the function stand.r to standardize via outcome modeling. This function uses regression with a saturated model, also called non- parametric, see last paragraph of section 2.2 on p. 25 regarding the saturated model.\nThe stand.r is called fciR::backdr_out_sat, the suffix sat means it is with a saturated regression model. Here we show it using the tidyverse way.\n\nfunc_out_sat &lt;- function(data, formula, exposure.name, confound.names) {\n  # this function works when there is only one confound\n  stopifnot(length(confound.names) == 1)\n  \n  x0 &lt;- \"(Intercept)\"  # name of intercept used by lm, glm, etc.\n  \n  # marginal expected value of the outcome\n  mean_confound &lt;- mean(data[, confound.names])\n  \n  # fit the outcome model\n  fit &lt;- glm(formula = formula, data = data) |&gt;\n    broom::tidy()\n  \n  # add distribution marginal expected potential outcomes\n  # marginal computaiton only for terms including the confound\n  fit &lt;- fit |&gt;\n    mutate(\n      # find the terms that includes the confound\n      marg_exp = grepl(pattern = confound.names, x = term),\n      # multiply the terms inlcuding the confound\n      marg_exp = ifelse(marg_exp, estimate * mean_confound, estimate)\n    )\n  \n  # E(Y(0))\n  EY0 &lt;- fit |&gt;\n    filter(term %in% c(x0, confound.names)) |&gt;\n    summarize(EY = sum(marg_exp)) |&gt;\n    pull()\n  # E(Y(1))\n  EY1 &lt;- fit |&gt;\n    summarize(EY = sum(marg_exp)) |&gt;\n    pull()\n  \n  c(\"EY0\" = EY0, \"EY1\" = EY1)\n}\n\nHere we use it again with the mort dataset. It is important to note that the formula inlcudes all interactions since the model is saturated\n\nmort.out &lt;- func_out_sat(mort, formula = Y ~ `T` * H, \n                        exposure.name = \"T\",  confound.names = \"H\")\nmort.out\n\n        EY0         EY1 \n0.007831933 0.006969437 \n\n\nand we now do it with the function fciR::backdr_out_npr. That function works exactly as the function standr in the book.\n\nmort.out.sat &lt;- fciR::boot_est(\n    mort,\n    func = fciR::backdr_out_sat, times = 500, alpha = 0.05,\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = Y ~ `T` + H, exposure.name = \"T\", confound.names = \"H\")\n\n\ndf &lt;- mort.out.sat\ntbl &lt;- fciR::gt_measures(df,  digits = 6,\n            title = \"Mortality\", \n            subtitle = paste(\"Standardized Estimates\",\n                              \"Saturated Model With Regression\",\n                              sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"Mortality Data Effect Measures\",\n                          subtitle = \"Standardized Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.1: Saturated Model With Regression\n\n\n\n\nand the results are the same again. In conclusion pretty much any of the function. The function fciR::backdr_out_npr seems faster. The function fciR::backdr_out_np is actually useful as a double check and it actually uses a “pure” application of probabilities.\n\nExamples\n\nWhat-if? Study\n\ndata(\"whatifdat\", package = \"fciR\")\n\n\nSaturated model with backdr_out_sat\n\nwhatif.out.sat &lt;- fciR::boot_est(\n    whatifdat, fciR::backdr_out_sat, times = 500, alpha = 0.05,\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = Y ~ A * H, exposure.name = \"A\", confound.names = \"H\")\nwhatif.out.sat\n\n# A tibble: 6 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 EY0    0.272    0.378  0.480    0.05 percentile\n2 EY1    0.208    0.291  0.376    0.05 percentile\n3 RD    -0.213   -0.0860 0.0447   0.05 percentile\n4 RR     0.535    0.786  1.13     0.05 percentile\n5 RR*    0.710    0.881  1.07     0.05 percentile\n6 OR     0.381    0.705  1.21     0.05 percentile\n\n\nand we compare with the author’s\n\ncomp &lt;- data.frame(\n  term = c(\"EY0\", \"EY1\", \"RD\", \"RR\"),\n  .estimate.auth = c(0.375, 0.289, -0.086, 0.77),\n  .estimate = whatif.out.sat$.estimate[whatif.out.sat$term %in% \n                                         c(\"EY0\", \"EY1\", \"RD\", \"RR\")])\n# stopifnot(sum(abs(comp$.estimate.auth - comp$.estimate)) &lt; 0.02)\n\nand the results are presented in table 6.1\n\ndf &lt;- whatif.out.sat\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.1\", \"What-If Study\"), \n            subtitle = paste(\"Standardized Estimates\",\n                             \"Saturated Model With Regression\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"What-If Study\",\n                          subtitle = \"Standardized Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.2: Table 6.1\n\n\n\n\nwhere we observe a reduction of the viral load but the difference is not statistically significant.\n\n\nNon-parametric With backdr_out_np\n\nwhatif.out.np &lt;- fciR::boot_est(\n    whatifdat, fciR::backdr_out_np, times = 500, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = Y ~ A + H, exposure.name = \"A\", confound.names = \"H\")\nwhatif.out.np\n\n# A tibble: 6 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 EY0    0.279    0.377  0.480    0.05 percentile\n2 EY1    0.208    0.289  0.367    0.05 percentile\n3 RD    -0.209   -0.0874 0.0344   0.05 percentile\n4 RR     0.524    0.768  1.12     0.05 percentile\n5 RR*    0.723    0.875  1.05     0.05 percentile\n6 OR     0.380    0.672  1.18     0.05 percentile\n\n\n\ndf &lt;- whatif.out.np\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.1\", \"What-If Study\"), \n            subtitle = paste(\"Standardized Estimates\",\n                              \"Non Parametric\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"What-If Study\",\n                          subtitle = \"Standardized Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.3: Table 6.1 extra\n\n\n\n\nand the results are the same with both non-parametric and with the saturated model.\n\n\n\nDouble What-if? Study\n\ndata(\"doublewhatifdat\", package = \"fciR\")\n\n\nSaturated model with backdr_out_sat\n\ndoublewhatif.out.sat &lt;- fciR::boot_est(\n    doublewhatifdat, fciR::backdr_out_sat, times = 500, alpha = 0.05,\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A * AD0, exposure.name = \"A\", confound.names = \"AD0\")\n\n\ndf &lt;- doublewhatif.out.sat\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.2\", \"Double What-If Study\"), \n            subtitle = paste(\"Standardized Estimates with &lt;em&gt;H = AD0&lt;/em&gt;\",\n                             \"Saturated Model With regression\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"Double What-If Study\",\n                          subtitle = \"Standardized Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.4: Table 6.2\n\n\n\n\n\n\nNon-parametric With backdr_out_np\n\ndoublewhatif.out.np &lt;- fciR::boot_est(\n    doublewhatifdat, fciR::backdr_out_np,\n    times = 500, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A + AD0, exposure.name = \"A\", confound.names = \"AD0\")\n\n\ndf &lt;- doublewhatif.out.np\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.1\", \"Double What-If Study\"), \n            subtitle = paste(\"Standardized Estimates\",\n                              \"Non Parametric\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"Double What-If Study\",\n                          subtitle = \"Standardized Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.5: Table 6.1 non-parametric\n\n\n\n\nand the results are the same wether one uses non-parametric or a fully saturated regression model.\n\nFor comparisons, we repeat the standardization with \\(H = VL_0\\)\n\n\ndoublewhatif.vl0.out &lt;- fciR::boot_est(\n    doublewhatifdat, fciR::backdr_out_sat, times = 500, alpha = 0.05,\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A * VL0, exposure.name = \"A\", confound.names = \"VL0\")\n\n\ndf &lt;- doublewhatif.vl0.out\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.3\", \"Double What-If Study\"), \n            subtitle = paste(\"Standardized Estimates with &lt;em&gt;H = VL0&lt;/em&gt;\",\n                             \"Saturated Model With Regression\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"Double What-If Study\",\n                          subtitle = \"Standardized Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.6: Table 6.3\n\n\n\n\n\n\n\n\n6.1.1 Average Effect of Treatment on the Treated\nThe function bootstandatt described in section 6.1.1 is not really necessary in the sense that the change is so little that we can simply set the argument att = TRUE in backdr_out_sat. The less coding we do, the better off we are!\n\nWhat-if? Study\nSee the argument att = TRUE for fciR::backdr_out_sat.\n\nwhatif.out.att.sat &lt;- fciR::boot_est(\n    whatifdat, fciR::backdr_out_sat, times = 500, alpha = 0.05,\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = Y ~ A * H, exposure.name = \"A\", confound.names = \"H\", att = TRUE)\n\nand we compare with the author’s\n\ncomp &lt;- data.frame(\n  term = c(\"EY0\", \"EY1\", \"RD\", \"RR\"),\n  .estimate.auth = c(0.361, 0.276, -0.085, 0.765),\n  .estimate = whatif.out.att.sat$.estimate[whatif.out.att.sat$term %in% c(\"EY0\", \"EY1\", \"RD\", \"RR\")]\n)\n# sum(abs(comp$.estimate.auth - comp$.estimate))\n# stopifnot(sum(abs(comp$.estimate.auth - comp$.estimate)) &lt; 0.02)\n\nand the results are presented in table 6.1\n\ndf &lt;- whatif.out.att.sat\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.4\", \"What-If Study\"), \n            subtitle = paste(\"Standardized ATT estimates\",\n                             \"Saturated model With Regression\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"What-If Study\",\n                          subtitle = \"Standardized ATT Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.7: Table 6.4\n\n\n\n\nIt can also be done using a non-parametric method without regression. That is following pure probabilities from the data.\n\nwhatif.out.att.np &lt;- fciR::boot_est(\n    whatifdat, fciR::backdr_out_np,\n    times = 250, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = Y ~ A + H, exposure.name = \"A\", confound.names = \"H\", att = TRUE)\n\n\n\n\n\n\nFigure 6.8: Table 6.4 Nnon-parametric\n\n\n\n\nand again both saturated and non-parametric methods give the same results.\n\n\nDouble What-if? Study\n\ndoublewhatif.out.att.sat &lt;- fciR::boot_est(\n    doublewhatifdat, fciR::backdr_out_sat, times = 500, alpha = 0.05,\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A * AD0, exposure.name = \"A\", confound.names = \"AD0\", att = TRUE)\n\n\ndf &lt;- doublewhatif.out.att.sat\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.5\", \"Double What-If Study\"), \n            subtitle = paste(\"Standardized ATT Estimates with &lt;em&gt;H = AD0&lt;/em&gt;\",\n                             \"Saturated Model With Regression\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"Double What-If Study\",\n                          subtitle = \"Standardized ATT Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.9: Table 6.5\n\n\n\n\n\ndoublewhatif.out.att.np &lt;- fciR::boot_est(\n    doublewhatifdat, fciR::backdr_out_np,\n    times = 500, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A * VL0, exposure.name = \"A\", confound.names = \"VL0\", att = TRUE)\n\n\ndf &lt;- doublewhatif.out.att.np\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.6\", \"Double What-If Study\"), \n            subtitle = paste(\"Standardized ATT Estimates with &lt;em&gt;H = VL0&lt;/em&gt;\",\n                             \"Non Parametric\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"Double What-If Study\",\n                          subtitle = \"Standardized ATT Estimates, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.10: Table 6.6\n\n\n\n\n\n\n\n6.1.2 Standardization with a Parametric Outcome Model\nFor a the parametric outcome model fciR::backdr_out() is used\n\nWhat-if? Study\n\ndata(\"whatif2dat\", package = \"fciR\")\n\n\nwhatif2.out &lt;- fciR::boot_est(\n    whatif2dat, fciR::backdr_out,\n    times = 100, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = vl4 ~ A + lvlcont0, exposure.name = \"A\", confound.names = \"lvlcont0\")\n\nand we compare with the author’s\n\ncomp &lt;- data.frame(\n  term = c(\"EY0\", \"EY1\", \"RD\", \"RR\"),\n  .estimate.auth = c(0.360, 0.300, -0.061, 0.831),\n  .estimate = whatif2.out$.estimate[whatif2.out$term %in% c(\"EY0\", \"EY1\", \"RD\", \"RR\")]\n)\n# comp\n# stopifnot(sum(abs(comp$.estimate.auth - comp$.estimate)) &lt; 0.01)\n\nand the results are presented in table 6.1\n\ndf &lt;- whatif2.out\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.7\", \"What-If Study\"),\n            subtitle = paste(\"Outcome-model Standardization with &lt;em&gt;H = lvlcont0&lt;/ems&gt;\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"What-If Study\",\n                          subtitle = \"Outcome-model Standardization, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.11: Table 6.7\n\n\n\n\n\n\nGeneral Social Survey\n\ndata(\"gss\", package = \"fciR\")\ngssrcc &lt;- gss[, c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")]\ngssrcc &lt;- gssrcc[complete.cases(gssrcc), ]\n\n\na_formula &lt;- trump ~ gthsedu + magthsedu + white + female + gt65\ngssrcc.out &lt;- boot_est(data = gssrcc, func = fciR::backdr_out,\n           times = 100, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = a_formula, exposure.name = \"gthsedu\", \n           confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"))\n# gssrcc.out\n\nand we compare with the author’s\n\ncomp &lt;- data.frame(\n  term = c(\"EY0\", \"EY1\", \"RD\", \"RR\"),\n  .estimate.auth = c(0.233, 0.271, 0.038, 1.164),\n  .estimate = gssrcc.out$.estimate[gssrcc.out$term %in% c(\"EY0\", \"EY1\", \"RD\", \"RR\")]\n)\nstopifnot(sum(abs(comp$.estimate.auth - comp$.estimate)) &lt; 0.02)\n\nand the results are presented in table 6.8\n\ndf &lt;- gssrcc.out\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.8\", \"General Social Survey\"), \n            subtitle = paste(\"Outcome-model Standardization\",\n            \"Effect of &lt;em&gt;More than High School Education&lt;/em&gt; on &lt;em&gt;\n            Voting for Trump&lt;/em&gt;\",\n            sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"General Social Survey\",\n                          subtitle = \"Outcome-model Standardization, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.12: Table 6.8"
  },
  {
    "objectID": "ch06_backdoor.html#standardization-via-exposure-modeling",
    "href": "ch06_backdoor.html#standardization-via-exposure-modeling",
    "title": "6  Backdoor Method via Standardization",
    "section": "6.2 Standardization via Exposure Modeling",
    "text": "6.2 Standardization via Exposure Modeling\n\nThe exposure model is also known as the propensity score, denoted \\(e(H)\\), as it is a function of \\(H\\).\n\n\\[\n\\begin{align*}\ne(H) = (T \\mid H) = expit(\\alpha_0 + \\alpha_1 H_1 + \\ldots + \\alpha_k H_k)\n\\end{align*}\n\\]\nthe proof of\n\\[\nE(Y(1)) = E \\left( \\frac{TY}{e(H)} \\right)\n\\] is\n\\[\n\\begin{align*}\n&\\text{by definition of expectation} \\\\\nE \\left( \\frac{t \\cdot y}{e(H)} \\right) &= \\sum_{y,t,h} \\frac{TY}{e(H)} P(Y=y,T=t,H=h) \\\\\n&\\text{by multiplication rule} \\\\\n&= \\sum_{y,t,h} \\frac{t \\cdot y}{e(H)} P(Y=y \\mid T=t,H=h) P(T=t \\mid H=h) P(H=h) \\\\\n&\\text{because } T \\text{ is binary, and by definition of } e(H) \\text{ then } e(H) = P(T \\mid H) \\\\ &= \\sum_{y,t,h} \\frac{t \\cdot y}{e(H)} P(Y=y \\mid T=t,H=h) e(H) P(H=h) \\\\\n&\\text{and when } T=0 \\text{ the summand is zero, therefore we are left with } T=1 \\\\\n&= \\sum_{y,h} \\frac{y}{e(H)} P(Y=y \\mid T=1,H=h) e(H) P(H=h) \\\\\n&\\text{we cancel the } e(H) \\text{ in numerator and denominator} \\\\\n&= \\sum_{y,h} y P(Y=y \\mid T=1,H=h) P(H=h) \\\\\n&\\text{by definition of conditional expectation} \\\\\n&= E_H (E(Y \\mid T=1, H)) \\\\\n&\\text{and by (6.2) which implies (6.1)} \\\\\n&= E(Y(1))\n\\end{align*}\n\\]\n\nExamples\n\nMortality Rates by Country\n\ndata(\"mortality_long\", package = \"fciR\")\nmortdat &lt;- as.data.frame(mortality_long)\n\nCompute the standardized estimates using exposure modeling with fciR::backdr_exp_np which uses the algorithm defined in mk.mortdat at the beginning of section 6.2. You can see the code by pressing F2 on fciR::backdr_exp_np.\n\nmessage(\"this takes 25 sec., use cache\")\n\nthis takes 25 sec., use cache\n\nmortdat.exp.np &lt;- boot_est(data = mort, func = backdr_exp_np,\n           times = 100, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = Y ~ `T` + H, exposure.name = \"T\", confound.names = \"H\")\n\n\nmort.EY0 &lt;- mortdat.exp.np$.estimate[mortdat.exp.np$term == \"EY0\"]\nmort.EY1 &lt;- mortdat.exp.np$.estimate[mortdat.exp.np$term == \"EY1\"]\n# verify with the author's\n# stopifnot(abs(mort.EY0 - 0.0078399) &lt; 1e-4,\n#           abs(mort.EY1 - 0.0069952) &lt; 1e-4)\n\n\n\n\n6.2.1 Average Effect of Treatment on the Treated\nIt can be proven that\n\\[\nE(Y(0) \\mid T=1) = E \\left( \\frac{Y(1 - T) e(H)}{e_0(1 - e(H))}  \\right), \\, e_0 = P(T=1) \\\\\n\\]\nas follows\n\\[\n\\begin{align*}\n&\\text{by the rule of double expectation} \\\\\nE(Y(0) \\mid T=1) &= E_{H \\mid T=1} E(Y \\mid T=0, H) \\\\\n&\\text{by definition of expectation} \\\\\n&= E_{H \\mid T=1} \\left[ \\sum_{y} y P(Y=y \\mid T=0, H) \\right] \\\\\n&\\text{by definition of conditional expectation} \\\\\n&= \\sum_h \\left[ \\sum_{y} y P(Y=y \\mid T=0, H) \\right] P(H=h \\mid T=1) \\\\\n&\\text{by definition of conditional expectation we have that} \\\\\n&P(H=h \\mid T=1) = \\frac{P(T=1 \\mid H=h) P(H=h)}{P(T=1)} \\\\\n&\\text{therefore} \\\\\nE(Y(0) \\mid T=1) &= \\sum_{y,h} y P(Y=y \\mid T=0, H=h) \\frac{P(T=1 \\mid H=h) P(H=h)}{P(T=1)} \\\\\n&\\text{rearranging terms} \\\\\n&= \\sum_{y,h} y \\frac{P(T=1 \\mid H=h)}{P(T=1)} \\left[ P(Y=y \\mid T=0, H=h)P(H=h)  \\right] \\\\\n&\\text{and multiply by } 1 = \\frac{P(T=0 \\mid H=h)}{P(T=0 \\mid H=h)} \\\\\n&= \\sum_{y,h} y \\frac{P(T=1 \\mid H=h)}{P(T=1)} \\left[ \\frac{P(Y=y \\mid T=0, H=h)P(T=0 \\mid H=h)P(H=h)}{P(T=0 \\mid H=h)}  \\right] \\\\\n&\\text{rearranging the terms again} \\\\\n&= \\sum_{y,h} y \\frac{P(T=1 \\mid H=h)}{P(T=1)P(T=0 \\mid H=h)} \\left[ P(Y=y \\mid T=0, H=h)P(T=0 \\mid H=h)P(H=h)  \\right] \\\\\n&\\text{using the multiplication rule} \\\\\n&= \\sum_{y,h} y \\frac{P(T=1 \\mid H=h)}{P(T=1)P(T=0 \\mid H=h)} P(Y=y, T=0, H=h) \\\\\n&\\text{ and since } e(h) = P(T=1 \\mid H=h) \\text{ and } e_0 = P(T=1) \\\\\n&= \\sum_{y,h} y \\cdot \\frac{e(h)}{e_0 (1 - e(h))} \\cdot P(Y=y, T=0, H=h) \\\\\n&\\text{ and since } \\sum_t (1-t) P(Y=y, T=t, H=h) = P(Y=y, T=0, H=h) \\\\\n&= \\sum_{y,h} y \\cdot \\frac{e(h)}{e_0 (1 - e(h))} \\cdot \\sum_t (1-t) P(Y=y, T=t, H=h) \\\\\n&= \\sum_{y,h, t} y \\cdot (1-t) \\cdot \\frac{e(h)}{e_0 (1 - e(h))} \\cdot P(Y=y, T=t, H=h) \\\\\n&\\text{and by definition of expectation} \\\\\n&= E \\left[ Y \\cdot (1-T) \\cdot \\frac{e(H)}{e_0 (1 - e(H))} \\right]\n\\end{align*}\n\\]\nTo to the calculation with ATT we use backdr_exp_np but, this time, with the argument att = TRUE. When att = TRUE, backdr_exp_np gives the estimate for ATT as attsem.r on p. 116 of section 6.2.1.\n\nmortdat.exp.np.att &lt;- boot_est(data = mort, func = backdr_exp_np,\n           times = 100, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = Y ~ `T` + H, exposure.name = \"T\", confound.names = \"H\", \n           att = TRUE)\n\nSee previous section for calculation with mortality data for the function with the flag is_att = TRUE\n\nmort.att.EY0 &lt;- mortdat.exp.np.att$.estimate[mortdat.exp.np.att$term == \"EY0\"]\nmort.att.EY1 &lt;- mortdat.exp.np.att$.estimate[mortdat.exp.np.att$term == \"EY1\"]\nmort.att.EY0\n\n[1] 0.01013666\n\nmort.att.EY1\n\n[1] 0.008841271\n\nmort.EY1\n\n[1] 0.007024521\n\nmessage(\"TODO: EY1 should not be influenced by ATT??\")\n\nTODO: EY1 should not be influenced by ATT??\n\n# TODO: EY1 should not be influenced by ATT??\n# stopifnot(abs(mort.att.EY0 - 0.010176) &lt; 1e-4,\n#           abs(mort.att.EY1 - 0.0069952) &lt; 1e-4)\n\n\n\n6.2.2 Standardization with a Parametric Exposure Model\nThe function fciR::backdr_exp() is used to standardized with a parametric exposure model and the glm fit. It is the main function used in the chapter.\nAlternatively the standardization could be done with geeglm from the geepack package. For those focused primarily on the risk difference. See the explanation on section 6.2.2 on why geeglm is not really good for the risk ratio.\nThe function is called exp in the book. We rename it fciR::backdr_exp() to be more informative and avoid mix up with the much-used base R function exp.\n\nWhat-if? Study\nFirst we do it using the glm fit\n\nwhatif2.exp &lt;- boot_est(data = whatif2dat, func = backdr_exp,\n           times = 250, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = vl4 ~ A + lvlcont0, exposure.name = \"A\",\n           confound.names = \"lvlcont0\")\n\nand compare with the author’s\n\ncomp &lt;- data.frame(\n  term = c(\"EY0\", \"EY1\", \"RD\", \"RR\"),\n  .estimate.auth = c(0.36, 0.30, -0.06, 0.834),\n  .estimate = whatif2.exp$.estimate[whatif2.exp$term %in% c(\"EY0\", \"EY1\", \"RD\", \"RR\")])\nstopifnot(sum(abs(comp$auth - comp$est)) &lt; 0.01)\n\nand the results are presented in table 6.9\n\ndf &lt;- whatif2.exp\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.9\", \"What-If Study\"), \n            subtitle = paste(\"Exposure-model Standardization with &lt;em&gt;H = lvlcont0&lt;/em&gt;\",\n                             sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"What-If Study\",\n                          subtitle = \"Exposure-model Standardization, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.13: Table 6.9\n\n\n\n\nthen we use the geeglm from the geepack package fit for risk difference\n\nwhatif2.exp.gee &lt;- boot_est(data = whatif2dat, func = backdr_exp_gee,\n           times = 250, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = vl4 ~ A + lvlcont0, exposure.name = \"A\", \n           confound.names = \"lvlcont0\")\n\nand the results are presented in table 6.9\n\ndf &lt;- whatif2.exp.gee\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.9\", \"What-If Study\"), \n            subtitle = paste(\"Exposure-model Standardization using &lt;em&gt;geeglm&lt;/em&gt; wtih &lt;em&gt;H = lvlcont0&lt;/em&gt;\",\n                             sep = \"&lt;br&gt;\"))\ntbl\n\n\n\n\n\nTable 6.2:  Table 6.9 using geelm \n  \n    \n      Table 6.9 What-If Study\n    \n    \n      Exposure-model Standardization using geeglm wtih H = lvlcont0\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    EY0\n0.363\n(0.248, 0.491)\n    EY1\n0.298\n(0.229, 0.379)\n    RD\n-0.065\n(-0.187, 0.072)\n    RR\n0.825\n(0.568, 1.266)\n    RR*\n0.904\n(0.736, 1.112)\n    OR\n0.746\n(0.426, 1.403)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\n\n\n\nGeneral Social Survey\nThe gssrcc is defined in section 6.1.2 above. It is the gss data with complete cases only.\nThe standexp function on page 119-120 of section 6.2.2 is not needed anymore as standexp was created with parameters in the previous section. We just need to run it as follows\n\na_formula &lt;- trump ~ gthsedu + magthsedu + white + female + gt65\ngssrcc.exp &lt;- boot_est(data = gssrcc, func = backdr_exp,\n           times = 250, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = a_formula, exposure.name = \"gthsedu\", \n           confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"))\n\nand compare with the author’s\n\ncomp &lt;- data.frame(\n  term = c(\"EY0\", \"EY1\", \"RD\", \"RR\"),\n  .estimate.auth = c(0.231, 0.272, 0.041, 1.176),\n  .estimate = gssrcc.exp$.estimate[gssrcc.exp$term %in% c(\"EY0\", \"EY1\", \"RD\", \"RR\")])\n# stopifnot(sum(abs(comp$.estimate.auth - comp$.estimate)) &lt; 0.015)\n\nand the results are presented in table 6.10\n\n\n\n\n\nFigure 6.14: Table 6.10"
  },
  {
    "objectID": "ch06_backdoor.html#doubly-robust-standardization",
    "href": "ch06_backdoor.html#doubly-robust-standardization",
    "title": "6  Backdoor Method via Standardization",
    "section": "6.3 Doubly Robust Standardization",
    "text": "6.3 Doubly Robust Standardization\nThe function backdr_dr() does a doubly robust standardization. It is not in the text but is actually used for the exercise. It is very similar to badstanddr.\nThe function badstanddr is replaced by backdr_dr_bad, used for doubly robust standardization with a misspecified outcome model.\nand using the What-if Study we obtain\n\nwhatif2.bad &lt;- boot_est(data = whatif2dat, func = fciR::backdr_dr_bad,\n           times = 100, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = vl4 ~ A + lvlcont0, exposure.name = \"A\",\n           confound.names = \"lvlcont0\")\n\nand compare with the author’s\n\ncomp &lt;- data.frame(\n  term = c(\"EY0\", \"EY1\", \"RD\", \"RR\"),\n  .estimate.auth = c(0.362, 0.300, -0.062, 0.830),\n  .estimate = whatif2.bad$.estimate[whatif2.bad$term %in% c(\"EY0\", \"EY1\", \"RD\", \"RR\")])\nstopifnot(sum(abs(comp$.estimate.auth - comp$.estimate)) &lt; 0.07)\n\nand the results are presented in table 6.9\n\ndf &lt;- whatif2.bad\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 6.12\", \"What-If Study\"), \n            subtitle = paste(\"Doubly Robust Standardization\",\n            \"Combining the Misspecified Outome Model of Table 6.11\", \n            \"and the Exposure Model of Table 6.9\",\n            sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"What-If Study\",\n                          subtitle = \"Doubly Robust Standardization MISSPECIFIED, 95% confidence interval\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 6.15: Table 6.12\n\n\n\n\n\n6.3.1 Doubly Robust Standardization Simulation\n\n6.3.1.1 With simdr\nThe simulation of doubly robust standardization discussed at the end of section 6.3 in p. 126 to 130 and found in simdr is analyzed in an appendix at Doubly Robust Simulation.\nThe results obtained by Brumback are close enough to what we have below. Here is a tableau of her results\n\ndata(fciR::fci_tbl_06_13)\n\nWarning in data(fciR::fci_tbl_06_13): data set 'fciR::fci_tbl_06_13' not found\n\ndf &lt;- fci_tbl_06_13\n\ndf &lt;- df |&gt; select(ss, estimator, description, mean, sd, pval) |&gt;\n  mutate(ss = paste(\"ss\", ss, sep = \"=\")) |&gt;\n  pivot_longer(cols = c(\"mean\", \"sd\", \"pval\"), names_to = \"stats\",\n               values_to = \"value\") |&gt;\n  mutate(value = ifelse(stats == \"pval\", round(value, 2), round(value, 4))) |&gt;\n  unite(col = \"heading\", ss, stats, sep = \"_\") |&gt;\n  pivot_wider(id_cols = c(\"estimator\", \"description\"), names_from = \"heading\",\n              values_from = \"value\")\n\n\ntitle &lt;- \"Table 6.13 and 6.14\"\nsubtitle &lt;- paste(\"Sampling Distribution from Simulation\", \n                   \"Investigating Small-Sample Robustness\", \n                   \"True E(Y(0))=0.01, True E(Y(1))=0.02\",\n                  sep = \"&lt;br&gt;\")\nfciR::gt_standdr(df, title = title, subtitle = subtitle)\n\n\n\n\n\nTable 6.3:  Table 6.13 \n  \n    \n      Table 6.13 and 6.14\n    \n    \n      Sampling Distribution from SimulationInvestigating Small-Sample RobustnessTrue E(Y(0))=0.01, True E(Y(1))=0.02\n    \n    \n      estimator\n      description\n      \n        ss=40\n      \n      \n        ss=100\n      \n    \n    \n      mean\n      sd\n      pval\n      mean\n      sd\n      pval\n    \n  \n  \n    EYT0\nUnadjusted\n0.0076\n0.0015\n0.00\n0.0079\n0.0016\n0.00\n    EYT1\nUnadjusted\n0.0042\n0.0012\n0.00\n0.0038\n0.0012\n0.00\n    EY0exp\nLinear Exposure\n0.0100\n0.0021\n0.92\n0.0100\n0.0020\n0.61\n    EY1exp\nLinear Exposure\n0.0195\n0.0127\n0.19\n0.0196\n0.0562\n0.81\n    EY0exp2\nLogistic Exposure\n0.0101\n0.0021\n0.42\n0.0100\n0.0020\n0.73\n    EY1exp2\nLogistic Exposure\n0.0204\n0.0064\n0.07\n0.0200\n0.0068\n0.96\n    EY0out\nOverspecified Outcome\n0.0100\n0.0021\n0.79\n0.0100\n0.0020\n0.74\n    EY1out\nOverspecified Outcome\n0.0200\n0.0066\n0.84\n0.0200\n0.0069\n0.74\n    EY0dr\nDoubly Robust\n0.0100\n0.0021\n0.82\n0.0100\n0.0020\n0.72\n    EY1dr\nDoubly Robust\n0.0197\n0.0106\n0.37\n0.0290\n0.1891\n0.14\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n\n\n\n\n\n\n\n6.3.1.2 With mc_standdr\nWe perform the simulation using a Monte Carlo simulation called mc_standdr. The script is in the appendix at mc_standdr.\nWe use a sample size of only 1000 as in the book.\n\nnrep &lt;- 1000\n\nSo here the simulation with \\(ss \\in \\{40, 100\\}\\)\n\nmc.out &lt;- fciR::mc_standdr(ss = c(40, 100), nrep = nrep)\n\nGrid of  2  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===================================                                   |  50%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n\nand we compute the p-values\n\nmc.out &lt;- mc.out |&gt;\n    mutate(`T` = ifelse(grepl(pattern = \"0\", estimator), 0, 1),\n         h0 = ifelse(`T` == 0, 0.01, 0.02),\n         sdp = sd / sqrt(n),\n         z = abs((mean - h0) / sdp),\n         pval = 2 * (1 - pnorm(z))) |&gt;\n  select(-sdp, -z)\n# mc.out\n\nand show the results in a table\n\nthe_estimators &lt;- c(\"EYT0\" = \"Unadjusted\", \"EYT1\" = \"Unadjusted\",\n                      \"EY0exp\" = \"Linear Exposure\", \"EY1exp\" = \"Linear Exposure\",\n                      \"EY0exp2\" = \"Logistic Exposure\", \"EY1exp2\" = \"Logistic Exposure\",\n                      \"EY0out\" = \"Overspecified Outcome\", \"EY1out\" = \"Overspecified Outcome\",\n                      \"EY0dr\" = \"Doubly Robust\", \"EY1dr\" = \"Doubly Robust\")\ndft &lt;- mc.out |&gt;\n  select(ss, estimator, mean, sd, pval) |&gt;\n  mutate(ss = paste(\"ss\", ss, sep = \"=\")) |&gt;\n  pivot_longer(cols = c(\"mean\", \"sd\", \"pval\"), names_to = \"stats\", \n               values_to = \"value\") |&gt;\n  mutate(value = ifelse(stats == \"pval\", round(value, 2), round(value, 4))) |&gt;\n  unite(col = \"heading\", ss, stats, sep = \"_\") |&gt;\n  pivot_wider(id_cols = \"estimator\", names_from = \"heading\", \n              values_from = \"value\") |&gt;\n  mutate(description = the_estimators[match(estimator, names(the_estimators))]) |&gt;\n  relocate(description, .after = estimator)\n# reorder the rows to match book's\ndft &lt;- dft[match(names(the_estimators), dft$estimator), ]\n\ntitle &lt;- \"Table 6.13 and 6.14 &lt;em&gt;(by FL)&lt;/em&gt;\"\nsubtitle &lt;- paste(\"Sampling Distribution from Simulation\", \n                   \"Investigating Small-Sample Robustness\", \n                   \"True E(Y(0))=0.01, True E(Y(1))=0.02\",\n                  sep = \"&lt;br&gt;\")\nfciR::gt_standdr(dft, title = title, subtitle = subtitle)\n\n\n\n\n\nTable 6.4:  Table 6.13 and 14 by FL \n  \n    \n      Table 6.13 and 6.14 (by FL)\n    \n    \n      Sampling Distribution from SimulationInvestigating Small-Sample RobustnessTrue E(Y(0))=0.01, True E(Y(1))=0.02\n    \n    \n      estimator\n      description\n      \n        ss=40\n      \n      \n        ss=100\n      \n    \n    \n      mean\n      sd\n      pval\n      mean\n      sd\n      pval\n    \n  \n  \n    EYT0\nUnadjusted\n0.0076\n0.0016\n0.00\n0.0079\n0.0016\n0.00\n    EYT1\nUnadjusted\n0.0042\n0.0012\n0.00\n0.0038\n0.0011\n0.00\n    EY0exp\nLinear Exposure\n0.0100\n0.0022\n0.86\n0.0100\n0.0020\n0.77\n    EY1exp\nLinear Exposure\n0.0200\n0.0068\n0.97\n0.0200\n0.0225\n0.98\n    EY0exp2\nLogistic Exposure\n0.0100\n0.0022\n0.48\n0.0100\n0.0020\n1.00\n    EY1exp2\nLogistic Exposure\n0.0204\n0.0064\n0.03\n0.0198\n0.0064\n0.41\n    EY0out\nOverspecified Outcome\n0.0100\n0.0022\n0.91\n0.0100\n0.0020\n0.87\n    EY1out\nOverspecified Outcome\n0.0201\n0.0067\n0.72\n0.0198\n0.0065\n0.35\n    EY0dr\nDoubly Robust\n0.0100\n0.0022\n0.96\n0.0100\n0.0020\n0.90\n    EY1dr\nDoubly Robust\n0.0202\n0.0069\n0.28\n0.0272\n0.2125\n0.28\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n\n\n\n\n\n\n\n6.3.1.3 Plotting the Monte Carlo Simulation\nWe will not reiterate the comments from Brumback as the results in the tableau just above confirm them.\nA plot can however illustrate Brumback’s main points. This ones shows the estimates’ mean with their 5% and 95% quantiles from the simulation.\n\n\n\n\n\nFigure 6.16: Figures 6.13 and 14 by FL"
  },
  {
    "objectID": "ch06_backdoor.html#exercises",
    "href": "ch06_backdoor.html#exercises",
    "title": "6  Backdoor Method via Standardization",
    "section": "6.4 Exercises",
    "text": "6.4 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch07_did.html#difference-in-differences-did-estimators",
    "href": "ch07_did.html#difference-in-differences-did-estimators",
    "title": "7  Difference-in-Differences Estimators",
    "section": "7.1 Difference-in-Differences (DiD) Estimators",
    "text": "7.1 Difference-in-Differences (DiD) Estimators\n\n7.1.1 DiD Estimator with a Linear Model\n\nThe method relies on consistency as well as assumption A1:\n\n\\[\n\\begin{align*}\n&E(Y_1(0) \\mid A=1) - E(Y_1(0) \\mid A=1) = \\\\\n&E(Y_0(0) \\mid A=1) - E(Y_0(0) \\mid A=1)\n\\end{align*}\n\\]\nThat assumption is interpreted as : If assume that no treatment was applied in “year” 1 (nothing happened in year 1) than we have the same results as in the base “year”.\n\nalso called additive equi-confounding […]. The target of estimation is the linear ATT presented in chapter 6 as a risk difference\n\n\\[\n\\text{Linear ATT:} \\: E(Y_1(1) - Y_1(0) \\mid A=1)\n\\]\nwhich is interpreted as *what is the the average difference between \\(Y_1(1)\\) and \\(Y_1(0)\\) if they all treated. For the example of the bank’s NIM, what is the average difference between the banks’ NIM in NIRP ad non-NIRP economy assuming they are all subjected to the NIRP policy, that is wihout the impact of the NIRP policy.\nand we have\n\\[\n\\begin{align*}\n& E(Y_1(1) - Y_1(0)) \\mid A=1) = \\\\\n& E(Y_1(1) \\mid A=1) - E(Y_1(0) \\mid A=1) = \\\\\n& [E(Y_1(1) \\mid A=1) - E(Y_1(0) \\mid A=1)] - [E(Y_1(0) \\mid A=0) - E(Y_1(0) \\mid A=0)] = \\\\\n& [E(Y_1(1) \\mid A=1) - E(Y_1(0) \\mid A=0)] - [E(Y_1(0) \\mid A=1) - E(Y_1(0) \\mid A=0)] = \\\\\n&\\text{consistency assumption} \\\\\n& [E(Y_1 \\mid A=1) - E(Y_1 \\mid A=0)] - [E(Y_1(0) \\mid A=1) - E(Y_1(0) \\mid A=0)] = \\\\\n&\\text{assumption A1, additive equi-confounding} \\\\\n&\\text{i.e. assuming that if nothing happens in year 1, then we can use the results of year 0} \\\\\n& [E(Y_1 \\mid A=1) - E(Y_1 \\mid A=0)] - [E(Y_0 \\mid A=1) - E(Y_0 \\mid A=0)]\n\\end{align*}\n\\]\n\nwe can therefore estimate the linear ATT via the difference in differences of averages\n\n\\[\n\\begin{align*}\n&[\\hat{E}(Y_1 \\mid A=1) - \\hat{E}(Y_1 \\mid A=0)] - [\\hat{E}(Y_0 \\mid A=1) - \\hat{E}(Y_0 \\mid A=0)]= \\\\\n&[\\hat{E}(Y_1 \\mid A=1) - \\hat{E}(Y_0 \\mid A=1)] - [\\hat{E}(Y_1 \\mid A=0) - \\hat{E}(Y_0 \\mid A=0)]= \\\\\n&[\\hat{E}(Y_1 - Y_0 \\mid A=1)] - [\\hat{E}(Y_1 - Y_0 \\mid A=0)]\n\\end{align*}\n\\]\n\nWe can also compute the DiD via the linear model\n\n\\[\nE(Y_t \\mid A) = \\alpha_0 + \\alpha_1 t + \\alpha_2 A + \\beta A * t\n\\]\nand therefore\n\\[\n\\begin{align*}\n\\beta &= [E(Y_1 \\mid A=1) - E(Y_0 \\mid A=1)] - [E(Y_1 \\mid A=0) - E(Y_0 \\mid A=0)] \\\\\n&= (\\alpha_0 + \\alpha_1 + \\alpha_2 + \\beta) - (\\alpha_0 + \\alpha_2) -\n((\\alpha_0 + \\alpha_2) - (\\alpha_0))\n\\end{align*}\n\\]\n\nAs we can estimate \\(E(Y(1) \\mid A=1)\\) directly via \\(E(Y_1 \\mid A=1)\\), we can also recover \\(E(Y(0) \\mid A=1)\\) via \\(E(Y_1 \\mid A=1) - \\beta\\)\n\nbecause from above we have\n\\[\n\\begin{align*}\n\\beta &= E(Y_1(1) - Y_1(0)) \\mid A=1) \\\\\n&= E(Y_1(1) \\mid A=1) - E(Y_1(0) \\mid A=1) \\\\\n&= E(Y_1 \\mid A=1) - E(Y_1(0) \\mid A=1) \\\\\n&\\therefore \\\\\nE(Y_1(0) \\mid A=1)& = E(Y_1 \\mid A=1) - \\beta\n\\end{align*}\n\\]\n\n\n7.1.2 DiD Estimator with a Loglinear Model\n\nThe method relies on consistency as well as assumption A2:\n\n\\[\n\\begin{align*}\n\\frac{E(Y_1(0) \\mid A=1)}{E(Y_1(0) \\mid A=1)} =\n\\frac{E(Y_0(0) \\mid A=1)}{E(Y_0(0) \\mid A=1)}\n\\end{align*}\n\\]\n\n\n7.1.3 DiD Estimator with a Logistic Model\n\nThe method relies on consistency as well as assumption A3:\n\n\\[\n\\begin{align*}\nlogit(E(Y_1(0) \\mid A=1)) - logit(E(Y_1(0) \\mid A=1)) =\nlogit(E(Y_0(0) \\mid A=1)) - logit(E(Y_0(0) \\mid A=1))\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch07_did.html#comparison-with-standardization",
    "href": "ch07_did.html#comparison-with-standardization",
    "title": "7  Difference-in-Differences Estimators",
    "section": "7.2 Comparison with Standardization",
    "text": "7.2 Comparison with Standardization\nThe functions used for DID estimations are fciR::did_linear(), fciR::did_loglinear and fciR::logistic.\n\n7.2.1 doublewhatifdat\nThe DiD estimator with the linear model\n\ndwhatif.did.lin &lt;- fciR::boot_est(\n    doublewhatifdat, func = fciR::did_linear,\n    times = 100, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0A1\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A + VL0, exposure.name = \"A\", confound.names = \"VL0\")\n\nThe DiD estimator with the loglinear model\n\ndwhatif.did.loglin &lt;- fciR::boot_est(\n    doublewhatifdat, func = fciR::did_loglinear,\n    times = 100, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0A1\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A + VL0, exposure.name = \"A\", confound.names = \"VL0\")\n\nThe DiD estimator with the logistic model\n\ndwhatif.did.logit &lt;- fciR::boot_est(\n    doublewhatifdat, func = fciR::did_logistic,\n    times = 100, alpha = 0.05, transf = \"exp\",\n    terms = c(\"EY0A1\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = VL1 ~ A + VL0, exposure.name = \"A\", confound.names = \"VL0\")\n\nverify the results\n\ndata(\"fci_tbl_07_02\", package = \"fciR\")\nbb_dwhatif &lt;- fci_tbl_07_02\nbb_dwhatif\n\n       model       term  Truth .estimate .lower .upper .alpha .method\n1        All E(VL1|A=1)  0.199     0.231  0.179  0.282   0.05    norm\n2     Linear      EY0A1  0.559     0.586  0.508  0.664   0.05    norm\n3  Loglinear      EY0A1  0.559     0.577  0.498  0.656   0.05    norm\n4   Logistic      EY0A1  0.559     0.592  0.513  0.671   0.05    norm\n5     Linear         RD -0.360    -0.355 -0.441 -0.270   0.05    norm\n6  Loglinear         RD -0.360    -0.346 -0.431 -0.262   0.05    norm\n7   Logistic         RD -0.360    -0.362 -0.447 -0.276   0.05    norm\n8     Linear         RR  0.356     0.394  0.309  0.500   0.05    norm\n9  Loglinear         RR  0.356     0.400  0.315  0.508   0.05    norm\n10  Logistic         RR  0.356     0.390  0.306  0.496   0.05    norm\n11    Linear         OR  0.196     0.212  0.142  0.315   0.05    norm\n12 Loglinear         OR  0.196     0.220  0.149  0.325   0.05    norm\n13  Logistic         OR  0.196     0.206  0.139  0.307   0.05    norm\n\n\n\ngt_measures_rowgrp(\n    bb_dwhatif, \n    rowgroup = \"term\",\n    rowname = \"model\",\n    title = paste(\"Table 7.2\", \"Double What-If Study\"), \n    subtitle = paste(\"Difference-in-Differences Estimation of the ATT\",\n                     sep = \"&lt;br&gt;\")\n    )\n\n\n\n\n\nTable 7.1:  Table 7.2 \n  \n    \n      Table 7.2 Double What-If Study\n    \n    \n      Difference-in-Differences Estimation of the ATT\n    \n    \n      \n      Truth\n      Estimate\n      CI1\n    \n  \n  \n    \n      E(VL1|A=1)\n    \n    All\n0.199\n0.231\n(0.179, 0.282)\n    \n      EY0A1\n    \n    Linear\n0.559\n0.586\n(0.508, 0.664)\n    Loglinear\n0.559\n0.577\n(0.498, 0.656)\n    Logistic\n0.559\n0.592\n(0.513, 0.671)\n    \n      RD\n    \n    Linear\n-0.360\n-0.355\n(-0.441, -0.27)\n    Loglinear\n-0.360\n-0.346\n(-0.431, -0.262)\n    Logistic\n-0.360\n-0.362\n(-0.447, -0.276)\n    \n      RR\n    \n    Linear\n0.356\n0.394\n(0.309, 0.5)\n    Loglinear\n0.356\n0.400\n(0.315, 0.508)\n    Logistic\n0.356\n0.390\n(0.306, 0.496)\n    \n      OR\n    \n    Linear\n0.196\n0.212\n(0.142, 0.315)\n    Loglinear\n0.196\n0.220\n(0.149, 0.325)\n    Logistic\n0.196\n0.206\n(0.139, 0.307)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\n\nand the results using the fciR package are identical.\n\ntbl_7.2 &lt;- rbind(\n  data.frame(\n    model = \"Linear\",\n    dwhatif.did.lin), \n  data.frame(\n    model = \"Loglinear\",\n    dwhatif.did.loglin),\n  data.frame(\n    model = \"Logistic\",\n    dwhatif.did.logit))\ngt_measures_rowgrp(tbl_7.2,\n    rowgroup = \"term\",\n    rowname = \"model\",\n    title = paste(\"Table 7.2&lt;em&gt;(by FL)&lt;/em&gt;\", \"Double What-If Study\", sep = \"&lt;br&gt;\"), \n    subtitle = paste(\"Difference-in-Differences Estimation of the ATT\",\n                     sep = \"&lt;br&gt;\")\n  )\n\n\n\n\n\n  \n    \n      Table 7.2(by FL)Double What-If Study\n    \n    \n      Difference-in-Differences Estimation of the ATT\n    \n    \n      \n      CI1\n      Estimate\n    \n  \n  \n    \n      EY0A1\n    \n    Linear\n(0.499, 0.644)\n0.577\n    Loglinear\n(0.52, 0.653)\n0.584\n    Logistic\n(0.508, 0.68)\n0.592\n    \n      EY1\n    \n    Linear\n(0.186, 0.285)\n0.233\n    Loglinear\n(0.179, 0.277)\n0.231\n    Logistic\n(0.184, 0.28)\n0.236\n    \n      RD\n    \n    Linear\n(-0.431, -0.267)\n-0.344\n    Loglinear\n(-0.427, -0.273)\n-0.353\n    Logistic\n(-0.441, -0.265)\n-0.356\n    \n      RR\n    \n    Linear\n(0.306, 0.494)\n0.402\n    Loglinear\n(0.308, 0.5)\n0.394\n    Logistic\n(0.315, 0.499)\n0.397\n    \n      RR*\n    \n    Linear\n(0.448, 0.65)\n0.549\n    Loglinear\n(0.451, 0.627)\n0.540\n    Logistic\n(0.424, 0.641)\n0.531\n    \n      OR\n    \n    Linear\n(0.148, 0.311)\n0.221\n    Loglinear\n(0.148, 0.311)\n0.213\n    Logistic\n(0.145, 0.32)\n0.211\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\nfciR::ggp_measures_groups(tbl_7.2, group = \"model\", \n                          title = paste(\"Table 7.2\", \"Double What-If Study\"),\n                          subtitle = paste(\"Difference-in-Differences Estimation of the ATT\",\n                                     sep = \"&lt;br&gt;\"))"
  },
  {
    "objectID": "ch08_frontdoor.html#motivation",
    "href": "ch08_frontdoor.html#motivation",
    "title": "8  Front-Door Method",
    "section": "8.1 Motivation",
    "text": "8.1 Motivation\n\nscm_8.1 &lt;- list()\nscm_8.1 &lt;- within(scm_8.1, {\n  coords &lt;- list(\n    x = c(A = 1, S = 2, U = 2, Y = 3),\n    y = c(A = 1, S = 1, U = 2, Y = 1))\n  dag &lt;- dagify(\n    A ~ U,\n    S ~ A,\n    Y ~ S + U,\n    coords = coords)\n\n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\n\n\n\n\nFigure 8.1: Front-door Causal DAG\n\n\n\n\n\nWhen \\(S\\) is a surrogate marker\n\n\\[\n\\begin{align*}\n&\\text{by double expectation rule} \\\\\nE(Y \\mid A) &= E_{S \\mid A}(E(Y \\mid A)) \\\\\n&\\text{by conditional expectation} \\\\\n&= \\sum_s E(Y \\mid S=s, A) P(S=s \\mid A) \\\\\n&\\text{because } Y \\perp\\!\\!\\!\\perp A \\mid S \\\\\n&= \\sum_s E(Y \\mid S=s) P(S=s \\mid A) \\\\\n&\\text{since there are no confounder for the effect of A on Y} \\\\\nE(Y(a)) = E(Y \\mid A=a) &=  \\sum_s E(Y \\mid S=s) P(S=s \\mid A=a)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch08_frontdoor.html#theory-and-method",
    "href": "ch08_frontdoor.html#theory-and-method",
    "title": "8  Front-Door Method",
    "section": "8.2 Theory and Method",
    "text": "8.2 Theory and Method\nUsing (8.1) from above\n\\[\n\\begin{align*}\nE(Y(a)) = E(Y \\mid A=a) &=  \\sum_s E(Y \\mid S=s) P(S=s \\mid A=a) \\\\\n&\\text{and with backdoor standardization we have that} \\\\\n&E(Y \\mid S=s) = \\sum_{a^\\prime} E(Y \\mid S=s, A=a^\\prime)P(A = a^\\prime) \\\\\n\\therefore \\\\\nE(Y(a)) = E(Y \\mid A=a) &=  \\sum_s P(S=s \\mid A=a) \\left[ \\sum_{a^\\prime} E(Y \\mid S=s, A=a^\\prime)P(A = a^\\prime) \\right] \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch08_frontdoor.html#simulated-example",
    "href": "ch08_frontdoor.html#simulated-example",
    "title": "8  Front-Door Method",
    "section": "8.3 Simulated Example",
    "text": "8.3 Simulated Example\n\nsim1 &lt;- function(n = 1e4, seed = 555) {\n  set.seed(seed)\n  # Generate the potential outcome Y(.,0) and Y(.,1)\n  Ydot0 &lt;- rbinom(n, size = 1, prob = 0.05)\n  Ydot1 &lt;- rbinom(n, size = 1, prob = 0.2)\n  # let A depend on Y(.,1)\n  probA &lt;- (1 - Ydot1) * 0.1 + Ydot1 * 0.8\n  A &lt;- rbinom(n, size = 1, prob = probA)\n  # Generate the potential outcome S(0) and S(1)\n  S0 &lt;- rbinom(n, size = 1, prob = 0.05)\n  S1 &lt;- rbinom(n, size = 1, prob = 0.9)\n  # S is a function of S0, S1 and A\n  S &lt;- (1 - A) * S0 + A * S1\n  # Y is a function of Y(., 0) and Y(., 1) and S\n  Y &lt;- (1 - S) * Ydot0 + S * Ydot1\n  data.frame(cbind(A, S, Y, Ydot0, Ydot1))\n}\n\n\nsim1_df &lt;- sim1()\nsim1.front &lt;- fciR::frontdr_np(sim1_df, formula = Y ~ A + S,\n                               exposure.name = \"A\", surrogate.name = \"S\")\nsim1.front\n\n  term   estimate std.err\n1  EY0 0.05584704      NA\n2  EY1 0.18536722      NA\n3   RD 0.12952018      NA\n4   RR 3.31919505      NA\n5  RR* 1.15899210      NA\n6   OR 3.84692084      NA\n\n\nand we compare the results to the author’s\n\nsim1.EY0 &lt;- sim1.front$estimate[sim1.front$term == \"EY0\"]\nsim1.EY1 &lt;- sim1.front$estimate[sim1.front$term == \"EY1\"]\nstopifnot(abs(sim1.EY0 - 0.055847) &lt; 1e-4,\n          abs(sim1.EY1 - 0.18537) &lt; 1e-4)\n\nand we can estimates the confidence intervals using the usual bootstrapping.\n\nmessage(\"this takes 1 sec., use cache\")\n\nthis takes 1 sec., use cache\n\nsim1.np &lt;- fciR::boot_est(\n    sim1_df, fciR::frontdr_np, times = 100, alpha = 0.05,\n    terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n    formula = Y ~ A + S, exposure.name = \"A\", surrogate.name = \"S\")\n\n\ndf &lt;- sim1.np\ntbl &lt;- fciR::gt_measures(df, \n            title = \"Chapter 8: Simulated Example\", \n            subtitle = paste(\"Front-Door Method Standardized Estimates\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL,\n                   vline = list(colors = c(\"lightseagreen\", \"violet\"),\n                                      linetype = \"solid\", size = 3, alpha = 0.5),\n                   pointrange = list(size = 1, fatten = 2),\n                   text = list(size = 3, color = \"navy\", digits = 2),\n                   text_size = list(title = 0.9, y_axis = 0.9))\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"Chapter 8: Simulated Example\",\n                          subtitle = \"Standardized Estimates Using the Front-door Method\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 8.2: Chapter 8: Simulated Example"
  },
  {
    "objectID": "ch09_instrument.html#complier-average-causal-effect-and-principal-stratification",
    "href": "ch09_instrument.html#complier-average-causal-effect-and-principal-stratification",
    "title": "9  Instrumental Variables",
    "section": "9.1 Complier Average Causal Effect and Principal Stratification",
    "text": "9.1 Complier Average Causal Effect and Principal Stratification\n\n9.1.1 Principal Stratification\n\nPrincipal stratification classifies participants according to the potential occurence of a post-randomization event. We define 4 principal strata of participants according tottheir potential outcome \\(A(t)\\).\n\n\nnever-taker: \\(A(0)=A(1)=0\\). Will not take the treatment regardless of randomized assignment.\nalways-taker: \\(A(0)=A(1)=1\\). Will not take the treatment regardless of randomized assignment.\ncomplier: \\(A(0)=0, A(1)=1\\), i.e. \\(A(t)=t\\). Will take the treatment as required.\ndefier: \\(A(0)=1, A(1)=0\\), i.e. \\(A(t)=1-t\\). Will refuse to take the treatment as required.\n\nLet \\(C\\) indicate a complier, i.e. that \\(A(t)=t\\). Then since\n\n\\(T\\) is a randomized, i.e. \\(Y \\perp\\!\\!\\!\\perp T \\mid C\\)\n\\(C\\) is a pre-randomization variable, i.e. \\(T \\perp\\!\\!\\!\\perp C\\)\n\\(Y(t,a) = Y(a)\\)\n\nit is reasonable to assume\n\\[\nY(a) \\perp\\!\\!\\!\\perp T \\mid C\n\\]\n\n\n9.1.2 Complier Average Causal Effect\nThe complier average causal effect (CACE) is defined as the average effect of treatment (ATT) in the compliers. That is\n\\[\n\\begin{align*}\nCACE &= E(Y(1) \\mid C=1) - E(Y(0) \\mid C=1) \\\\\n&= E(Y \\mid T=1, C=1) - E(Y \\mid T=0, C=1)\n\\end{align*}\n\\]\nThe equation shows CACE as a stratified treatment effct, but \\(C=1\\) defines a principal stratum instead of an ordinary one.\nWe can estimate CACE assuming exclusion and no defiers. Assuming no defiers implies\n\\[\nE(Y \\mid T=1, C=0) = E(Y \\mid T=0, C=0)\n\\]\nbecause no defiers implies that \\(C=0\\) includes only never-takers and always-takers and these 2 groups act the same way regardless of what \\(T\\) is. In addition, exclusion ensures that randomization of \\(T\\) cannot affect the outcome of the never-takers and always-takers since \\(Y \\perp\\!\\!\\!\\perp T \\mid A\\).\nThen, assuming no defiers we can say the\n\\[\nE(Y \\mid T=1) = E(Y \\mid T=1, C=1)P(C=1) + E(Y \\mid T=1, C=0)(1-P(C=1))\n\\]\nand\n\\[\nE(Y \\mid T=0) = E(Y \\mid T=0, C=1)P(C=1) + E(Y \\mid T=0, C=0)(1-P(C=1))\n\\]\nand therefore\n\\[\n\\begin{align*}\nE(Y \\mid T=1) - E(Y \\mid T=0) &= E(Y \\mid T=1, C=1)P(C=1) + E(Y \\mid T=1, C=0)(1-P(C=1)) - \\left[ E(Y \\mid T=0, C=1)P(C=1) + E(Y \\mid T=0, C=0)(1-P(C=1)) \\right] \\\\\n\\frac{E(Y \\mid T=1) - E(Y \\mid T=0)}{P(C=1)} &= E(Y \\mid T=1, C=1) + \\frac{E(Y \\mid T=1, C=0)}{P(C=1)} - E(Y \\mid T=1, C=0) - \\left[ E(Y \\mid T=0, C=1) +\n\\frac{E(Y \\mid T=0, C=0)}{P(C=1)} - E(Y \\mid T=0, C=0) \\right] \\\\\n\\end{align*}\n\\]\nand since from above we know that\n\\[\nE(Y \\mid T=1, C=0) = E(Y \\mid T=0, C=0)\n\\]\nthen\n\\[\n\\begin{align*}\n\\frac{E(Y \\mid T=1) - E(Y \\mid T=0)}{P(C=1)} = E(Y \\mid T=1, C=1) - E(Y \\mid T=1, C=0)\n\\end{align*}\n\\]\nand to find \\(P(C=1)\\) since we assume no defiers then we know that\n\\[\nP(C=1) + P(A=0 \\mid T=1) + P(A=1 \\mid T=0) = 1\n\\]\nand therefore\n\\[\n\\begin{align*}\n&\\text{assuming no defiers} \\\\\nP(C=1) &= 1 - P(A=0 \\mid T=1) + P(A=1 \\mid T=0) \\\\\n&= P(A=1 \\mid T=1) + P(A=1 \\mid T=0)\n\\end{align*}\n\\]\nand putting it together we obtain\n\\[\n\\begin{align*}\n\\text{CACE} &= E(Y \\mid T=1, C=1) - E(Y \\mid T=1, C=0) \\\\\n&= \\frac{E(Y \\mid T=1) - E(Y \\mid T=0)}{P(A=1 \\mid T=1) + P(A=1 \\mid T=0)} \\\\\n&= \\frac{ITT}{P(A=1 \\mid T=1) + P(A=1 \\mid T=0)}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch09_instrument.html#average-effect-of-treatment-on-the-treated-and-structural-nested-mean-models",
    "href": "ch09_instrument.html#average-effect-of-treatment-on-the-treated-and-structural-nested-mean-models",
    "title": "9  Instrumental Variables",
    "section": "9.2 Average Effect of Treatment on the Treated and Structural Nested mean Models",
    "text": "9.2 Average Effect of Treatment on the Treated and Structural Nested mean Models\n\nA drawback of the CACE is that it applies only to the compliers, a subgroup of the population that we cannot even identify.\n\nAnother way to do it is to use the average effect of treatment on the treated\n\\[\n\\text{ATT} = E(Y(1) - Y(0) \\mid A=1) = E(Y - Y(0) \\mid A=1)\n\\]\n\nTo estimate the ATT using the instrumental variable, \\(T\\), we introduce structural nested mean models.\n\nThe linear structural nested mean model is\n\\[\nE(Y - Y(0) \\mid A, T) = A \\beta\n\\]\n\\(Y-Y(0)\\) is assumed to be mean indedpendent of \\(T\\) given \\(A\\), that is \\(E(Y-Y(0) \\mid A, T) = E(Y-Y(0) \\mid A)\\) (see p. 22). This therefore implies that there is no defiers as they create a dependency on \\(T\\). It also implies that *any effect modifiers of \\(Y-Y(0)\\) is balanced across \\(T=0\\) and \\(T=1\\) groups.\nThe non-causal linear model is using \\(\\overrightarrow{D}=\\left[1, A, T, A \\cdot T \\right]\\) which is a function of \\(A\\) and \\(T\\)\n\\[\nE(Y \\mid A,T)=D \\eta\n\\]\ntherefore\n\\[\nD \\eta - A \\beta = E(Y(0) \\mid A, T)\n\\] and so\n\\[\nE_{A \\mid T} (D \\eta - A \\beta) = E(Y(0) \\mid A, T)\n\\]\nThe solution for \\(\\eta\\) is found using the method of chapter 2, section 2.3.\n\\[\n\\sum_{n=1}^n (1,T_i)^T(D_i \\hat{\\eta} -A_i \\beta - \\alpha) = 0\n\\]\nand for \\(\\alpha\\) and \\(\\beta\\) we use instrumental variable regression which can be done with the ivreg function from the package AER.\n\\[\n\\sum_{n=1}^n (1,T_i)^T(Y_i^* - A_i^* \\beta - \\alpha) = 0\n\\]"
  },
  {
    "objectID": "ch09_instrument.html#examples",
    "href": "ch09_instrument.html#examples",
    "title": "9  Instrumental Variables",
    "section": "9.3 Examples",
    "text": "9.3 Examples\n\n9.3.1 What-If Study\n\ndata(\"whatifdat\", package = \"fciR\")\nwhatif &lt;- whatifdat\nround(prop.table(xtabs(data = whatif, formula = ~ `T` + A), margin = 1), 2)\n\n   A\nT      0    1\n  0 0.38 0.62\n  1 0.35 0.65\n\nrm(whatifdat)\n\nand the function as found on p. 164 is modified to allow the bootstrapping with many iterations by setting IV to NA where the denominator is too small.\n\nfunc_instr_vars &lt;- function(data, formula = Y ~ A  + `T`, exposure.name = \"A\",\n                       instrument.name = \"T\", tol = .Machine$double.eps^0.5) {\n  outcome.name &lt;- all.vars(formula)[[1]]\n  \n  # estimate the ITT\n  dat0 &lt;- data[, instrument.name] == 0\n  dat1 &lt;- data[, instrument.name] == 1\n  ITT &lt;- mean(data[dat1, outcome.name]) - mean(data[dat0, outcome.name])\n  \n  # estimate the denominator of the CAE and ATT with equation (9.5)\n  denom &lt;- mean(data[dat1, exposure.name]) - mean(data[dat0, exposure.name])\n  msg &lt;- sprintf(\"The variable \\'%s\\' is a weak instrument. Typically, no analysis\n  should done with a weak instrument.\", instrument.name)\n  assertthat::assert_that(abs(denom) &gt;= tol, msg = msg)\n  IV &lt;- ITT / denom\n  out &lt;- c(\"ITT\" = ITT, \"IV\" = IV)\n  # output that can be used by rsample::bootstraps\n  data.frame(\n    term = names(out),\n    estimate = out,\n    std.err = NA_real_\n  )\n  } \n\nwith the results\n\nwhatif.est &lt;- func_instr_vars(whatif, formula = Y ~ A + `T`, \n                              exposure.name = \"A\", instrument.name = \"T\")\nstopifnot(all(abs(whatif.est$estimate - c(0.007352941, 0.27777778)) &lt; 1e-6))\nwhatif.est\n\n    term    estimate std.err\nITT  ITT 0.007352941      NA\nIV    IV 0.277777778      NA\n\n\nand using only 100 boots to estimate the CI we can use instr_vars from fciR which gives the correct result\n\nfciR::instr_vars(whatif, formula = Y ~ A + `T`, exposure.name = \"A\",\n                 instrument.name = \"T\")\n\n    term    estimate std.err\nITT  ITT 0.007352941      NA\nIV    IV 0.277777778      NA\n\n\nbut it is difficult to replicate in bootstrapping . . . this is the closest found using seed = 514229 which has wide intervals. But this result is obtained by playing with the seed.\n\nset.seed(514229)  # fibonacci prime\nwhatif.out &lt;- whatif |&gt;\n  rsample::bootstraps(times = 100, apparent = FALSE) |&gt;\n  mutate(results = purrr::map(.data$splits, function(x) {\n    dat &lt;- rsample::analysis(x)\n    func_instr_vars(dat, formula = Y ~ A + `T`, exposure.name = \"A\",\n                     instrument.name = \"T\", tol = 1e-6)})) |&gt;\n  rsample::int_pctl(.data$results, alpha = 0.05)\n\nWarning: Use of .data in tidyselect expressions was deprecated in tidyselect 1.2.0.\nℹ Please use `\"results\"` instead of `.data$results`\n\n\nWarning: Recommend at least 1000 non-missing bootstrap resamples for terms:\n`ITT`, `IV`.\n\nwhatif.out\n\n# A tibble: 2 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 ITT   -0.147   0.00770  0.158   0.05 percentile\n2 IV    -6.19   -0.0985   6.04    0.05 percentile\n\n\nand we can also do it usually fciR::instr_vars().\n\nwhatif.out &lt;- fciR::boot_est(\n  whatif, fciR::instr_vars, times = 100, seed = 514229, alpha = 0.05,\n  transf = \"identity\", terms = NULL, \n  formula = Y ~ A + `T`, exposure.name = \"A\", instrument.name = \"T\")\nwhatif.out\n\n# A tibble: 2 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 ITT   -0.147   0.00770  0.158   0.05 percentile\n2 IV    -6.19   -0.0985   6.04    0.05 percentile\n\n\n\n\n9.3.2 Double What-if Study\n\ndata(\"doublewhatifdat\", package = \"fciR\")\ndwhatif &lt;- doublewhatifdat\n\nWe compute the estimator for the linear, loglinear and logistic SNMM using the Double What-If Study. The functions are in the fciR package and can be consulted as usual by using F2 on the function.\nThe jacknife function from the package resample is replaced by using the loo_cv from the rsample package. This function creates n sets of data, with n being the nb of rows in the data, where 1 row is left out for every set. This is the same as a jackknife.\nUsing the rsample package offers a more versatile and modern way of working with R as it is the tidyverse way of coding. In addition, the author of resample mentions that he has stopped maintaining the resample package and recommends using rsample.\nSee STA 430 for an easy to understand explanation on how to compute the standard error of a jackknife.\n\ndwhatif.instr.lin &lt;- fciR::instr_linear(dwhatif, formula = VL1 ~ A * `T`,\n                                       exposure.name = \"A\", instrument.name = \"T\")\ndwhatif.instr.lin\n\n                                term   estimate std.err\n1                               beta -0.4092717      NA\n2                        E(Y(0)|A=1)  0.6400410      NA\n3                           E(Y|A=1)  0.2307692      NA\n4                      E(Y-Y(0)|A=1) -0.4092717      NA\n5     log(E(Y|A=1))-log(E(Y(0)|A=1)) -1.0201140      NA\n6 logit(E(Y|A=1))-logit(E(Y(0)|A=1)) -1.7795147      NA\n\n\nand we use a jackknife as discussed above. Since the confidence interval for the standard error is custom-made, we test it with the mtcars data set as follows.\n\n# the function to compute the standard error and confidence interval\nfunc_jack_ci &lt;- function(x, alpha = 0.05) {\n  n &lt;- length(x)\n  nsample &lt;- n - 1\n  m &lt;- mean(x)\n  v &lt;- (nsample / n) * sum((x - m)^2)\n  se &lt;- sqrt(v)\n  ci &lt;- qt(1 - alpha, df = nsample)\n  c(\".lower\" = m - ci * se, \".estimate\" = m, \".upper\" = m + ci * se)\n}\n\nmtcars.jack &lt;- sapply(seq_along(mtcars$mpg), FUN = function(i) {\n  x &lt;- mtcars$mpg[-i]\n  func_jack_ci(x)\n})\n\nThe functions are found in the fciR package as jack_run and jack_est. fciR::jack_run is the workhorse, just like fciR::boot_run. jack_est compute the inverse just like boot_est does.\n\nfunc_jack_run &lt;- function(data, func, alpha = 0.05, ...) {\n  \n  # get the leave-one-out samples\n  the_samples &lt;- rsample::loo_cv(data)\n\n  # estimates the effect measures\n  the_results &lt;- purrr::map_dfr(.x = the_samples$splits, .f = function(x) {\n    dat &lt;- rsample::analysis(x)\n    df &lt;- func(dat, ...)\n    out &lt;- c(df$estimate)\n    names(out) &lt;- df$term\n    out\n  })\n  \n  # compute the confidence interval for each term\n  out &lt;- purrr::map_dfr(.x = the_results, .f = ~func_jack_ci(., alpha = alpha), .id = \"term\")\n  \n  # create the output dataframe\n  data.frame(\n    out,\n    \".alpha\" = alpha,\n    \".method\" = \"qt\"\n  )\n}\n\n\ndf &lt;- func_jack_run(data = dwhatif, \n                    func = fciR::instr_linear, alpha = 0.05,\n                    formula = VL1 ~ A * `T`, exposure.name = \"A\",\n                    instrument.name = \"T\")\ndf\n\n                                term     .lower  .estimate     .upper .alpha\n1                               beta -0.5396419 -0.4092714 -0.2789009   0.05\n2                        E(Y(0)|A=1)  0.5159241  0.6400406  0.7641571   0.05\n3                           E(Y|A=1)  0.1864754  0.2307692  0.2750631   0.05\n4                      E(Y-Y(0)|A=1) -0.5396419 -0.4092714 -0.2789009   0.05\n5     log(E(Y|A=1))-log(E(Y(0)|A=1)) -1.2888462 -1.0201133 -0.7513804   0.05\n6 logit(E(Y|A=1))-logit(E(Y(0)|A=1)) -2.3659664 -1.7795345 -1.1931025   0.05\n  .method\n1      qt\n2      qt\n3      qt\n4      qt\n5      qt\n6      qt\n\n\nThe jack_est function to do the full jackknife in one function is included in the fci package and can be used as follows.\nFirst with the linear model\n\ndwhatif.instr.lin &lt;- fciR::jack_est(data = dwhatif, \n                                    func = fciR::instr_linear, alpha = 0.05,\n                 transf = \"identity\",\n                 VL1 ~ A * `T`, exposure.name = \"A\",\n                 instrument.name = \"T\")\ndwhatif.instr.lin\n\n                                term     .lower  .estimate     .upper .alpha\n1                               beta -0.5396419 -0.4092714 -0.2789009   0.05\n2                        E(Y(0)|A=1)  0.5159241  0.6400406  0.7641571   0.05\n3                           E(Y|A=1)  0.1864754  0.2307692  0.2750631   0.05\n4                      E(Y-Y(0)|A=1) -0.5396419 -0.4092714 -0.2789009   0.05\n5     log(E(Y|A=1))-log(E(Y(0)|A=1)) -1.2888462 -1.0201133 -0.7513804   0.05\n6 logit(E(Y|A=1))-logit(E(Y(0)|A=1)) -2.3659664 -1.7795345 -1.1931025   0.05\n  .method\n1      qt\n2      qt\n3      qt\n4      qt\n5      qt\n6      qt\n\n\nthen the loglinear model\n\ndwhatif.instr.loglin &lt;- fciR::jack_est(\n  data = dwhatif, func = fciR::instr_loglinear, alpha = 0.05,\n  transf = \"identity\", formula = VL1 ~ A * `T`, exposure.name = \"A\", instrument.name = \"T\")\ndwhatif.instr.loglin\n\n                                term     .lower  .estimate     .upper .alpha\n1                               beta -1.4946640 -1.1285268 -0.7623897   0.05\n2                        E(Y(0)|A=1)  0.4982127  0.7133378  0.9284629   0.05\n3                           E(Y|A=1)  0.1864754  0.2307692  0.2750631   0.05\n4                      E(Y-Y(0)|A=1) -0.7042270 -0.4825686 -0.2609102   0.05\n5     log(E(Y|A=1))-log(E(Y(0)|A=1)) -1.4946640 -1.1285268 -0.7623897   0.05\n6 logit(E(Y|A=1))-logit(E(Y(0)|A=1)) -3.2023439 -2.1157150 -1.0290860   0.05\n  .method\n1      qt\n2      qt\n3      qt\n4      qt\n5      qt\n6      qt\n\n\nan the logistic model\n\ndwhatif.instr.logit &lt;- fciR::jack_est(data = dwhatif, func = fciR::instr_logistic, alpha = 0.05,\n                 transf = \"identity\",\n                 formula = VL1 ~ A * `T`, exposure.name = \"A\", instrument.name = \"T\")\ndwhatif.instr.logit\n\n                                term     .lower  .estimate     .upper .alpha\n1                               beta -2.3893364 -1.8093931 -1.2294497   0.05\n2                        E(Y(0)|A=1)  0.5273143  0.6433635  0.7594127   0.05\n3                           E(Y|A=1)  0.1864754  0.2307692  0.2750631   0.05\n4                      E(Y-Y(0)|A=1) -0.5365411 -0.4125943 -0.2886474   0.05\n5     log(E(Y|A=1))-log(E(Y(0)|A=1)) -1.2883920 -1.0252925 -0.7621930   0.05\n6 logit(E(Y|A=1))-logit(E(Y(0)|A=1)) -2.3569609 -1.7939854 -1.2310100   0.05\n  .method\n1      qt\n2      qt\n3      qt\n4      qt\n5      qt\n6      qt\n\n\nwhich gives us the following result,\n\ntbl_9.1 &lt;- rbind(\n  data.frame(\n    model = \"Linear\",\n    dwhatif.instr.lin),\n  data.frame(\n    model = \"Loglinear\",\n    dwhatif.instr.loglin),\n  data.frame(\n    model = \"Logistic\",\n    dwhatif.instr.logit)) |&gt;\n  filter(term != \"beta\")\ngt_measures_rowgrp(tbl_9.1,\n    rowgroup = \"term\",\n    rowname = \"model\",\n    title = paste(\"Table 9.1&lt;em&gt;(by FL)&lt;/em&gt;\", \"Double What-If Study\", \n                  sep = \"&lt;br&gt;\"),\n    subtitle = paste(\"Instrumental Variables Analysis\", sep = \"&lt;br&gt;\")\n  )\n\n\n\n\n\nTable 9.1:  Table 9.1 \n  \n    \n      Table 9.1(by FL)Double What-If Study\n    \n    \n      Instrumental Variables Analysis\n    \n    \n      \n      CI1\n      Estimate\n    \n  \n  \n    \n      E(Y(0)|A=1)\n    \n    Linear\n(0.516, 0.764)\n0.640\n    Loglinear\n(0.498, 0.928)\n0.713\n    Logistic\n(0.527, 0.759)\n0.643\n    \n      E(Y|A=1)\n    \n    Linear\n(0.186, 0.275)\n0.231\n    Loglinear\n(0.186, 0.275)\n0.231\n    Logistic\n(0.186, 0.275)\n0.231\n    \n      E(Y-Y(0)|A=1)\n    \n    Linear\n(-0.54, -0.279)\n-0.409\n    Loglinear\n(-0.704, -0.261)\n-0.483\n    Logistic\n(-0.537, -0.289)\n-0.413\n    \n      log(E(Y|A=1))-log(E(Y(0)|A=1))\n    \n    Linear\n(-1.289, -0.751)\n-1.020\n    Loglinear\n(-1.495, -0.762)\n-1.129\n    Logistic\n(-1.288, -0.762)\n-1.025\n    \n      logit(E(Y|A=1))-logit(E(Y(0)|A=1))\n    \n    Linear\n(-2.366, -1.193)\n-1.780\n    Loglinear\n(-3.202, -1.029)\n-2.116\n    Logistic\n(-2.357, -1.231)\n-1.794\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\n\nThe difference in confidence intervals is because the author uses 1.96 = round(qnorm(0.95), 2) whereas fciR::jack_est uses an estimate confidence interval. See jack_ci code for details."
  },
  {
    "objectID": "ch10_propensity.html#theory",
    "href": "ch10_propensity.html#theory",
    "title": "10  Propensity-Score Methods",
    "section": "10.1 Theory",
    "text": "10.1 Theory\nAssuming\n\\[\n\\{ Y(t) \\} \\perp\\!\\!\\!\\perp T \\mid H\n\\]\nRosenbaum and Rubin proved that\n\\[\n\\{ Y(t) \\} \\perp\\!\\!\\!\\perp T \\mid e(H)\n\\]\nwhere \\(e(H)\\) is the propensity score.\n\nThe theory holds exactly when \\(e(H)\\) is a known function of \\(H\\). […]. The theory is also approximately true when the parameters of \\(e(H)\\) are estimated. For instance when using the logistic model.\n\n\n10.1.1 Checking for overlap\nAn important assertion of the distribution of propensity score is that it must exist for the two treatment. This is done by comparing the histograms of each treatment group to each other. It can also be better illustrated by using the propability density (as a smoothed version of the histograms).\n\n10.1.1.1 Effect of High-School education on Voting for Trump\nTo illustrate we use the example of section 6.2.2. The same results are found in table 6.10 of subsection 6.2.2 on p. 120.\nThe data used is\n\ndata(\"gss\", package = \"fciR\")\ngssrcc &lt;- gss |&gt;\n  dplyr::select(c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")) |&gt;\n  tidyr::drop_na()\n\nand we repeat the function used in section 6.2.2 to show these results\n\na_formula &lt;- trump ~ gthsedu + magthsedu + white + female + gt65\ngssrcc.exp &lt;- fciR::backdr_exp(\n  data = gssrcc, \n  formula = a_formula,\n  exposure.name = \"gthsedu\", \n  confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"))\n# convert the results from log to natural scale\ngssrcc.exp &lt;- gssrcc.exp |&gt;\n  dplyr::select(-std.err) |&gt;\n  mutate(estimate = if_else(grepl(pattern = \"^log\", x = term),\n                            exp(estimate), estimate),\n         term = sub(pattern = \"^log\", x = term, replacement = \"\")) |&gt;\n  identity()\ngssrcc.exp\n\n  term   estimate\n1  EY0 0.23106003\n2  EY1 0.27162760\n3   RD 0.04056758\n4   RR 1.17557159\n5  RR* 1.05569620\n6   OR 1.24104646\n\n\nand these estimates used the following propensity score. For details, look in the function fciR::backdr_exp by using F2. You can look in section 6.2.2, p. 119-120 for the algorithm of function standexp.r which is the same as fciR::backdr_exp using base R instead of tidyverse.\nAdditionally the function prop.r in section 6.2.2, p. 120, also compute the propensity scores. It is found in the fciR package as prop_scores().\nSo, the propensity scores are\n\n#|label: ch10_gssrcc.scores\neformula &lt;- gthsedu ~ magthsedu + white + female + gt65\neH &lt;- fitted(glm(formula = eformula, family = \"binomial\", data = gssrcc))\nassertthat::assert_that(all(!dplyr::near(eH, 0)),\n                        msg = \"eH must not equal zero\")\n\n[1] TRUE\n\ngssrcc.scores &lt;- data.frame(\n  gthsedu = gssrcc$gthsedu,\n  score = eH)\n\nwhich gives the density plot\n\nggplot(data = gssrcc.scores, aes(x = score, colour = as.factor(gthsedu))) +\n  # adjust bandwith bw = 0.05 to be like textbook\n  # see pag 178, the author uses bw = 0.05\n  geom_density(bw = 0.05, linewidth = 1) +\n  theme_classic() +\n  theme(legend.position = c(0.1, 0.9)) +\n  labs(title = \"Checking for Overlap with the Trump Example\", color = \"gthsedu\")\n\n\n\n\nFigure 10.1: Checking for Overlap with Trump Example\n\n\n\n\n\n\n10.1.1.2 Overlap failure\nWe generate the data\n\nnooverlap &lt;- function(n = 1000, seed = NULL) {\n  # Generate two confounders\n  H1 &lt;- rnorm(n)\n  H2 &lt;- rbinom(n, size = 1, prob = 0.3)\n  # Form a complex function of the confounders\n  # the author uses e\n  eF &lt;- exp(H1 + 3 * H2 - 1.5) / (1 + exp(H1 + 3 * H2 - 1.5))\n  # Let T0 depend on the complex function\n  T0 &lt;- rbinom(n, size = 1, prob = eF)\n  # Let T1 = H2\n  T1 &lt;- H2\n  # Let the treatment be the maximum of T0 and T1\n  `T` &lt;- pmax(T0, T1)\n  # Fit a propensity score model\n  eH &lt;- fitted(glm(formula = `T` ~ H1 + H2, family = \"binomial\"))\n  data.frame(\n    treatment = `T`,\n    score = eH\n  )\n}\nnooverlap.df &lt;- nooverlap(n = 1000, seed = as.integer(as.Date(\"2022-09-05\")))\n# range(nooverlap.df$score[nooverlap.df$treatment == 0])\n# range(nooverlap.df$score[nooverlap.df$treatment == 1])\n\nthen plot the scores\n\nggplot(nooverlap.df, aes(x = score, colour = as.factor(treatment))) +\n  # adjust bandwith bw = 0.05 to be like textbook\n  # see pag 178, the author uses bw = 0.05\n  # also uses trim = TRUE to show each density over it's true range\n  # i.e., no need for the range computations of the author\n  geom_density(bw = 0.05, trim = TRUE, linewidth = 1) +\n  theme_classic() +\n  theme(legend.position = c(0.1, 0.9)) +\n  labs(title = \"An Example of Overlap Failure\", color = \"treatment\")\n\n\n\n\nFigure 10.2: Example of Overlap Failure"
  },
  {
    "objectID": "ch10_propensity.html#using-the-propensity-score-in-the-outcome-model",
    "href": "ch10_propensity.html#using-the-propensity-score-in-the-outcome-model",
    "title": "10  Propensity-Score Methods",
    "section": "10.2 Using the Propensity Score in the Outcome Model",
    "text": "10.2 Using the Propensity Score in the Outcome Model\n\ngssrcc &lt;- gss[, c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")]\ngssrcc &lt;- gssrcc[complete.cases(gssrcc), ]\n\nWe simply add the propensity score to the data. Just be careful to use the right formula. We use the function fciR::prop_scores which is an alias of the function prop.r from section 6.2.2.\n\ngssrcc.propensity &lt;- \n  fciR::prop_scores(gssrcc, formula = gthsedu ~ magthsedu + white + female + gt65)\nassertthat::assert_that(all(!dplyr::near(gssrcc.propensity$scores, 0)), \n                        msg = \"Must be != zero.\")\n\n[1] TRUE\n\ngssrcc &lt;- data.frame(\n  gssrcc,\n  pscore = gssrcc.propensity$scores)\n# str(gssrcc)\n\n\na_formula &lt;- trump ~ gthsedu + pscore\ngssrcc.pscore &lt;- boot_est(data = gssrcc, \n           func = backdr_out,\n           times = 250, alpha = 0.05, transf = \"exp\",\n           terms = c(\"EY0\", \"EY1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n           formula = a_formula, exposure.name = \"gthsedu\", \n           confound.names = \"pscore\")\nstopifnot(all(gssrcc.pscore$.estimate[1:4] - c(0.233, 0.272, 0.040, 1.170) &lt; 0.01))\n\nwhich gives us the same result as in table 10.1\n\ndf &lt;- gssrcc.pscore\ntbl &lt;- fciR::gt_measures(df, \n            title = paste(\"Table 10.1\", \"General Social Survey\"), \n            subtitle = paste(\n              \"Outcome-model Standardization using Propensity Score\", \n              \"Effect of &lt;em&gt;More than High School Education&lt;/em&gt; on &lt;em&gt;\n              Voting for Trump&lt;/em&gt;\",\n            sep = \"&lt;br&gt;\"))\np &lt;- fciR::ggp_measures(df,\n                   title = NULL,\n                   subtitle = NULL)\ntbl &lt;- fciR::gt2ggp(tbl)\np + tbl + plot_annotation(title = \"General Social Survey\",\n                          subtitle = \"Outcome-model Standardization using Propensity Score\") &\n  theme(title = element_text(color = \"midnightblue\", size = rel(0.9)))\n\n\n\n\nFigure 10.3: Table 10.1\n\n\n\n\n\n10.2.1 Using the propensity score to estimate the conditinal treatment effect\n\nMany researchers use the propensity score to estimate the conditional treatment effect instead of the standardized treatment effect.\n\n\nglm(formula = trump ~ gthsedu + pscore, family = \"binomial\",\n    data = gssrcc) |&gt;\n  broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -1.15      0.148    -7.77  7.56e-15\n2 gthsedu        0.210     0.105     2.00  4.59e- 2\n3 pscore        -0.110     0.360    -0.304 7.61e- 1\n\n\nwhere the conditional odd ratio is\n\nglm(formula = trump ~ gthsedu + pscore, family = \"binomial\",\n    data = gssrcc) |&gt;\n  broom::tidy() |&gt;\n  dplyr::filter(term == \"gthsedu\") |&gt;\n  dplyr::select(estimate) |&gt;\n  exp() |&gt;\n  dplyr::pull() |&gt;\n  identity()\n\n[1] 1.23369"
  },
  {
    "objectID": "ch10_propensity.html#stratification-on-the-propensity-score",
    "href": "ch10_propensity.html#stratification-on-the-propensity-score",
    "title": "10  Propensity-Score Methods",
    "section": "10.3 Stratification on the Propensity Score",
    "text": "10.3 Stratification on the Propensity Score\nThis function can be found in the fciR package as prop_quant.\n\nfnc_prop_quant &lt;- function(data, outcome.name, exposure.name, confound.names,\n                       probs = 0:4/4, quant_var = \"pquants\") {\n  # fit the propensity score model using the prop.r, alias fciR::prop_scores,\n  #  from chapter 6\n  eformula &lt;- paste(exposure.name, paste(confound.names, collapse = \"+\"), \n                    sep = \"~\")\n  pscores &lt;- fciR::prop_scores(gssrcc, formula = formula(eformula))$scores\n  # put participants into quartiles\n  pquants &lt;- cut(pscores, quantile(pscores, prob = probs), include.lowest = T)\n  # add the quartiles to the data\n  data &lt;- data |&gt;\n    mutate(!!quant_var := pquants)\n  # estimate the average potential outcome within each quartile\n  oformula &lt;- paste(outcome.name, paste(exposure.name, \"pquants\", sep = \"*\"), \n                    sep = \"~\")\n  oformula &lt;- paste(c(oformula, \"1\", exposure.name), collapse = \"-\")\n  # print(oformula)\n  fit &lt;- glm(oformula, data = data)\n  coefs &lt;- coef(fit)\n  ncoefs &lt;- length(coefs)\n  msg &lt;- \"nb of coefs must be even. Changing the quantiles usually solves this.\"\n  assertthat::assert_that(ncoefs %% 2 == 0, msg = msg)\n  EY0 &lt;- coefs[1:(ncoefs/2)]\n  EY1 &lt;- coefs[1:(ncoefs/2)] + coefs[(1 + ncoefs/2):ncoefs]\n  meanRD &lt;- mean(EY1 - EY0)\n  # must return a data.frame of tidy results to use with bootstrap later\n  data.frame(\n    term = 'meanRD',\n    estimate = meanRD,\n    std.err = NA_real_)\n}\n\n\ngssrcc &lt;- gss[, c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")]\ngssrcc &lt;- gssrcc[complete.cases(gssrcc), ]\n\n\noutquant &lt;- fnc_prop_quant(gssrcc, outcome.name = \"trump\", exposure.name = \"gthsedu\", \n                       confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"),\n                       probs = 0:4/4)\noutquant\n\n    term   estimate std.err\n1 meanRD 0.03045919      NA\n\n\nWe obtain very similar results to those of the author’s. The diference in intervals is caused, as usual, by the fact that we the percentile method of interval rather than the normalized one (using 1.96 as critical value) as the author usually does.\n\ngssrcc.pquant &lt;- fciR::boot_est(\n    seed = 18181,\n    gssrcc, func = fnc_prop_quant,\n    outcome.name = \"trump\", \n    exposure.name = \"gthsedu\", \n    confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"),\n    probs = 0:4/4)\ngssrcc.pquant\n\n# A tibble: 1 × 6\n  term     .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 meanRD -0.00891    0.0372 0.0831   0.05 percentile\n\n\nand the histogram is\n\ngssrcc &lt;- gss[, c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")]\ngssrcc &lt;- gssrcc[complete.cases(gssrcc), ]\n\n\ngssrcc.propensity &lt;- \n  fciR::prop_scores(gssrcc, formula = gthsedu ~ magthsedu + white + female + gt65)\nassertthat::assert_that(all(!dplyr::near(gssrcc.propensity$scores, 0)), \n                        msg = \"Must be != zero.\")\n\n[1] TRUE\n\ngssrcc.out &lt;- data.frame(\n  gssrcc,\n  pscore = gssrcc.propensity$scores) |&gt;\n  mutate(pquant = cut(pscore, quantile(pscore, prob = 0:4/4), \n                      include.lowest = TRUE, ordered_result = TRUE,\n                      dig.lab = 3)) |&gt;\n  group_by(gthsedu, pquant) |&gt;\n  summarize(mean_out = mean(trump)) |&gt;\n  mutate(gthsedu = as.factor(gthsedu))\n# gssrcc.out\n\n\nggplot(gssrcc.out, aes(x = pquant, y = mean_out, fill = gthsedu)) +\n  geom_bar(position = \"dodge\", stat = \"identity\") +\n  scale_fill_manual(values = c(\"0\" = \"mediumorchid\", \"1\" = \"darksalmon\")) +\n  theme_classic() +\n  theme(legend.position = c(0.9, 0.9)) +\n  labs(title = \"Stratification by Quartiles of Propensity Score\",\n       x = \"Quartiles of propensity scores\",\n       y = \"Average potential outcome\")\n\n\n\n\nFigure 10.4: Stratification by Quartiles of Propensity Score"
  },
  {
    "objectID": "ch10_propensity.html#matching-on-the-propensity-score",
    "href": "ch10_propensity.html#matching-on-the-propensity-score",
    "title": "10  Propensity-Score Methods",
    "section": "10.4 Matching on the Propensity Score",
    "text": "10.4 Matching on the Propensity Score\n\ngssrcc.pscores &lt;- data.frame(\n  gssrcc,\n  pscore = gssrcc.propensity$scores)\n# str(gssrcc.pscores)\n\n\ngssrcc.match &lt;- Matching::Match(Y = gssrcc.pscores$trump, \n                                Tr = gssrcc.pscores$gthsedu,\n                                X = gssrcc.pscores$pscore,\n                                estimand = \"ATE\", caliper = 0.25,\n                                replace = TRUE, ties = FALSE)\nsummary(gssrcc.match)\n\n\nEstimate...  0.043358 \nSE.........  0.012614 \nT-stat.....  3.4372 \np.val......  0.00058786 \n\nOriginal number of observations..............  2168 \nOriginal number of treated obs...............  874 \nMatched number of observations...............  2168 \nMatched number of observations  (unweighted).  2168 \n\nCaliper (SDs)........................................   0.25 \nNumber of obs dropped by 'exact' or 'caliper'  0 \n\n\n\nMatching::MatchBalance(trump ~ magthsedu + white + female + gt65,\n                       data = gssrcc, match.out = gssrcc.match)\n\n\n***** (V1) magthsedu *****\n                       Before Matching       After Matching\nmean treatment........    0.18182           0.24493 \nmean control..........    0.26581           0.24493 \nstd mean diff.........    -21.756                 0 \n\nmean raw eQQ diff.....   0.083488                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.041995                 0 \nmed  eCDF diff........   0.041995                 0 \nmax  eCDF diff........   0.083989                 0 \n\nvar ratio (Tr/Co).....    0.76322                 1 \nT-test p-value........ 2.6725e-05                 1 \n\n\n***** (V2) white *****\n                       Before Matching       After Matching\nmean treatment........    0.95176            0.7214 \nmean control..........    0.64518            0.7214 \nstd mean diff.........     142.95                 0 \n\nmean raw eQQ diff.....    0.30798                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........    0.15329                 0 \nmed  eCDF diff........    0.15329                 0 \nmax  eCDF diff........    0.30658                 0 \n\nvar ratio (Tr/Co).....     0.2008                 1 \nT-test p-value........ &lt; 2.22e-16                 1 \n\n\n***** (V3) female *****\n                       Before Matching       After Matching\nmean treatment........    0.46939           0.55627 \nmean control..........    0.58502           0.55627 \nstd mean diff.........    -23.149                 0 \n\nmean raw eQQ diff.....    0.11503                 0 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 0 \n\nmean eCDF diff........   0.057817                 0 \nmed  eCDF diff........   0.057817                 0 \nmax  eCDF diff........    0.11563                 0 \n\nvar ratio (Tr/Co).....     1.0272                 1 \nT-test p-value........ 3.4011e-06                 1 \n\n\n***** (V4) gt65 *****\n                       Before Matching       After Matching\nmean treatment........    0.31725            0.2131 \nmean control..........    0.17864           0.21218 \nstd mean diff.........     29.756           0.22523 \n\nmean raw eQQ diff.....    0.13915        0.00092251 \nmed  raw eQQ diff.....          0                 0 \nmax  raw eQQ diff.....          1                 1 \n\nmean eCDF diff........   0.069308        0.00046125 \nmed  eCDF diff........   0.069308        0.00046125 \nmax  eCDF diff........    0.13862        0.00092251 \n\nvar ratio (Tr/Co).....     1.4781            1.0032 \nT-test p-value........ 6.9199e-10           0.15725 \n\n\nBefore Matching Minimum p.value: &lt; 2.22e-16 \nVariable Name(s): white  Number(s): 2 \n\nAfter Matching Minimum p.value: 0.15725 \nVariable Name(s): gt65  Number(s): 4"
  },
  {
    "objectID": "ch11_precision.html#theory",
    "href": "ch11_precision.html#theory",
    "title": "11  Precision Variables",
    "section": "11.1 Theory",
    "text": "11.1 Theory\n\nscm_11.1 &lt;- list()\nscm_11.1 &lt;- within(scm_11.1, {\n  coords &lt;- list(\n    x = c(`T` = 1, V = 1.5, Y = 3),\n    y = c(`T` = 1, V = 1.5, Y = 1))\n  dag &lt;- dagify(\n    Y ~ V + `T`,\n    coords = coords)\n\n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\nscm_11.2 &lt;- list()\nscm_11.2 &lt;- within(scm_11.2, {\n  coords &lt;- list(\n    x = c(`T` = 1.5, U = 1, V = 1.5, Y = 3),\n    y = c(`T` = 1, U = 1.5, V = 2, Y = 1))\n  dag &lt;- dagify(\n    V ~ U,\n    Y ~ U + `T`,\n    coords = coords)\n\n  plot &lt;- fciR::ggp_dag(dag)\n})\n\n\nscm_11.1$plot + scm_11.2$plot\n\n\n\n\nFigure 11.1: V is a Precision Variable for Estimating the Effect of T on Y"
  },
  {
    "objectID": "ch11_precision.html#examples",
    "href": "ch11_precision.html#examples",
    "title": "11  Precision Variables",
    "section": "11.2 Examples",
    "text": "11.2 Examples\nThis function computes the precision efficiency by comparing the linear fit with and without the precision variable.\n\nfnc_precision_eff &lt;- function(data, formula, exposure.name, precision.name,\n                              none.name = \"None\") {\n  # Estimate the effect without the precision variable\n  vars &lt;- all.vars(formula)\n  vars &lt;- vars[vars != precision.name]  # remove the precision variable\n  formula1 &lt;- paste(vars[[1]], vars[[2]], sep = \"~\")\n  fit_none &lt;- lm(formula = formula1, data = data)\n  # Estimate the effect with the precision variable\n  fit &lt;- lm(formula = formula, data = data)\n  est &lt;- c(coef(fit_none)[2], coef(fit)[2])\n  names(est) &lt;- c(none.name, precision.name)\n  std_err &lt;- c(sqrt(diag(vcov(fit_none)))[2], sqrt(diag(vcov(fit)))[2])\n  # must return a data.frame of tidy results to use with bootstrap later\n  data.frame(\n    term = names(est),\n    estimate = unname(est),\n    std.err = std_err\n  )\n}\n\nThis funciton compute the stats obtained from the fnc_precision_eff using bootstrapping and output them in a dataframe.\n\nfnc_precision_stats &lt;- function(data, models, times = 1000, none.name = \"None\") {\n  for (i in seq_along(models)) {\n    frml &lt;- models[[i]]$formula\n    vars &lt;- models[[i]]$vars\n    \n    # Get the fit for each model\n    fit &lt;- fnc_precision_eff(\n      data, formula = frml,\n      exposure.name = vars[\"exposure\"], precision.name = vars[\"precision\"],\n      none.name = none.name)\n    models[[i]]$fit &lt;- fit\n    \n    # bootstrap the models\n    boot &lt;- fciR::boot_est(\n      data = data,\n      func = fnc_precision_eff,\n      times = times, alpha = 0.05,\n      formula = frml,\n      exposure.name = vars[\"exposure\"], precision.name = vars[\"precision\"])\n    models[[i]]$boot &lt;- boot\n    \n    # create the output for each model\n    models[[i]]$out &lt;- fit |&gt;\n      bind_cols(boot[, c(\".lower\", \".upper\")])\n  }\n  # create the consolidated output\n  out.conso &lt;- lapply(seq_along(models), FUN = function(i) {\n    models[[i]]$out\n    })\n  out.conso &lt;- out.conso |&gt;\n    bind_rows() |&gt;\n    mutate(across(.cols = where(is.numeric), .fns = round, digits = 3)) |&gt;\n    # remove duplicate terms (\"None\")\n    slice(match(unique(term), term))\n  \n  list(\"stats\" = out.conso, \"models\" = models)\n  }\n\n\n11.2.1 CYT107 Effect\n\n\n11.2.2 Double What-If study\n\ndata(\"doublewhatifdat\", package = \"fciR\")\n\nCreate the list of models\n\ndoublewhatifdat.models &lt;- list(\n  \"Model VL0\" = list(\n    \"formula\" = VL1 ~ `T` + VL0,\n    \"vars\" = c(\"exposure\" = \"T\", \"precision\" = \"VL0\")),\n  \"Model AD0\" = list(\n    \"formula\" = VL1 ~ `T` + AD0,\n    \"vars\" = c(\"exposure\" = \"T\", \"precision\" = \"AD0\"))\n  )\n\nand get the stats using the above function\n\ndoublewhatifdat.prec &lt;- fnc_precision_stats(data = doublewhatifdat,\n                      models = doublewhatifdat.models, \n                      times = 500)\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(.cols = where(is.numeric), .fns = round, digits = 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\ndoublewhatifdat.prec$stats\n\n  term estimate std.err .lower .upper\n1 None   -0.147   0.031 -0.214 -0.084\n2  VL0   -0.146   0.030 -0.209 -0.083\n3  AD0   -0.151   0.026 -0.205 -0.083"
  },
  {
    "objectID": "ch12_mediation.html#theory",
    "href": "ch12_mediation.html#theory",
    "title": "12  Mediation",
    "section": "12.1 Theory",
    "text": "12.1 Theory\nWe let\n\n\\(M(a)\\) be the potential outcome of the mediator \\(M\\) to assigning treatment \\(A=a\\)\n\\(Y(a, m)\\) be the potential outcome for the outcome \\(Y\\) to assigning treatment \\(A=a\\) and mediator \\(M=m\\)\n\\(Y(a, M(a^*))\\) where we assign tretment \\(A=a\\) and then we assign the mediator \\(M\\) to its potential outcome had we assigned treatment \\(A=a^*\\)\n\nThe total effect \\(TE\\) compares the potential outcome for \\(Y\\) to assigning treatment \\(A=a\\) with that of assigning treatment \\(A=a\\).\n\\[\nTE(a,a^*) = Y(a, M(a)) - Y(a^*, M(a^*))\n\\]\nThe controlled direct effect \\(CDE\\) compares the potential outcome to assigning treatment \\(A=a\\) with that of assinging treatment \\(A=a^*\\) while controlling the mediator at \\(M=m\\).\n\\[\nCDE(a,a^*,m) = Y(a, m) - Y(a^*, m)\n\\]\nand the controlled indirect effect \\(CIE\\) is what is left to obtain the total effect \\(TE\\).\n\\[\nCIE(a, a^*,m)=TE(a,a^*) - CDE(a,a^*,m)\n\\]\nAlso another partition, the natural direct effect \\(NDE\\) does not control the mediator and lets it assume the value it would naturally take at a controlled level of \\(A\\).\n\\[\nNDE(a,a^*;a^*) = Y(a, M(a^*)) - Y(a^*, M(a^*))\n\\]\nand the natural indirect effect \\(NIE\\) compares the potential outcome to assigning \\(M=M(a)\\) with that of assinging \\(M=M(a^*)\\) while assigning \\(A=a\\).\n\\[\nNDE(a,a^*;a) = Y(a, M(a)) - Y(a, M(a^*))\n\\]\nand therefore the total effect \\(TE\\) can be expressed in two was\n\\[\nTE = CDE + CIE = NDE + NIE\n\\]\nWe make the four following assumptins about the confounders\nAssumption 1: There are no unmeasured treatment confounders given \\(H\\) where \\(H = \\{H_1, H_2, H_3 \\}\\).\n\\[\nY(a,m) \\perp\\!\\!\\!\\perp A \\mid H\n\\]\nAssumption 2: There are no unmeasured mediator-outcome confounders given \\(\\{H,A\\}\\)\n\\[\nY(a,m) \\perp\\!\\!\\!\\perp M \\mid H, A\n\\]\nAssumption 3: There are no unmeasured treatment-mediator confounders given \\(H\\)\n\\[\nM(a) \\perp\\!\\!\\!\\perp A \\mid H\n\\]\nAssumption 4: There are no mediator-outcome confounder affected by treatment, i.e. no arrow from \\(A\\) to \\(H_2\\).\n\\[\nY(a,m) \\perp\\!\\!\\!\\perp M(a^*) \\mid H\n\\]\n\n12.1.1 Prove \\(E(TE(a, a^*) \\mid H = h)\\)\nUnder assumption 1, the total effect conditional on \\(H\\) is\n\\[\nE(TE(a, a^*) \\mid H = h) = E(Y \\mid A=a, H=h) - E(Y \\mid A=a^*, H=h)\n\\]\nand the proof is is based on the method of section 3.2, p. 41, with assumption 1 \\(Y(a,m) \\perp\\!\\!\\!\\perp A \\mid H\\) being the same as the assumption of stratified randomized trial (3.2).\nBy definition we have that \\(TE(a, a^*) = Y(a, M(a)) - Y(a^*, M(a^*)\\), hence\n\\[\n\\begin{align*}\nE(TE(a, a^*) \\mid H = h) &= E(Y(a, M(a)) - Y(a^*, M(a^*)) \\mid H = h) \\\\\n&= E(Y(a, M(a)) \\mid H = h) - E(Y(a^*, M(a^*)) \\mid H = h)\n\\end{align*}\n\\]\nand by assumption 1\n\\[\n\\begin{align*}\n=E(Y(a, M(a)) \\mid A = a, H = h) - E(Y(a^*, M(a^*)) \\mid A = a^*, H = h) \\\\\n\\end{align*}\n\\]\nand by the assumption of consistency which implies that \\(E(Y(a, M(a)) \\mid A = a, H = h)= E(Y \\mid H = h, A = a)\\) and \\(E(Y(a, M(a)) \\mid A = a^*, H = h)= E(Y \\mid H = h, A = a^*)\\) we obtain\nBy definition we have that \\(TE(a, a^*) = Y(a, M(a)) - Y(a^*, M(a^*)\\), hence\n\\[\n\\begin{align*}\n=E(Y \\mid A = a, H = h) - E(Y \\mid A = a^*, H = h) \\\\\n\\end{align*}\n\\]\n\n\n12.1.2 Prove \\(E(CDE(a, a^*;m) \\mid H = h)\\)\nBy definition \\(CDE(a, a^*;m) = Y(a,m) - Y(a^*, m)\\) therefore\n\\[\n\\begin{align*}\nE(CDE(a, a^*;m) \\mid H = h) = E(Y(a,m) \\mid H = h) - E(Y(a^*, m) \\mid H = h)\n\\end{align*}\n\\]\nand by assumption 1\n\\[\n\\begin{align*}\n= E(Y(a,m) \\mid A = a, H = h) - E(Y(a^*, m) \\mid A = a^* H = h)\n\\end{align*}\n\\]\nby assumption 2\n\\[\n\\begin{align*}\n= E(Y(a,m) \\mid A = a, M = m, H = h) - E(Y(a^*, m) \\mid A = a^*, M = m, H = h)\n\\end{align*}\n\\]\nby assumption of consistency\n\\[\n\\begin{align*}\n= E(Y \\mid A = a, M = m, H = h) - E(Y \\mid A = a^*, M = m, H = h)\n\\end{align*}\n\\]\n\n\n12.1.3 Prove \\(E(NDE(a, a^* ; a^*) \\mid H = h)\\)\nBy definition \\(NDE(a, a^*;a^*) = Y(a,M(a^*) - Y(a^*, M(a^*)\\) so\n\\[\n\\begin{align*}\nE(NDE(a, a^*;a^*) \\mid H = h) = E(Y(a,M(a^*)) \\mid H = h) - E(Y(a^*, M(a^*)) \\mid H = h)\n\\end{align*}\n\\]\nnow for the part \\(E(Y(a,M(a^*)) \\mid H = h)\\) we have\n\\[\n\\begin{align*}\n\\text{by the law of total expectations} \\\\\nE(Y(a,M(a^*)) \\mid H = h) &= \\sum_m{E(Y(a,M(a^*))\\mid M(a^*)=m, H)P(M(a^*)=m \\mid H)} \\\\\n\\text{by consistency} \\\\\n&= \\sum_m{E(Y(a,m)\\mid M(a^*)=m, H)P(M(a^*)=m \\mid H)} \\\\\n\\text{by assumption 3} \\\\\n&= \\sum_m{E(Y(a,m)\\mid M(a^*)=m, H)P(M(a^*)=m \\mid A = a^*, H)} \\\\\n\\text{by assumption 4} \\\\\n&= \\sum_m{E(Y(a,m)\\mid H)P(M(a^*)=m \\mid A = a^*, H)} \\\\\n\\text{by assumption 1} \\\\\n&= \\sum_m{E(Y(a,m)\\mid A=a, H)P(M(a^*)=m \\mid A = a^*, H)} \\\\\n\\text{by consistency} \\\\\n&= \\sum_m{E(Y(a,m)\\mid A=a, H)P(M=m \\mid A = a^*, H)} \\\\\n\\text{by assumption 2} \\\\\n&= \\sum_m{E(Y(a,m)\\mid A=a, M=m, H)P(M=m \\mid A = a^*, H)} \\\\\n\\text{by consistency} \\\\\n&= \\sum_m{E(Y \\mid A=a, M=m, H)P(M=m \\mid A = a^*, H)} \\\\\n\\end{align*}\n\\]\nand we can do the same for the part \\(E(Y(a^*,M(a^*)) \\mid H = h)\\).\n\n\n12.1.4 Prove \\(E(NIE(a, a^* ; a^*) \\mid H = h)\\)\nBy definition \\(NIE(a, a^*;a^*) = Y(a,M(a)) - Y(a, M(a^*))\\) therefore\n\\[\n\\begin{align*}\nE(NIE(a, a^*;a^*) \\mid H = h) &= E(Y(a,M(a)) \\mid H = h) - E(Y(a, M(a^*)) \\mid H = h)\n\\end{align*}\n\\]\nand we can use the same method as for \\(NDE(a, a^*;a^*)\\) on each of the 2 right-hand side parts to prove the result.\nUsing \\(CDE\\) and \\(CIE\\) partition, we can estimate the proportion of the effect that would be eliminated if we fixed the mediator to a level \\(M=m\\).\n\\[\nPE = \\frac{TE-CDE(m)}{TE}\n\\]\nand using \\(NDE\\) and \\(NIE\\) partition we can estimate the proportion mediated as\n\\[\nPM =\\frac{NIE}{TE}\n\\]\nThe dimension of \\(H\\) will often require parametric methods. The parametric models in that case can be\n\\[\n\\begin{align*}\nE(Y \\mid A=a, M = m, H=h) &= \\beta_0 + \\beta_1 a+ \\beta_2 m + \\beta_3 a m + \\beta_4^T h \\\\\nE(M \\mid A = a, H = h) &= \\alpha_0 + \\alpha_1 a + \\alpha_2^T h\n\\end{align*}\n\\]\nand with assumptions 1 through 4 we combine the estimate to obtain\n\\[\n\\begin{align*}\nCDE(a, a^*;m) &= (\\beta_1 + \\beta_3 m)(a-a^*) \\\\\nNDE(a, a^*;a^*) &= (\\beta_1 + \\beta_3 (\\alpha_0 + \\alpha_1 a + \\alpha_2^T E(H)))(a-a^*) \\\\\nNIE(a, a^*;a) &= (\\beta_2 \\alpha_1 + \\beta_3 \\alpha_1 a)(a-a^*)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch12_mediation.html#traditional-parametric-methods",
    "href": "ch12_mediation.html#traditional-parametric-methods",
    "title": "12  Mediation",
    "section": "12.2 Traditional Parametric Methods",
    "text": "12.2 Traditional Parametric Methods\nTwo traditional parametric methods assume the parametric models from above but without the interaction term, that is \\(\\beta_3=0\\).\n\n12.2.1 Product method\nLetting \\(a=1\\) and \\(a^*=0\\), the \\(NIE\\) is estimated by \\(\\hat{\\beta_2} \\hat{\\alpha_1}\\).\n\n\n12.2.2 Difference method\nEstimate \\(NIE\\) by comparing the coefficient of \\(a\\) in\n\\[\n\\begin{align*}\nE(Y \\mid A=a, H=h) = \\beta_0 + \\beta_1 a+ \\beta_2 (\\alpha_0 +\\alpha_1 a + \\alpha_2^T h) + \\beta_4^T h\n\\end{align*}\n\\]\nwhich equals \\(\\beta_1 + \\beta_2 \\alpha_1\\) to the coeficient of \\(a\\) in\n\\[\n\\begin{align*}\nE(Y \\mid A=a, M=m, H=h) = \\beta_0 + \\beta_1 a + \\beta_2 m + \\beta_4^T h\n\\end{align*}\n\\]\nwhich equals \\(\\beta_1\\).\nThe idea is to see if the coefficient \\(\\beta_1 + \\beta_2 \\alpha_1\\) is significantly reduced by adding \\(M\\) to the regression.\n\n\n12.2.3 Example\n\ndata(\"gss\", package = \"fciR\")\ngssmed &lt;- gss |&gt;\n  dplyr::select(c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\",\n                  \"conservative\")) |&gt;\n  tidyr::drop_na()\nassertthat::assert_that(nrow(gssmed) == 2084)\n\n[1] TRUE\n\n\nand we use this function to compute the \\(NIE\\)\n\nfunc_mediation_NIE &lt;- function(data, formula, exposure.name, mediator.name, confound.names) {\n  outcome.name &lt;- all.vars(formula[[2]])\n  \n  # reduced formula, without exposure in input\n  exposure.confound.names &lt;- c(exposure.name, confound.names)\n  formula_red &lt;- as.formula(paste(outcome.name, \n                                  paste(exposure.confound.names, collapse = \"+\"), \n                                  sep = \"~\"))\n  # mediator formula\n  formula_med &lt;- as.formula(paste(mediator.name,\n                                  paste(exposure.confound.names, collapse = \"+\"), \n                                  sep = \"~\"))\n  \n  # the models\n  the_models &lt;- list(\n    \"full\" = formula,\n    \"reduced\" = formula_red,\n    \"mediator\" = formula_med)\n  \n  # the coefficients\n  the_coefs &lt;- lapply(the_models, function(x) {\n    glm(formula = x, data = data) |&gt;\n      coef()\n  }) |&gt;\n    setNames(names(the_models))\n  \n  \n  NIE_prod &lt;- the_coefs$full[mediator.name] * the_coefs$mediator[exposure.name]\n  \n  NIE_diff &lt;- the_coefs$reduced[exposure.name] - the_coefs$full[exposure.name]\n\n  msg &lt;- sprintf(\"NIE_prod of %f != NIE_diff of %f\", NIE_prod, NIE_diff)\n  assertthat::assert_that(dplyr::near(NIE_prod, NIE_diff), msg = msg)\n  \n  # output compatible with boostrap function\n  data.frame(\n    term = c(\"NIE_prod\", \"NIE_diff\"),\n    estimate = unname(c(NIE_prod, NIE_diff)),\n    std.err = NA_real_\n  )\n}\n\n\ngssmed.mediate_effect &lt;- func_mediation_NIE(gssmed, \n                                            formula = \n                                              trump ~ magthsedu + conservative,\n                                            exposure.name = \"magthsedu\", \n                                            mediator.name  = \"conservative\",\n                                            confound.names = NULL)\ngssmed.mediate_effect\n\n      term    estimate std.err\n1 NIE_prod -0.02238265      NA\n2 NIE_diff -0.02238265      NA\n\n\nand we evaluate the intervals with bootstrapping\n\ngssmed.mediate_effect &lt;- boot_est(data = gssmed, func = func_mediation_NIE,\n           times = 250, alpha = 0.05,\n           formula = trump ~ magthsedu + conservative, \n           exposure.name = \"magthsedu\", \n           mediator.name  = \"conservative\",\n           confound.names = NULL)\ngssmed.mediate_effect\n\n# A tibble: 2 × 6\n  term      .lower .estimate   .upper .alpha .method   \n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 NIE_diff -0.0393   -0.0230 -0.00587   0.05 percentile\n2 NIE_prod -0.0393   -0.0230 -0.00587   0.05 percentile"
  },
  {
    "objectID": "ch12_mediation.html#more-examples",
    "href": "ch12_mediation.html#more-examples",
    "title": "12  Mediation",
    "section": "12.3 More Examples",
    "text": "12.3 More Examples\nThis function is used to compute the final results by the func_mediation_np and func_mediation below.\n\nfunc_mediation_calc &lt;- function(NDE, NIE, CDE0, CDE1) {\n  # estimate the total effect\n  TE &lt;- NDE + NIE\n  assertthat::assert_that(!dplyr::near(TE, 0),\n                          msg = \"Total effect must not be zero.\")\n  # estimate the proportion mediated\n  PM &lt;- NIE / TE\n  # estimate the proportion eliminated setting M=1\n  PE1 &lt;- (TE - CDE1) / TE\n  # estimate the proportion eliminated setting M=0\n  PE0 &lt;- (TE - CDE0) / TE\n  \n  # output format can be used with bootstrap\n  data.frame(\n    term = c(\"TE\", \"PM\", \"PE(0)\", \"PE(1)\"),\n    estimate = c(TE, PM, PE0, PE1),\n    std.err = NA_real_\n  )\n}\n\n\n12.3.1 Mediation nonparametric\n\nfunc_mediation_np &lt;- function(data, formula, exposure.name, mediator.name,\n                           confound.names) {\n  # character vectors of names used below\n  outcome.name &lt;- all.vars(formula[[2]])\n  exposure.confound.names &lt;- c(exposure.name, confound.names)\n  mediator.confound.names &lt;- c(mediator.name, confound.names)\n  input.names &lt;- c(exposure.name, mediator.name, confound.names)\n  all.names &lt;- c(outcome.name, exposure.name, mediator.name, confound.names)\n  \n  # P(H)\n  probH &lt;- data |&gt;\n    fciR::calc_prob(var.name = confound.names, prob.name = \"probH\")\n  assertthat::assert_that(sum(probH$probH) == 1,\n                          msg = \"Sum of prob. H must equal 1.\")\n  \n  # P(M = m | A, H)\n  probMcondAH &lt;- data |&gt;\n    fciR::calc_prob_cond(condition.names = exposure.confound.names, \n                        var.name = mediator.name, \n                        prob.name = \"probM\") |&gt;\n    tidyr::pivot_wider(names_from = exposure.name, values_from = \"probM\",\n                       names_prefix = \"probMA\") |&gt;\n    # add P(H) to th data, it will be used later for standardization by H\n    dplyr::inner_join(y = probH, by = confound.names)\n  \n  # E(Y | A, M, H)\n  expYcondAMH &lt;- data |&gt;\n    group_by(across(all_of(input.names))) |&gt;\n    summarize(expYcondAMH = mean(.data[[outcome.name]])) |&gt;\n    tidyr::pivot_wider(names_from = exposure.name, values_from = expYcondAMH,\n                       names_prefix = \"EYA\")\n  \n  # NOTE: This is the final dataframe used to make all computations\n  #       It is an important dataframe when debuging\n  processed_data &lt;- inner_join(x = expYcondAMH, y = probMcondAH,\n                    by = mediator.confound.names) |&gt;\n    ungroup()\n  \n  NDE &lt;- processed_data |&gt;\n    mutate(NDEH = (EYA1 - EYA0) * probMA0) |&gt;\n    # standardize over H\n    summarize(NDE = sum(NDEH * probH)) |&gt;\n    pull()\n\n  NIE &lt;- processed_data |&gt;\n    mutate(NIEH = EYA1 * (probMA1 - probMA0)) |&gt;\n    # standardize over H\n    summarize(NIE = sum(NIEH * probH)) |&gt;\n    pull()\n\n  CDE &lt;- processed_data |&gt;\n    group_by(across(all_of(mediator.name))) |&gt;\n    summarize(CDE = sum((EYA1 - EYA0) * probH)) |&gt;\n    mutate(term = paste0(\"CDE(\", .data[[mediator.name]], \")\"))\n  \n  # calculated variables\n  calc &lt;- func_mediation_calc(NDE = NDE, NIE = NIE, \n                              CDE0 = CDE$CDE[CDE$term == \"CDE(0)\"], \n                              CDE1 = CDE$CDE[CDE$term == \"CDE(1)\"])\n  \n  # output format can be used with bootstrap\n  out &lt;- data.frame(\n    term = c(\"NDE\", \"NIE\", CDE$term),\n    estimate = c(NDE, NIE, CDE$CDE),\n    std.err = NA_real_) |&gt;\n    dplyr::bind_rows(calc)\n  # relocate the columns\n  the_terms &lt;- c(\"TE\", \"CDE(0)\", \"CDE(1)\", \"NDE\", \"NIE\", \"PE(0)\", \"PE(1)\", \"PM\")\n  pos &lt;- match(the_terms, out$term)\n  out &lt;- out[pos, ]\n  out\n}\n\n\ndata(\"doublewhatifdat\", package = \"fciR\")\n\n\ndwhatif.med.np &lt;- func_mediation_np(doublewhatifdat,\n                                formula = VL1 ~ `T` + A + AD0,\n                                exposure.name = \"T\", \n                                mediator.name = \"A\",\n                                confound.names = c(\"AD0\"))\n\nWarning: Using an external vector in selections was deprecated in tidyselect 1.1.0.\nℹ Please use `all_of()` or `any_of()` instead.\n  # Was:\n  data %&gt;% select(exposure.name)\n\n  # Now:\n  data %&gt;% select(all_of(exposure.name))\n\nSee &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.\n\n# relocate the columns\ndwhatif.med.np\n\n    term    estimate std.err\n5     TE -0.15147635      NA\n3 CDE(0) -0.04240090      NA\n4 CDE(1)  0.04705889      NA\n1    NDE -0.03552486      NA\n2    NIE -0.11595150      NA\n7  PE(0)  0.72008238      NA\n8  PE(1)  1.31066825      NA\n6     PM  0.76547589      NA\n\n\n\n\n12.3.2 Mediation parametric\n\n# estimate natural direct effect (NDE) and natural indirect effect (NIE)\nfunc_mediation &lt;- function(data, formula, exposure.name, mediator.name,\n                           confound.names) {\n  # character vectors of names used below\n  outcome.name &lt;- all.vars(formula[[2]])\n  exposure.confound.names &lt;- c(exposure.name, confound.names)\n  input.names &lt;- c(exposure.name, mediator.name, confound.names)\n\n  # fit a saturated model including the mediator\n  formula_out &lt;- paste(outcome.name, paste(input.names, collapse = \"*\"), \n                       sep = \"~\")\n  fit_out &lt;- glm(formula = formula_out, family = \"binomial\", data = data)\n  # fit a saturated model for the mediator\n  formula_med &lt;- paste(mediator.name, paste(exposure.confound.names, collapse = \"*\"), \n                       sep = \"~\")\n  fit_med &lt;- glm(formula = formula_med, family = \"binomial\", data = data)\n  \n  # create new data sets\n  newdat &lt;- list()\n  newdat$A0M0 &lt;- data |&gt;\n    mutate(\n      !!exposure.name := 0L,\n      !!mediator.name := 0L)\n  newdat$A1M0 &lt;- data |&gt;\n    mutate(\n      !!exposure.name := 1L,\n      !!mediator.name := 0L)\n  newdat$A0M1 &lt;- data |&gt;\n    mutate(\n      !!exposure.name := 0L,\n      !!mediator.name := 1L)\n  newdat$A1M1 &lt;- data |&gt;\n    mutate(\n      !!exposure.name := 1L,\n      !!mediator.name := 1L)\n  \n  # estimate the first term  of equation (1) as a funciton of H\n  EY1M0H &lt;- \n    predict(fit_out, newdata = newdat$A1M0, type = \"response\") *\n    (1 - predict(fit_med, newdata = newdat$A0M0, type = \"response\")) +\n    predict(fit_out, newdata = newdat$A1M1, type = \"response\") *\n    predict(fit_med, newdata = newdat$A0M1, type = \"response\")\n  # standardize with respect to marginal distribution of H\n  EY1M0 &lt;- mean(EY1M0H)\n  # estimate the second term  of equation (1) as a funciton of H\n  EY0M0H &lt;- \n    predict(fit_out, newdata = newdat$A0M0, type = \"response\") *\n    (1 - predict(fit_med, newdata = newdat$A0M0, type = \"response\")) +\n    predict(fit_out, newdata = newdat$A0M1, type = \"response\") *\n    predict(fit_med, newdata = newdat$A0M1, type = \"response\")\n  # standardize with respect to marginal distribution of H\n  EY0M0 &lt;- mean(EY0M0H)\n    # estimate the first term  of equation (2) as a funciton of H\n  EY1M1H &lt;- \n    predict(fit_out, newdata = newdat$A1M0, type = \"response\") *\n    (1 - predict(fit_med, newdata = newdat$A1M0, type = \"response\")) +\n    predict(fit_out, newdata = newdat$A1M1, type = \"response\") *\n    predict(fit_med, newdata = newdat$A1M1, type = \"response\")\n  # standardize with respect to marginal distribution of H\n  EY1M1 &lt;- mean(EY1M1H)\n  \n  NDE &lt;- EY1M0 - EY0M0\n  NIE &lt;- EY1M1 - EY1M0\n  \n  # estimate the controlled direct effects\n  CDE1H &lt;- predict(fit_out, newdata = newdat$A1M1, type = \"response\") -\n    predict(fit_out, newdata = newdat$A0M1, type = \"response\")\n  # standardize with respect to marginal distribution of H\n  CDE1 &lt;- mean(CDE1H)\n  \n  # estimate the controlled direct effects\n  CDE0H &lt;- predict(fit_out, newdata = newdat$A1M0, type = \"response\") -\n    predict(fit_out, newdata = newdat$A0M0, type = \"response\")\n  # standardize with respect to marginal distribution of H\n  CDE0 &lt;- mean(CDE0H)\n  \n    # calculated variables\n  calc &lt;- func_mediation_calc(NDE = NDE, NIE = NIE, CDE0 = CDE0, CDE1 = CDE1)\n  \n  # output format can be used with bootstrap\n  out &lt;- data.frame(\n    term = c(\"NDE\", \"NIE\", \"CDE(0)\", \"CDE(1)\"),\n    estimate = c(NDE, NIE, CDE0, CDE1),\n    std.err = NA_real_) |&gt;\n    dplyr::bind_rows(calc)\n  # relocate the columns\n  the_terms &lt;- c(\"TE\", \"CDE(0)\", \"CDE(1)\", \"NDE\", \"NIE\", \"PE(0)\", \"PE(1)\", \"PM\")\n  pos &lt;- match(the_terms, out$term)\n  out &lt;- out[pos, ]\n  out\n}\n\n\n# estimate controlled direct effect\ndwhatif.med &lt;- func_mediation(doublewhatifdat,\n                                formula = VL1 ~ `T` + A + AD0,\n                                exposure.name = \"T\", \n                                mediator.name = \"A\",\n                                confound.names = c(\"AD0\"))\ndwhatif.med\n\n    term    estimate std.err\n5     TE -0.15147635      NA\n3 CDE(0) -0.04240090      NA\n4 CDE(1)  0.04705890      NA\n1    NDE -0.03552485      NA\n2    NIE -0.11595149      NA\n7  PE(0)  0.72008239      NA\n8  PE(1)  1.31066828      NA\n6     PM  0.76547591      NA\n\n\n\n\n12.3.3 simulation simmed\n\nsimmed &lt;- function(n = 1000, seed = 44444) {\n  set.seed(seed)\n  A &lt;- rbinom(n, size = 1, prob = 0.5)\n  H &lt;- rbinom(n, size = 1, prob = 0.5)\n  # let the mediator increase with A and/or H\n  tmppm &lt;- A + H\n  pm &lt;- exp(tmppm) / (1 + exp(tmppm))\n  M &lt;- rbinom(n, size = 1, prob = pm)\n  # let the outcome increase with A and/or H but dcrease with M\n  tmppy &lt;- H - M + A\n  py &lt;- exp(tmppy) / (1 + exp(tmppy))\n  Y &lt;- rbinom(n, size = 1, prob = py)\n  data.frame(\n    \"H\" = H,\n    \"A\" = A,\n    \"M\" = M,\n    \"Y\" = Y\n  )\n}\n\n\nsimmeddat &lt;- simmed()\n\n\n# estimate controlled direct effect\nsimmed.np &lt;- func_mediation_np(simmeddat,\n                                formula = Y ~ A + M + H,\n                                exposure.name = \"A\", \n                                mediator.name = \"M\",\n                                confound.names = \"H\")\nsimmed.np\n\n    term    estimate std.err\n5     TE  0.15979086      NA\n3 CDE(0)  0.27923100      NA\n4 CDE(1)  0.16768035      NA\n1    NDE  0.20900534      NA\n2    NIE -0.04921448      NA\n7  PE(0) -0.74747792      NA\n8  PE(1) -0.04937384      NA\n6     PM -0.30799310      NA\n\n\n\nthe_terms &lt;- c(\"TE\", \"CDE(0)\", \"CDE(1)\", \"NDE\", \"NIE\", \"PE(0)\", \"PE(1)\", \"PM\")\nsimmed.np.boot &lt;- boot_est(data = simmeddat, func = func_mediation_np,\n           times = 500, alpha = 0.05,\n           terms = the_terms,\n           formula = Y ~ A + M + H, \n           exposure.name = \"A\",\n           mediator.name = \"M\",\n           confound.names = \"H\")\n\n\nfciR::gt_measures(simmed.np.boot,\n                  title = \"Table 12.3\",\n                  subtitle = paste(\"Nonparametric Mediation Analysis of the\",\n                                   \"data generated by &lt;em&gt;simmed.r&lt;/em&gt;\",\n                                   sep = \"&lt;br&gt;\"))\n\n\n\n\n\nTable 12.1:  Table 12.3 \n  \n    \n      Table 12.3\n    \n    \n      Nonparametric Mediation Analysis of thedata generated by simmed.r\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    TE\n0.158\n(0.099, 0.219)\n    CDE(0)\n0.277\n(0.183, 0.368)\n    CDE(1)\n0.167\n(0.099, 0.242)\n    NDE\n0.208\n(0.154, 0.265)\n    NIE\n-0.049\n(-0.072, -0.029)\n    PE(0)\n-0.786\n(-1.605, -0.18)\n    PE(1)\n-0.058\n(-0.327, 0.2)\n    PM\n-0.329\n(-0.595, -0.151)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval"
  },
  {
    "objectID": "ch13_time.html#marginal-structural-models",
    "href": "ch13_time.html#marginal-structural-models",
    "title": "13  Time-Dependent Confounding",
    "section": "13.1 Marginal Structural Models",
    "text": "13.1 Marginal Structural Models\nMarginal structural models require consistency of four potential outcomes \\(Y(a_1, a_2)\\) for \\(a_1 = 0, 1\\) and \\(a_2 = 0, 1\\).\nSequential randomization is assumed, so\n\\[\nA_1 \\perp\\!\\!\\!\\perp Y(a_1, a_2)\n\\]\nand\n\\[\nA_2 \\perp\\!\\!\\!\\perp Y(a_1, a_2) \\mid A_1, H_2\n\\]\nTo estimate the parameter \\(\\beta = (\\beta_0, \\beta_1 a_1, \\beta_2 a_2, \\beta_3 a_1 a_2)\\) we use the following\n\\[\n\\begin{align*}\n\\text{Given} \\, A_1 \\perp\\!\\!\\!\\perp Y(a_1, a_2) \\\\\nE(Y(a_1, a_2)) &= E(Y(a_1, a_2) \\mid A_1=a_1) \\\\\n\\text{and using double expectation theorem} \\\\\n&= E_{H_2 \\mid A_1 = a_1}(E(Y(a_1, a_2) \\mid A_1=a_1, H_2)) \\\\\n\\text{and since } \\, A_2 \\perp\\!\\!\\!\\perp Y(a_1, a_2) \\mid A_1, H_2 \\\\\n&= E_{H_2 \\mid A_1 = a_1}(E(Y(a_1, a_2) \\mid A_1=a_1, H_2, A_2 = a_2)) \\\\\n\\text{and by consistency} \\\\\n&= E_{H_2 \\mid A_1 = a_1}(E(Y \\mid A_1=a_1, H_2, A_2 = a_2)) \\\\\n\\end{align*}\n\\]\n\nfunc_time_msm &lt;- function(data, outcome.name, exposure.names, confound.names) {\n  # exposure names must be in sequential order as in c(\"A!\", \"A2\")\n  stopifnot(length(exposure.names) == 2)\n  \n  # estimate the probability of treatment at time 2\n  eformula &lt;- paste(exposure.names[2],\n                    paste(c(exposure.names[1], confound.names), collapse = \"*\"),\n                    sep = \"~\")\n  eA1H &lt;- fitted(glm(formula = eformula, data = data, family = \"binomial\"))\n  # estimate the weights\n  datA2 &lt;- data[, exposure.names[2]]\n  wghts &lt;- (eA1H * datA2 + (1 - eA1H) * (1 - datA2))\n  inv_wghts &lt;- 1 / unname(wghts)\n  # IMPORTANT: Must add the weights to data to avoid error \n  #            \"object 'inv_wghts' not found\".\n  data$inv_wghts &lt;- inv_wghts\n  # fit the marginal structural model\n  msmformula &lt;- paste(outcome.name,\n                      paste(exposure.names, collapse = \"*\"),\n                      sep = \"~\")\n  msm_fit &lt;- glm(formula = msmformula, data = data, family = \"gaussian\",\n                 weights = inv_wghts)\n  coefs &lt;- coef(msm_fit)\n  # return contrasts of the MSM parameters\n  out &lt;- c(coefs,\n           sum(coefs[c(\"A1\", \"A1:A2\")]),\n           sum(coefs[c(\"A2\", \"A1:A2\")]),\n           sum(coefs[coefs != \"(Intercept)\"]))\n  names(out) &lt;- c(paste(\"beta\", seq_along(coefs) - 1), \n                  \"beta 1+3\", \"beta 2+3\", \"beta 1+2+3\")\n  # output compatible with boostrap function\n  data.frame(\n    term = names(out),\n    estimate = unname(out),\n    std.err = NA_real_)\n}\n\n\ndata(\"cogdat\", package = \"fciR\")\n\n\ncogdat.msm &lt;- func_time_msm(cogdat, outcome.name = \"Y\",\n                            exposure.names = c(\"A1\", \"A2\"),\n                            confound.names = \"H2\") |&gt;\n  mutate(estimate = round(estimate, 3))\n  \ncogdat.msm\n\n        term estimate std.err\n1     beta 0    0.261      NA\n2     beta 1   -0.008      NA\n3     beta 2   -0.060      NA\n4     beta 3    0.208      NA\n5   beta 1+3    0.199      NA\n6   beta 2+3    0.148      NA\n7 beta 1+2+3    0.401      NA"
  },
  {
    "objectID": "ch13_time.html#structural-nested-mean-models",
    "href": "ch13_time.html#structural-nested-mean-models",
    "title": "13  Time-Dependent Confounding",
    "section": "13.2 Structural Nested Mean Models",
    "text": "13.2 Structural Nested Mean Models\nStructural nested mean models require consistency of two potential outcomes: \\(Y_1\\) is the potential outcome to treatment with the observed \\(A_1\\) and then treatment with \\(A_2 = 0\\), \\(Y_0\\) is the potential outcome to \\(A_1 = 0\\) followed by \\(A_2 = 0\\).\nSequential randomization is assumed, so\n\\[\nA_1 \\perp\\!\\!\\!\\perp Y(a_1, a_2)\n\\]\nand\n\\[\nA_2 \\perp\\!\\!\\!\\perp Y(a_1, a_2) \\mid A_1, H_2\n\\]\n\nfunc_time_snmm &lt;- function(data, outcome.name, exposure.names, confound.names) {\n  # exposure names must be in sequential order as in c(\"A!\", \"A2\")\n  stopifnot(length(exposure.names) == 2)\n  \n  # fit the saturated model for the second level of the nest\n  nest2_forml &lt;- paste(outcome.name,\n                         paste(c(exposure.names, confound.names), \n                               collapse = \"*\"),\n                         sep = \"~\")\n  # nest2_forml_old &lt;- \"Y ~ A1 + H2 + A2 + A1 * H2 + A1 * A2 + H2 * A2 + A1 * H2 * A2\"\n  nest2_fit &lt;- lm(formula = nest2_forml, data = data)\n  b2 &lt;- coef(nest2_fit)\n  \n  # estimate the potential outcome of setting A2 to zero\n  datY &lt;- data[, outcome.name]\n  datA1 &lt;- data[, exposure.names[1]]\n  datA2 &lt;- data[, exposure.names[2]]\n  datH2 &lt;- data[, confound.names]\n  Y1hat &lt;- datY - \n    b2[\"A2\"] * datA2 - \n    b2[\"A1:A2\"] * datA1 * datA2 -\n    b2[\"A2:H2\"] * datH2 * datA2 -\n    b2[\"A1:A2:H2\"] * datA1 * datH2 * datA2\n  \n  # fit the model for the first level of the nest\n  nest1_fit &lt;- lm(Y1hat ~ datA1)\n  b1 &lt;- coef(nest1_fit)[2]\n  out &lt;- c(\"B20\" = unname(b2[\"A2\"]), \n           \"B20+B22\" = sum(b2[c(\"A2\", \"A2:H2\")]), \n           \"B20+B21\" = sum(b2[c(\"A2\", \"A1:A2\")]),\n           \"B20+B21+B22+B23\" = sum(b2[c(\"A2\", \"A1:A2\", \"A2:H2\", \"A1:A2:H2\")]),\n           \"B1\" = unname(b1))\n  # output compatible with boostrap function\n  data.frame(\n    term = names(out),\n    estimate = unname(out),\n    std.err = NA_real_)\n  }\n\n\ncogdat.snmm &lt;- func_time_snmm(cogdat, outcome.name = \"Y\", \n                            exposure.names = c(\"A1\", \"A2\"), \n                            confound.names = \"H2\")\ncogdat.snmm\n\n             term     estimate std.err\n1             B20 -0.292682927      NA\n2         B20+B22  0.479166667      NA\n3         B20+B21  0.392857143      NA\n4 B20+B21+B22+B23 -0.135338346      NA\n5              B1 -0.008074411      NA"
  },
  {
    "objectID": "ch13_time.html#optimal-dynamic-treatment-regimes",
    "href": "ch13_time.html#optimal-dynamic-treatment-regimes",
    "title": "13  Time-Dependent Confounding",
    "section": "13.3 Optimal Dynamic Treatment Regimes",
    "text": "13.3 Optimal Dynamic Treatment Regimes\nThis script is called mkcogtab.r in text\n\n# this is called mkcogtab.r in text\nfunc_time_odtr_prop &lt;- function(data, outcome.name, A1, A2, H2) {\n  input.names &lt;- c(A1, A2, H2)\n  data |&gt;\n    dplyr::group_by(across(all_of(input.names))) |&gt;\n    dplyr::summarize(\n      freq = n(),\n      freqy = sum(.data[[outcome.name]]),\n      prop = freqy / freq)\n  }\n\n\ncogdat.tab &lt;- func_time_odtr_prop(cogdat, outcome.name = \"Y\", \n                        A1 = \"A1\", A2 = \"A2\", H2 = \"H2\")\ncogdat.tab\n\n# A tibble: 8 × 6\n# Groups:   A1, A2 [4]\n     A1    A2    H2  freq freqy  prop\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0     0   410   120 0.293\n2     0     0     1   160    30 0.188\n3     0     1     0    30     0 0    \n4     0     1     1    30    20 0.667\n5     1     0     0   280    30 0.107\n6     1     0     1   190    80 0.421\n7     1     1     0    20    10 0.5  \n8     1     1     1    70    20 0.286\n\n\n\nfunc_time_odtr_optA2 &lt;- function(data, A1 = \"A1\", A2 = \"A2\", H2 = \"H2\") {\n  input.names = c(A1, H2)\n  data |&gt;\n    group_by(across(all_of(input.names))) |&gt;\n    mutate(propA2opt = max(prop),\n           A2opt = .data[[A2]][match(propA2opt, prop)]) |&gt;\n    dplyr::relocate(propA2opt, .after = tidyselect::last_col())\n}\n\n\ncogdat.optA2 &lt;- func_time_odtr_optA2(cogdat.tab, A2 = \"A2\", A1 = \"A1\", H2 = \"H2\")\ncogdat.optA2\n\n# A tibble: 8 × 8\n# Groups:   A1, H2 [4]\n     A1    A2    H2  freq freqy  prop A2opt propA2opt\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1     0     0     0   410   120 0.293     0     0.293\n2     0     0     1   160    30 0.188     1     0.667\n3     0     1     0    30     0 0         0     0.293\n4     0     1     1    30    20 0.667     1     0.667\n5     1     0     0   280    30 0.107     1     0.5  \n6     1     0     1   190    80 0.421     0     0.421\n7     1     1     0    20    10 0.5       1     0.5  \n8     1     1     1    70    20 0.286     0     0.421\n\n\n\nfunc_time_odtr_optA1A2 &lt;- function(data, A1 = \"A1\", A2 = \"A2\", H2 = \"H2\")  {\n  optA1A2 &lt;- data |&gt;\n    group_by(.data[[A1]]) |&gt;\n    mutate(nA1 = sum(freq)) |&gt;\n    group_by(across(all_of(c(A1, H2)))) |&gt;\n    mutate(nA1H2 = sum(freq),\n           propA1H2 = nA1H2 / nA1,\n           margA1H2 = propA2opt * propA1H2) |&gt;\n    group_by(across(all_of(c(A1, A2)))) |&gt;\n    summarize(probA1A2 = sum(margA1H2)) |&gt;\n    distinct(.data[[A1]], probA1A2) |&gt;\n    ungroup() |&gt;\n    filter(probA1A2 == max(probA1A2)) |&gt;\n    rename(A1opt = A1,\n           propA1opt = probA1A2) |&gt;\n    identity()\n  optA1A2\n  \n  optA1A2_repeat &lt;- \n    bind_rows(replicate(nrow(data), optA1A2, simplify = FALSE))\n  \n  data |&gt;\n    bind_cols(optA1A2_repeat) |&gt;\n    identity()\n}\n\n\ncogdat.optA1A2 &lt;- func_time_odtr_optA1A2(cogdat.optA2, \n                                    A1 = \"A1\", A2 = \"A2\", H2 = \"H2\")\ncogdat.optA1A2\n\n# A tibble: 8 × 10\n# Groups:   A1, H2 [4]\n     A1    A2    H2  freq freqy  prop A2opt propA2opt A1opt propA1opt\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1     0     0     0   410   120 0.293     0     0.293     1     0.463\n2     0     0     1   160    30 0.188     1     0.667     1     0.463\n3     0     1     0    30     0 0         0     0.293     1     0.463\n4     0     1     1    30    20 0.667     1     0.667     1     0.463\n5     1     0     0   280    30 0.107     1     0.5       1     0.463\n6     1     0     1   190    80 0.421     0     0.421     1     0.463\n7     1     1     0    20    10 0.5       1     0.5       1     0.463\n8     1     1     1    70    20 0.286     0     0.421     1     0.463\n\n\n\nfunc_time_odtr_optimal &lt;- function(data, A1 = \"A1\", H2 = \"H2\") {\n  the_A1opt &lt;- data$A1opt[1]\n  \n  df &lt;- data |&gt;\n    filter(.data[[A1]] == the_A1opt) |&gt;\n    distinct(A1opt, propA1opt, .data[[A1]], .data[[H2]], A2opt, propA2opt)\n  \n  out &lt;- c(\n    \"A1opt\" = df$A1opt[1],\n    \"propA1opt\" = df$propA1opt[1],\n    \"A2optH20\" = df$A2opt[df[, H2] == 0],\n    \"propA2optH20\" = df$propA2opt[df[, H2] == 0],\n    \"A2optH21\" = df$A2opt[df[, H2] == 1],\n    \"propA2optH21\" = df$propA2opt[df[, H2] == 1]\n  )\n  # output compatible with boostrap function\n  data.frame(\n    term = names(out),\n    estimate = unname(out),\n    std.err = NA_real_)\n}\n\n\ncogdat.opt &lt;- func_time_odtr_optimal(cogdat.optA1A2, A1 = \"A1\", H2 = \"H2\")\ncogdat.opt\n\n          term  estimate std.err\n1        A1opt 1.0000000      NA\n2    propA1opt 0.4633459      NA\n3     A2optH20 1.0000000      NA\n4 propA2optH20 0.5000000      NA\n5     A2optH21 0.0000000      NA\n6 propA2optH21 0.4210526      NA\n\ncogdat.check &lt;- c(\"A1opt\" = 1, \"propA1opt\" = 0.4633459,\n                  \"A2optH20\" = 1, \"propA2optH20\" = 0.5,\n                  \"A2optH21\" = 0, \"propA2optH21\" = 0.4210526)\nassertthat::assert_that(all(abs(cogdat.opt$estimate - cogdat.check) &lt; 1e-6),\n                        msg = \"results must match text.\")\n\n[1] TRUE\n\n\nWe can do the whole thing with pipes in a wrapper function. We will use it with boostraping.\n\nfunc_time_odtr &lt;- function(data, outcome.name = \"Y\", \n                          A1 = \"A1\", A2 = \"A2\", H2 = \"H2\") {\n  data |&gt;\n    func_time_odtr_prop(outcome.name = outcome.name, A1 = A1, A2 = A2, H2 = H2) |&gt;\n    func_time_odtr_optA2(A2 = A2, A1 = A1, H2 = H2) |&gt;\n    func_time_odtr_optA1A2(A1 = A1, A2 = A2, H2 = H2) |&gt;\n    func_time_odtr_optimal(A1 = A1, H2 = H2)\n}\n\n\ncogdat.opt &lt;- func_time_odtr(cogdat, outcome.name = \"Y\", \n                            A1 = \"A1\", A2 = \"A2\", H2 = \"H2\")\nassertthat::assert_that(all(abs(cogdat.opt$estimate - cogdat.check) &lt; 1e-6),\n                        msg = \"results must match text.\")\n\n[1] TRUE\n\ncogdat.opt\n\n          term  estimate std.err\n1        A1opt 1.0000000      NA\n2    propA1opt 0.4633459      NA\n3     A2optH20 1.0000000      NA\n4 propA2optH20 0.5000000      NA\n5     A2optH21 0.0000000      NA\n6 propA2optH21 0.4210526      NA\n\n\n\ncogdat.boot &lt;- cogdat |&gt;\n    rsample::bootstraps(times = 1000, apparent = FALSE) |&gt;\n    mutate(results = purrr::map(.data$splits, function(df) {\n      dat &lt;- rsample::analysis(df)\n      func_time_odtr(dat, outcome.name = \"Y\", A1 = \"A1\", A2 = \"A2\", H2 = \"H2\")\n      })) |&gt;\n    rsample::int_pctl(.data$results, alpha = 0.05) |&gt;\n    suppressWarnings()\n\n\ncogdat.boot\n\n# A tibble: 6 × 6\n  term         .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 A1opt         0         0.792  1       0.05 percentile\n2 A2optH20      0         0.792  1       0.05 percentile\n3 A2optH21      0         0.222  1       0.05 percentile\n4 propA1opt     0.385     0.472  0.592   0.05 percentile\n5 propA2optH20  0.272     0.486  0.722   0.05 percentile\n6 propA2optH21  0.355     0.484  0.793   0.05 percentile\n\n\nThe results are similar to those obtained in the book except for the following:\n\n\\(propA2optH21\\) has a narrower range than in the book. This is cause probably caused by the fact that the author compute the variances using 1.96 (normality). Throughout the book, the range obtained by rsample::int_pctl has usually been shorter\n\\(propA2optH21\\) is very similar to \\(propA2optH20\\) and therefore we do not conclude as he author does that the survival probability decreases if \\(H_2=1\\).\n\nImportant note: When using the estimate, without boostraping, we get the exact same estimate as the author! Therefore the calculations are correct.\n\ncogdat.opt\n\n          term  estimate std.err\n1        A1opt 1.0000000      NA\n2    propA1opt 0.4633459      NA\n3     A2optH20 1.0000000      NA\n4 propA2optH20 0.5000000      NA\n5     A2optH21 0.0000000      NA\n6 propA2optH21 0.4210526      NA\n\n\nSo which one is right? The boostraped estimates or the calculated ones? I vote for the boostraped results."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brumback, Babette A. 2022. Fundamentals of Causal Inference with\nr. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and\nRandom Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP:\nOxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning\nEffect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app01_errata.html#preface",
    "href": "app01_errata.html#preface",
    "title": "Appendix A — Errata",
    "section": "A.1 Preface",
    "text": "A.1 Preface\npage xi, last word of first paragraph is standaridzation, s/b standardization"
  },
  {
    "objectID": "app01_errata.html#chapter-1",
    "href": "app01_errata.html#chapter-1",
    "title": "Appendix A — Errata",
    "section": "A.2 Chapter 1",
    "text": "A.2 Chapter 1\n\nA.2.1 Section 1.2.3.2, p. 11\nThe sentence of the 6th line on top of the page is\n\nWe simulated the data according to the hyothetical\n\nShould be hypothetical."
  },
  {
    "objectID": "app01_errata.html#chapter-2",
    "href": "app01_errata.html#chapter-2",
    "title": "Appendix A — Errata",
    "section": "A.3 Chapter 2",
    "text": "A.3 Chapter 2\n\nA.3.1 section 2.4 Compute the standard error\nThe standard error formula used is with the standard deviation of the population and equivalent to\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) * sqrt(n-1) / n\n\nwhen the correct definition is with the standard deviation of the sample\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) / sqrt(n)\n\n\n\nA.3.2 Figure 2.1, p. 30\nThis is really a small detail. The caption of the bottom plot is \\(\\hat{E_{np}}(Y \\mid A= 1, H =1, T = 1)\\), s/b \\(\\hat{E}_{np}\\)"
  },
  {
    "objectID": "app01_errata.html#chapter-3",
    "href": "app01_errata.html#chapter-3",
    "title": "Appendix A — Errata",
    "section": "A.4 Chapter 3",
    "text": "A.4 Chapter 3\n\nA.4.1 Page 37\nThe p-value found using a chi-square test is 3.207e-11. Using a t-test of the mean difference gives a p-value of 2.269e-9. It is not explained how the p-value of 0.032 is arrived at in the book.\n\n\nA.4.2 Typography: section 3.2 p. 40, equation 3.1\nThe current latex expression of conditional independence used seems to be (Y(0), Y(1)) \\ \\text{II} \\ T with the output\n\\[\n(Y(0), Y(1)) \\ \\text{II} \\ T\n\\]\na better typography would be \\perp\\!\\!\\!\\perp for the symbol \\(\\perp\\!\\!\\!\\perp\\). When used for equation 3.1 as (Y(0), Y(1)) \\perp\\!\\!\\!\\perp T we obtain\n\\[\n(Y(0), Y(1)) \\perp\\!\\!\\!\\perp T\n\\]\nIn the case when we want to show dependence, that is no independence then the latex expression is \\not\\!\\perp\\!\\!\\!\\perp for the symbol \\(\\not\\!\\perp\\!\\!\\!\\perp\\). For example equation 3.1 would become\n\\[\n(Y(0), Y(1)) \\not\\!\\perp\\!\\!\\!\\perp T\n\\]"
  },
  {
    "objectID": "app01_errata.html#chapter-4",
    "href": "app01_errata.html#chapter-4",
    "title": "Appendix A — Errata",
    "section": "A.5 Chapter 4",
    "text": "A.5 Chapter 4\n\nA.5.1 Section 4.1 p. 67 (on top)\nThe line is\n\nwhich is statistically signficant\n\nshould be significant"
  },
  {
    "objectID": "app01_errata.html#chapter-5",
    "href": "app01_errata.html#chapter-5",
    "title": "Appendix A — Errata",
    "section": "A.6 Chapter 5",
    "text": "A.6 Chapter 5\n\n\nnothing found"
  },
  {
    "objectID": "app01_errata.html#chapter-6",
    "href": "app01_errata.html#chapter-6",
    "title": "Appendix A — Errata",
    "section": "A.7 Chapter 6",
    "text": "A.7 Chapter 6\n\nA.7.1 Section 6.1 p. 100, first paragraph\nThe second sentence says\n\nMistakingly equation \\(E_H E(Y \\mid T=t \\mid H)\\) with […]\n\nShould it be \\(E_H (E(Y \\mid T=t) \\mid H)\\)? See extra \\()\\) before the last \\(\\mid\\).\n\n\nA.7.2 Section 6.3 p. 126, the script of simdr\nThe last paragraph of p. 126 says\n\nWe simulated \\(T\\) […] such that approximaly 600 individuals had \\(T=1\\)\n\nThe simdr gives an incorrect result of 540 with the constant 0.13. That constant should be 0.15 to obtain 600. See the mathematical proof and proof by simulation in the appendix Doubly Robust Simulation at Analyse \\(T\\)."
  },
  {
    "objectID": "app01_errata.html#chapter-7",
    "href": "app01_errata.html#chapter-7",
    "title": "Appendix A — Errata",
    "section": "A.8 Chapter 7",
    "text": "A.8 Chapter 7\n\nA.8.1 Section 7.2, equation (7.11), p. 139\n\n\\[\n\\begin{align*}\nE(Y_1 \\mid A=1) - (E(Y_1\\mid A=0, Y_0=1) - E(Y_1\\mid A=0, Y_0=0)) - E(Y_0 \\mid A=0) - (E(Y_1\\mid A=0, Y_0=0) - E(Y_1\\mid A=0, Y_0=0))\n\\end{align*}\n\\]\n\n\n\nA.8.2 Exercise 2\nIn the last paragraph of the exercise\n\nIn addition, use exsim.r to simulate […]\n\nIt should be ex2sim.r"
  },
  {
    "objectID": "app01_errata.html#chapter-8",
    "href": "app01_errata.html#chapter-8",
    "title": "Appendix A — Errata",
    "section": "A.9 Chapter 8",
    "text": "A.9 Chapter 8\n\nA.9.1 Section 8.2, p. 150\nThe very first sentence of section 8.2 says\n\n[…] the front-door theorm of Pearl […]\n\nIt should be theorem"
  },
  {
    "objectID": "app01_errata.html#chapter-9",
    "href": "app01_errata.html#chapter-9",
    "title": "Appendix A — Errata",
    "section": "A.10 Chapter 9",
    "text": "A.10 Chapter 9\n\nA.10.1 Beginning of chapter, p. 158\nMissing parentheses in the equation\n\\[\nITT = E(Y(1, A(1)) - E(Y(0, A(0))\n\\] but is missing parentheses and should be\n\\[\nITT = E(Y(1, A(1))) - E(Y(0, A(0)))\n\\]\nBased on the notation of mentioned in the second paragraph of p. 158, that we let \\(Y(t,a)\\) be the potential outcome of \\(Y\\) assuming we set \\(T=t\\) and then \\(A=a\\). Then the equation could be written more simply as\n\\[\nITT = E(Y(1, 1)) - E(Y(0, 0))\n\\]\n\n\nA.10.2 Section 9.3, p. 165\nIn the code for the example the problem is caused by the fact that IV &lt;- ITT / denom does not work when denom is too small. What about setting the result to NA when denom &lt; tolerance so the bootstrap will skip it and increase the number of bootstraps?\n\n\nA.10.3 Section 9.3, p. 169, 170, table 9.1\nMy results seem to be more consistent than the textbook’s. Is this a mistake, how to test these results which can possibly be too good to be true."
  },
  {
    "objectID": "app01_errata.html#chapter-10",
    "href": "app01_errata.html#chapter-10",
    "title": "Appendix A — Errata",
    "section": "A.11 Chapter 10",
    "text": "A.11 Chapter 10\n\nA.11.1 Section 10.3, code for equartiles.r\nThe following coding line is superfluous\nquartiles &lt;- quantile(eb, c(0, .25, .5, .75, 1))"
  },
  {
    "objectID": "app01_errata.html#chapter-11",
    "href": "app01_errata.html#chapter-11",
    "title": "Appendix A — Errata",
    "section": "A.12 Chapter 11",
    "text": "A.12 Chapter 11\n\nA.12.1 Section 11.2, p. 190\nThe data set i17dat is not in the material provided."
  },
  {
    "objectID": "app01_errata.html#chapter-12",
    "href": "app01_errata.html#chapter-12",
    "title": "Appendix A — Errata",
    "section": "A.13 Chapter 12",
    "text": "A.13 Chapter 12\n\nA.13.1 section 12.1, p. 198\nAt the bottom of the page, th first sentence of the paragraph says\n“by substituting parametric or nonparmetric. s/b nonparametric.\n\n\nA.13.2 section 12.3, p. 206\nJust before the start of the exercise, beneath table 12.3\n“is helpful of terms of teasing apart …”, s/b tearing"
  },
  {
    "objectID": "app02_notes.html#chapter-2",
    "href": "app02_notes.html#chapter-2",
    "title": "Appendix B — Notes",
    "section": "B.1 Chapter 2",
    "text": "B.1 Chapter 2\n\nB.1.1 section 2.4 p. 31\nThe second sentence of the last paragraph on p. 33 says\n\nWe also need the car package in order for the summary() function to operate on boot objects the way we describe.\n\nThis sentence is not required if we use the boot::boot.ci() which simplifies lmodboot.r() and does not require the car package. See the code in this document for lmodboot.r in chapter 2."
  },
  {
    "objectID": "app02_notes.html#chapter-3",
    "href": "app02_notes.html#chapter-3",
    "title": "Appendix B — Notes",
    "section": "B.2 Chapter 3",
    "text": "B.2 Chapter 3\nThe chi-square test on p. 37 gives a different result. Not clear how the author arrive at her result."
  },
  {
    "objectID": "app02_notes.html#chapter-4",
    "href": "app02_notes.html#chapter-4",
    "title": "Appendix B — Notes",
    "section": "B.3 Chapter 4",
    "text": "B.3 Chapter 4\n\nB.3.1 Section 4.1\nSee the plots in section 4.2. They could be helpful to visualize the changes in effect measures from one level of modifier to the other.\n\n\nB.3.2 Section 4.2\n\nB.3.2.1 Monte Carlo Simulation\nA Monte Carlo is provided in section 4.2 and coded in a function called betasim_effect_measures(). It uses the \\(Beta\\) distribution. It is helpful in that it\n\nconfirms the same results as in Jake Shannin (2021)\nis less CPU intensive as it needs only 5000 iterations to confirm Jake Shannin (2021)\nis easier to code than java and uses R which is the declared language of Brumback (2022)\nallows some extra flexibility with the shape parameters of \\(Beta\\) to investigate the conclusion with diffferent curves. See the suggestion for applications below.\n\n\n\nB.3.2.2 page 72, Figure 4.1\n\nThe probabilites shown in the Venn diagram do not add up to 100% because, for example, the event that RR changes in the same direction as RD but not in the same direction as the other two measures […]. It would akward to arbitrarily one of those 2 chances as zero.\n\nJake Shannin (2021) mentions that it is the result of not mutually exclusive events. That is true. Yet, these events, properly grouped are actually mutually exclusive. In section 4.2 they are called Opposite pairwise events. Using these definitions then yes, they are mutually exclusive but cannot be properly shown in the Venn diagram. This can be easily solved by splitting the probabilities. See section 4.2 for details.\nThe end result a proper partitioning of the sample space \\(\\Omega\\) and is, in fact, a \\(\\sigma-field\\) (See Geoffrey R. Grimmet (2001), section 1.2). Yet it does not change the conclusions reached in Jake Shannin (2021).\n\n\nB.3.2.3 Applications\nSee my sub-section 4.2 called Applications where 2 possible applications are mentioned.\n\nData pre-processing (data cleaning)\nBayesian prior for Beta-binomial model\n\n\n\n\nB.3.3 Exercises\n\nB.3.3.1 Exercise 1\nUsing the causal power, the conclusion is different than the official answer. It is not obvious why the official solution does not make use of the causal power.\n\n\nB.3.3.2 Exercise 5\nThe official solution uses gee with the default family, that is gaussian.\nSince the outcome \\(attend\\) is binary isn’t it better to use the binomial family?\nWe quote p. 50 from chapter 3 in that respect\n\nBecause our outcome is binary, we choose to fit the logistic parametric model"
  },
  {
    "objectID": "app02_notes.html#chapter-6",
    "href": "app02_notes.html#chapter-6",
    "title": "Appendix B — Notes",
    "section": "B.4 Chapter 6",
    "text": "B.4 Chapter 6\n\nB.4.1 Section 6.1.1 ATT\nThe function bootstandatt is not necessary. The small change is taken care of in bootstand with the argument att.\n\n\nB.4.2 Section 6.3\nThe conclusions with the simulation are the same as Brumback’s. However her sd for the estimators \\(EY1exp\\) and \\(EY1dr\\) for both \\(ss=40\\) and \\(ss=100\\) don’t agree with my results. They are so large that they raise questions."
  },
  {
    "objectID": "app02_notes.html#chapter-9",
    "href": "app02_notes.html#chapter-9",
    "title": "Appendix B — Notes",
    "section": "B.5 Chapter 9",
    "text": "B.5 Chapter 9\n\nB.5.1 Section 9.3\nReplace IV with NA when its value is too close to zero"
  },
  {
    "objectID": "app02_notes.html#chapter-10",
    "href": "app02_notes.html#chapter-10",
    "title": "Appendix B — Notes",
    "section": "B.6 Chapter 10",
    "text": "B.6 Chapter 10"
  },
  {
    "objectID": "app02_notes.html#section-10.1",
    "href": "app02_notes.html#section-10.1",
    "title": "Appendix B — Notes",
    "section": "B.7 Section 10.1",
    "text": "B.7 Section 10.1\nAdd subsection to highlight the checking of overlap.\n\n\n\n\nBrumback, Babette A. 2022. Fundamentals of Causal Inference with r. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and Random Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP: Oxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app03_linreg.html#model",
    "href": "app03_linreg.html#model",
    "title": "Appendix C — Linear Regression with R",
    "section": "C.1 Model",
    "text": "C.1 Model\nWe will use the model from Jake Shannin (2021). in chapter 5, section 5.2, on page 89. Its DAG is illustrated in figure 5.7.\n\nsim2 &lt;- function(n = 1000, seed = 888) {\n  set.seed(seed)\n  # Generate the observed confounder\n  H &lt;- rbinom(n, size = 1, prob = 0.4)\n  # Let the treatment depend on the confounder\n  probA &lt;- H * 0.8 + (1 - H) * 0.3\n  A &lt;- rbinom(n, size = 1, probA)\n  # Let the outcome depend on the treatment and the confounder\n  probY &lt;- A * (H * 0.5 + (1 - H) * 0.7) + (1 - A) * (H * 0.3 + (1 - H) * 0.5)\n  Y &lt;- rbinom(n, size = 1, prob = probY)\n  data.frame(\"H\" = H, \"A\" = A, \"Y\" = Y)\n}\ndf &lt;- sim2()\ndf |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nH\n0\n1\n0.40\n0.49\n0\n0\n0.0\n1\n1\n▇▁▁▁▆\n\n\nA\n0\n1\n0.50\n0.50\n0\n0\n0.5\n1\n1\n▇▁▁▁▇\n\n\nY\n0\n1\n0.51\n0.50\n0\n0\n1.0\n1\n1\n▇▁▁▁▇"
  },
  {
    "objectID": "app03_linreg.html#fitted-vs-predict",
    "href": "app03_linreg.html#fitted-vs-predict",
    "title": "Appendix C — Linear Regression with R",
    "section": "C.2 fitted() vs predict()",
    "text": "C.2 fitted() vs predict()\n\nC.2.1 fitted()\nfitted() returns the expected values of the response after the link function is applied, i.e. \\(E(Y_I)\\). Also fitted() uses only the original data.\nFor example\n\nn &lt;- 5\nx &lt;- rnorm(n)\ny &lt;- rpois(n, lambda = exp(x))\ndf &lt;- data.frame(\"x\" = x, \"y\" = y)\nfit &lt;- glm(y ~ x, data = df, family = \"poisson\")\n\nand fitted() give the \\(E(Y_i)\\)\n\nfitted.out &lt;- fitted(fit)\nfitted.out\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\nand if you try to fit with new data, nothing will happen. It doesn’t use the new data and returns the same output, no warning is given about the fact that newdata is not used.\n\nnewdf &lt;- df\nnewdf$x &lt;- rnorm(n, mean = 1, sd = 0.5)\nfitted(fit, newdata = newdf)\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\n\n\nC.2.2 predict()\nOn the other hand predict gives the result before the link function is applied and uses newdata\nFirst lets use predict() with the existing data\n\npredict.out &lt;- predict(fit)\npredict.out\n\n         1          2          3          4          5 \n-0.0600878  1.6274351 -0.4642585 -2.3923702 -1.3963289 \n\n\nwhich is before the link function is applied and so\n\nexp(predict.out) == fitted.out\n\n   1    2    3    4    5 \nTRUE TRUE TRUE TRUE TRUE \n\n\nand there is an option, type = \"response\" that tells predict to applied the link function so you don’t have to do it\n\npredict(fit, type = \"response\")\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\n\n\nC.2.3 Conclusion\nIn the context of Jake Shannin (2021) we use fitted() when we use the original data to get \\(E(E(Y_i))= \\hat{Y}\\) and predict()..., type = \"response) when we ave a counterfactual in the data, that is \\(E(E(Y_i \\mid T = t))= \\hat{Y} \\mid T=t\\)\n\nmean(fitted.out)\n\n[1] 1.4\n\n\n\n\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app04_simdr.html#errata6a",
    "href": "app04_simdr.html#errata6a",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.1 Analyse \\(T = \\sum_{i=1}^J{T_i}\\) (Errata)",
    "text": "D.1 Analyse \\(T = \\sum_{i=1}^J{T_i}\\) (Errata)\nSection 6.3 claims that \\(T=600\\) and \\(P(T=1 \\mid H) \\in [0.041, 0.0468]\\) but the results obtained from running simdr as shown in the book are different. Not a lot but enough to puzzle the avid reader (like me for example).\nSo lets go through simdr to explain the differences.\n\nsimdr.out &lt;- simdr(seed = 1009)$stats\n\n\nOne might think that standardization with the exposure model would be preferable when the outcome indicates a rare condition. To see this, first suppose the condition is not rare. We might have 3000 individuals and 50%, or 1500, with the condition. Using the rule of thumb for logistic regression of Peduzzi et al. presented in chapter 2, we should be able to include 150 covariates in the outcome model.\n\nThe Peduzzi rule can be found at the end of section 2.3, on top of p. 29. where it states that\n\nboth the numbers of individuals with \\(Y=0\\) and \\(Y=1\\) need to be larger than ten times the number of parameters.\n\nand therefore\n\nNow suppose the exposure, \\(T\\), is divided more evenly: that is, we have 600 with \\(T=1\\). This would suggest we can include 60 covariates in the exposure model.\n\nBrumback uses a linear function of \\(H\\) to create a distribution of \\(T_i\\) that makes it dependent on \\(H\\) and which should give \\(\\sum T_i \\approx 600\\). However, the simulation with simdr returns \\(\\sum T_i \\approx 540\\).\n\n# sum of T\nsimdr.out$`T`$sum\n\n[1] 541\n\n\n\nWe observe that \\(\\sum_i T_i \\approx 540 \\not \\approx 600\\).\n\nThis is the part that needs explaining. The difference seems too large to be explained by the usual culprit, random process.\nTo be able to do that lets use some notations and go through the mechanics of simdr.\nLet \\(H_{j}\\) be the covariate \\(j\\)\n\\[\n\\begin{align*}\nH_j \\text{ i.i.d. } \\mathcal{Bernoulli}(p), \\, j = 1, \\ldots, J\n\\end{align*}\n\\]\nwhere \\(J\\) is the same variable as \\(ss\\) from Brumback, i.e. \\(J = ss = 100\\). Also \\(H_{i, j}\\) is the value of covariate \\(H_j\\) for individual \\(i\\).\nand let \\(T_i\\) be the treatment of individual \\(i\\)\n\\[\n\\begin{align*}\nT_i \\sim \\mathcal{Bernoulli}(prob = P_i), \\, i = 1, \\ldots, I\n\\end{align*}\n\\]\nwhere \\(I\\) is set at \\(I=3000\\) by Brumback.\nIn addition, we let\n\\[\nS_i = \\sum_{j=1}^{J}H_{i, j} \\\\\n\\text{where } J = ss \\text{ as mentioned above}\n\\]\nand simdr defines \\(P_i\\) as a random variable\n\\[\n\\begin{align*}\n&P_i = \\alpha \\frac{\\beta}{J} S_i + p \\times X_i \\\\\n\\\\\n&\\text{where} \\\\\n&\\alpha = 0.13 \\\\\n&\\beta = 20 \\\\\n&p = 0.05 \\\\\n&X_i \\sim \\mathcal{Normal}(mean = 1, sd = 0.1)\n\\end{align*}\n\\]\nnow, since\n\\[\n\\begin{align*}\nH_i \\text{ i.i.d. } \\mathcal{Bernoulli}(p) &\\implies E(H_i)=p \\\\\nT_i \\sim \\mathcal{Bernoulli}(p_{i}) &\\implies E(T_i) = E(P_i) \\\\\nX_i \\sim \\mathcal{Normal}(mean = 1, sd = 0.1) &\\implies E(X_i) = 1\n\\end{align*}\n\\]\nand since \\(E()\\) is a linear function then\n\\[\n\\begin{align*}\nE(T_i) &= E(P_i) \\\\\n&= E(\\alpha \\cdot \\frac{\\beta}{J} S_i + p \\cdot X_i) \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\sum_{j=1}^{J}E(H_{i, j}) + p \\cdot E(X_i) \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\sum_{j=1}^{J}p + p \\cdot 1 \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\cdot J \\cdot p + p \\\\\n&= \\alpha \\cdot \\beta \\cdot p + p\n\\end{align*}\n\\]\nand the sum of the number of people treated is \\(T = \\sum_i^IT_i\\)\n\\[\n\\begin{align*}\nE(T)  &= \\sum_{i=1}^I{E(T_i)} \\\\\n&= \\sum_{i=1}^I{(\\alpha \\cdot \\beta \\cdot p + p)} \\\\\n&= I (\\alpha \\cdot \\beta \\cdot p + p) \\\\\n\\\\\n&\\text{and since} \\\\\n&I = 3000, \\, \\alpha = 0.13, \\, \\beta = 20, \\, p = 0.05 \\\\\n\\\\\n&\\text{then} \\\\\nE(T) &= 3000 \\cdot (0.13 \\cdot 20 \\cdot 0.05 + 0.05) \\\\\n&= 540\n\\end{align*}\n\\]\nwhich proves that the expected number of \\(T\\) is not really close to 600 but rather 540 which is also the result of the simulation. Therefore we could use 54 covariates rather than 60 by the Peduzzi rule.\nWe note that this does not change the conclusions drawn from the simulation significantly.\nFinally, we note that the formula \\(\\alpha \\cdot \\beta \\cdot p + p\\) could be modified to get \\(T=600\\) by varying the coefficient \\(\\alpha=0.13\\) and/or \\(\\beta=20\\). If we change only \\(\\alpha\\) and keep \\(\\beta=20\\) we could use the result from above and with simple algebra\n\\[\n\\begin{align*}\nE(T)&= I (\\alpha \\cdot \\beta \\cdot p + p) \\\\\n\\\\\n&\\text{and since we want } E(T) = 600 \\\\\n&\\text{and that} \\\\\n&\\beta = 20, \\, I = 3000, \\, p = 0.05 \\\\\n\\\\\n\\text{then} \\\\\n600 &= 3000 \\cdot (\\alpha \\cdot 20 \\cdot 0.05 + 0.05) \\\\\n\\alpha &= \\frac{600 - 0.05 \\cdot 3000}{3000 \\cdot 20 \\cdot0.05} = \\frac{450}{3000} = 0.15\n\\end{align*}\n\\]\nand we simulate using \\(\\alpha = 0.15\\) to validate\n\nsimdr600.out &lt;- simdr(alpha = 0.15, seed = 1009)$stats\nsimdr600.out$`T`$sum\n\n[1] 593\n\n\n\nWe suggest that simdr be modified to use 0.15 instead of 0.13."
  },
  {
    "objectID": "app04_simdr.html#analyse-y-sum_i1i-y_i",
    "href": "app04_simdr.html#analyse-y-sum_i1i-y_i",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.2 Analyse \\(Y = \\sum_{i=1}^I Y_i\\)",
    "text": "D.2 Analyse \\(Y = \\sum_{i=1}^I Y_i\\)\nFirst we analyse \\(Y = \\sum Y_i\\) mathematically using the same notation as above.\n\\[\n\\begin{align*}\n&\\text{as defined in simdr} \\\\\nY_i &= 0.1 T_i + 0.1 \\frac{\\beta}{J} S_i \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y_i) &=0.1 E(T_i) + 0.1 \\frac{\\beta}{J} E(S_i) \\\\\n&= 0.01 E(P_i) + 0.01 \\frac{\\beta}{J} \\sum_{j=1}^J{E(H_{ij})} \\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\sum_{j=1}^J{p} \\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\frac{\\beta}{J} \\cdot J \\cdot p\\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\cdot \\beta \\cdot p \\\\\n\\\\\n&\\text{and given} \\\\\n&\\alpha = 0.13, \\, \\beta = 20, \\, p = 0.05 \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y_i) &= 0.01 \\cdot(0.13 \\cdot 20 \\cdot 0.05 + 0.05) + 0.01 \\cdot 20 \\cdot 0.05 \\\\\n&=0.0013 + 0.0005 + 0.01 \\\\\n&= 0.0118\n\\end{align*}\n\\]\nand since \\(Y = \\sum_{i=1}^I Y_i\\) then\n\\[\n\\begin{align*}\nY &= \\sum_{i=1}^I Y_i \\\\\nE(Y) &= \\sum_{i=1}^I E(Y_i) = I \\cdot E(Y_i) \\\\\n\\\\\n&\\text{and from above} \\\\\nE(Y_i) &= 0.0118 \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y) &= I \\cdot E(Y_i) = 3000 \\cdot 0.0118 = 35.4 \\\\\n&\\approx 35\n\\end{align*}\n\\]\nwhich mathematically proves the following\n\nWe simulated \\(Y_i\\) as a function of \\(T_i\\) and \\(\\sum_{k=1}^{ss}H_{i,k}\\), such that approximately 35 individuals had \\(Y=1\\).\n\nand the result we get from both simdr is 38, close enough.\n\nsimdr.out$Y$sum\n\n[1] 38\n\n\n\nThe mean of \\(\\sum_{k=1}^{ss}{H_{ik}}\\) was fixed at one, but when \\(ss\\) was set to 100, it ranged from 0.00 to 2.80.\n\nThat is exactly what we get.\n\nc(\"min\" = simdr.out$sumH$min, \"max\" = simdr.out$sumH$max)\n\nmin max \n0.0 2.8 \n\n\nthen\n\n\\(P(T = 1 \\mid H)\\) ranged from 0.041 to 0.468.\n\n\nThe results are different.\n\n\nc(\"min\" = simdr.out$probT$min, \"max\" = simdr.out$probT$max)\n\n       min        max \n0.03873516 0.41876534 \n\n\nProbably because the original simulation came up with \\(T=600\\) rather than \\(T=540\\) as discussed above. Indeed if we verify with the simulation with \\(T=600\\) the results are now reasonably close.\n\nc(\"min\" = simdr600.out$probT$min, \"max\" = simdr600.out$probT$max)\n\n       min        max \n0.03873516 0.47476534 \n\n\n\n\\(E(Y \\mid T, H)\\) ranged from 0.000 to 0.036.\n\nand the results are almost identical\n\nc(\"min\" = simdr.out$probY$min, \"max\" = simdr.out$probY$max)\n\n  min   max \n0.000 0.038"
  },
  {
    "objectID": "app04_simdr.html#functions",
    "href": "app04_simdr.html#functions",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.3 Functions",
    "text": "D.3 Functions\nThe function simdr is found on p. 127-128. See the script in the last section of this appendix. We just added some arguments and and output to facilitate the analysis. See the next section Analysis.\nThe Monte Carlo simulation is done with a non-parametric Monte Carlo function mc_standdr with the following script. See the section Simulation blow.\nNote that simdr was rewritten as function mc_standdr to\n\nfacilitate the analysis of the algorithm\nperform a Monte Carlo simulation with the MonteCarlo package\nseparate the data simulation from the measurement of estimates in 2 sub functions\n\nstanddr_sim: Simulate the data\nstanddr_est: Calculate the estimates using the simulated data from standdr_sim\n\n\nmc_standdr and simdr give exactly the same results for the simulatted data\n\nsimdr.out &lt;- simdr(seed = 1009)\nnew_simdr.out &lt;- standdr_sim(seed = 1009)\nstopifnot(identical(simdr.out$stats, new_simdr.out$stats))\n\nas well as the measurements\n\nnew_simdr.est &lt;- standdr_est(Y = new_simdr.out$data$Y,\n                             `T` = new_simdr.out$data$`T`,\n                             H = new_simdr.out$data$H)\nstopifnot(identical(simdr.out$est, new_simdr.est))"
  },
  {
    "objectID": "app04_simdr.html#scripts",
    "href": "app04_simdr.html#scripts",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.4 Scripts",
    "text": "D.4 Scripts\n\nD.4.1 simdr\n\n#' Doubly Robust Standardization Simulation\n#' \n#' Doubly robust standardization simulation.\n#' \n#' This is the function used in \\emph{Fundamentals of Causal Inference} by\n#' B. Brumback in section 6.3 of chapter 6, p.127-128. \\code{standdr_stats} is\n#' used in the output to give more statistics. Also the arguments \n#' \\code{beta = 0.13} and \\code{gamma = 20} were necessary to analyse the\n#' algorithm, they don't change anything.\n#'\n#' @param ss Number of covariates i.i.d with \\code{rbinom(n, size=1, prob=probH)}\n#' @param alpha coefficient used to compute the distribution of \\code{`T`}.\n#' @param beta coefficient used to compute the distribution of \\code{`T`}.\n#' @param seed Seed used for random number generation, default is \\code{NULL}.\n#'\n#' @return List of statistics for thesimulated data and estimates using\n#' different merhods.\n#'\n#' @examples\n#' \\dontrun{\n#' simdr()\n#' }\n#' @export\nsimdr &lt;- function(ss = 100, alpha = 0.13, beta = 20, seed = NULL) {\n  \n  set.seed(seed)\n  \n  # ss is the number of confounders\n  # i.e. the number of columns of H\n  H &lt;- matrix(0, 3000, ss)\n  # Let all components of H be independent Bernoulli variables with p=0.05\n  probH &lt;- rep(0.05, 3000)\n  for (i in 1:ss) {\n    H[, i] &lt;- rbinom(n = 3000, size = 1, prob = probH)\n  }\n  # Let the treatment depend on a function of H\n  sumH &lt;- apply(H, 1, sum) * beta / ss\n  # make sure P(T=1) is between 0 and 1, i.e. positivity assumption\n  probT &lt;- alpha * sumH + 0.05 * rnorm(n = 3000, mean = 1, sd = 0.1)\n  # validate the positivity assumption\n  stopifnot(probT &gt; 0, probT &lt; 1)\n  \n  `T` &lt;- rbinom(n = 3000, size = 1, prob = probT)\n  \n  # Generate the outcome depend on T and H\n  probY &lt;- 0.01 * `T` + 0.01 * sumH\n  # positivity assumption is not required for the outcome\n  # see intro to chapter 6 p. 99\n  stopifnot(probY &gt;= 0, probY &lt;= 1)\n  \n  Y &lt;- rbinom(n = 3000, size = 1, prob = probY)\n  \n  # put the simulated resuts in a list\n  stats &lt;- list(\"sumH\" = simdr_stats(sumH),\n              \"probT\" = simdr_stats(probT),\n              \"T\" = simdr_stats(`T`),\n              \"probY\" = simdr_stats(probY),\n              \"Y\" = simdr_stats(Y))\n  \n  # fit the exposure model\n  e &lt;- fitted(lm(`T` ~ H))\n  \n  # refit the exposure model using an incorrect logistic model\n  e2 &lt;- predict(glm(`T` ~ H, family = \"binomial\"), type = \"response\")\n  \n  # compute the weights\n  w0 &lt;- (1 - `T`) / (1 - e)\n  w1 &lt;- `T` / e\n  w02 &lt;- (1 - `T`) / (1 - e2)\n  w12 &lt;- T / e2\n  \n  # fit an overspecified (saturated) outcome model\n  mod.out &lt;- lm(Y ~ `T` * H)\n  \n  # Estimate the expected potential outcomes using the various methods\n  dat &lt;- data.frame(\"Y\" = Y, \"T\" = `T`)\n  dat0 &lt;- dat\n  dat0$`T` &lt;- 0\n  dat1 &lt;- dat\n  dat1$`T` &lt;- 1\n  \n  # the predicted data\n  preds0 &lt;- predict(mod.out, newdata = dat0)\n  preds1 &lt;- predict(mod.out, newdata = dat1)\n  \n  # calculate the estimates\n  EYT0 &lt;- mean(Y * (1 - `T`))\n  EYT1 &lt;- mean(Y * `T`)\n  EY0exp &lt;- weighted.mean(Y, w = w0)\n  EY1exp &lt;- weighted.mean(Y, w = w1)\n  EY0exp2 &lt;- weighted.mean(Y, w = w02)\n  EY1exp2 &lt;- weighted.mean(Y, w = w12)\n  EY0out &lt;- mean(preds0)\n  EY1out &lt;- mean(preds1)\n  EY0dr &lt;- mean(w0 * Y + preds0 * (`T` - e) / (1 - e))\n  EY1dr &lt;- mean(w1 * Y - preds1 * (`T` - e) / e)\n  \n  est &lt;- list(\n    \"EYT0\" = EYT0,\n    \"EYT1\" = EYT1,\n    \"EY0exp\" = EY0exp,\n    \"EY1exp\" = EY1exp,\n    \"EY0exp2\" = EY0exp2,\n    \"EY1exp2\" = EY1exp2,\n    \"EY0out\" = EY0out,\n    \"EY1out\" = EY1out,\n    \"EY0dr\" = EY0dr,\n    \"EY1dr\" = EY1dr\n    )\n  \n  list(\"stats\" = stats, \"est\" = est)\n}\n\n#' Compute statistics from \\code{simdr}. Sames as \\code{standdr_est}\n#'\n#' @param x Vector of numeric values.\n#'\n#' @return list of statistics: \\code{sun(x), mean(x), min(x), max(x)}.\n#' \n#' @seealso standdr_stats\n#'\n#' @examples\n#' simdr_stats(runif(20))\n#' @export\nsimdr_stats &lt;- function(x) {\n  list(\"sum\" = sum(x), \"mean\" = mean(x), \"min\" = min(x), \"max\" = max(x))\n}\n\n\n\nD.4.2 mc_standdr\n\n#' Monte Carlo Simulation of Doubly Robust Standardization\n#'\n#' @param ss Integer(). Number of covariates.\n#' @param nrep Number of Monte Carlo repetitions.\n#' @param width Width of interval. e.g. 0.95 will give interval c(0.025, 0.975).\n#' Default is 0.95.\n#' \n#' @seealso standdr_sim standdr_est\n#'\n#' @return Dataframe of results.\n#' @export\nmc_standdr &lt;- function(ss = c(40, 100), nrep = 1000, width = 0.95) {\n  stopifnot(all(ss &gt;= 1), nrep &gt;= 1, width &gt; 0, width &lt; 1)\n  \n  # We use alpha = 0.15 to match results with the books\n  ms_standdr_func &lt;- function(n = 3000, ss = 100, alpha = 0.13, beta = 20, \n                              probH = 0.05, seed = NULL) {\n    \n    # first we simulate the data\n    dat &lt;- standdr_sim(n = n, ss = ss, alpha = alpha, beta = beta, \n                       probH = probH, seed = seed)$data\n    # then output the estimates in a list\n    standdr_est(Y = dat$Y, `T` = dat$`T`, H = dat$H)\n  }\n  \n  params &lt;- list(\"ss\" = ss)\n  mc.out &lt;- MonteCarlo::MonteCarlo(func = ms_standdr_func,\n                                   nrep = nrep, param_list = params)\n  \n  # output results in a dataframe\n  out &lt;- suppressWarnings(MonteCarlo::MakeFrame(mc.out))\n  out %&gt;%\n    pivot_longer(cols = -ss, names_to = \"estimator\", values_to = \"value\") %&gt;%\n    group_by(ss, estimator) %&gt;%\n    summarize(n = n(), \n              mean = mean(value),\n              sd = sd(value),\n              lower = quantile(value, probs = (1 - width) / 2),\n              upper = quantile(value, probs = 1 - (1 - width) / 2)) %&gt;%\n    ungroup()\n}\n\n#' Data Simulation for Doubly Robust Standardization\n#'\n#' @param n Number of individuals/observations.\n#' @param ss Number of covariates.\n#' @param alpha coefficient used to compute the distribution of \\code{`T`}.\n#' @param beta coefficient used to compute the distribution of \\code{`T`}.\n#' @param probH probability of H.\n#' @param seed Seed value. default is \\code{NULL}.\n#'\n#' @return List with a dataframe of Y, T and H and summary statitics.\n#' @export\n#'\n#' @examples\n#' \\dontrun{\n#' standdr_sim()\n#' }\nstanddr_sim &lt;- function(n = 3000, ss = 100, alpha = 0.13, beta = 20, \n                        probH = 0.05, seed = NULL) {\n  set.seed(seed)\n  \n  # matrix of independent Bernoulli vector with prob = 0.05\n  # \"The columns of H were independent indicator variables each\n  #  with probability 0.05\"\n  H &lt;- cbind(replicate(n = ss, rbinom(n = n, size = 1, prob = probH)))\n  \n  # let the treatment depend on a function of H\n  # \"We simulated T  as indicator variables with probabilities that varied as\n  # a linear function  of H such that approximately 600 individuals had T=1\"\n  sumH &lt;- apply(H, MARGIN = 1, FUN = sum) * beta / ss\n  probT &lt;- alpha * sumH + probH * rnorm(n = n, mean = 1, sd = 0.1)\n  # validate the positivity assumption\n  stopifnot(probT &gt; 0, probT &lt; 1)\n  \n  `T` &lt;- rbinom(n = n, size = 1, prob = probT)\n  \n  \n  # generate the outcome depend on T and H\n  # \"We simulated Y as a function T ans sumH such hat approximatey 35 \n  # individuals had Y = 1\"\n  probY &lt;- 0.01 * `T` + 0.01 * sumH\n  # positivity assumption is not required for the outcome\n  # see intro to chapter 6 p. 99\n  stopifnot(probY &gt;= 0, probY &lt;= 1)\n  \n  Y &lt;- rbinom(n = n, size = 1, prob = probY)\n  \n  # put the data in a data.frame\n  df &lt;- data.frame(\"Y\" = Y, \"T\" = `T`, \"H\" = H)\n  \n  # output results in a list\n  list(\n    \"stats\" = list(\"sumH\" = standdr_stats(sumH),\n                 \"probT\" = standdr_stats(probT),\n                 \"T\" = standdr_stats(`T`),\n                 \"probY\" = standdr_stats(probY),\n                 \"Y\" = standdr_stats(Y)),\n    \"data\" = list(\"Y\" = Y, \"T\" = `T`, \"H\" = H)\n    )\n}\n\n\n#' Estimates from Doubly Robust Standardization Simulation\n#'\n#' @param Y Vector of outcomes\n#' @param `T` Vector of treatments\n#' @param H Matrix of covariates\n#'\n#' @return List of estimates\n#' @export\nstanddr_est &lt;- function(Y, `T`, H) {\n  \n  # fit the exposure model\n  e &lt;- fitted(lm(`T` ~ H))\n  \n  # refit the exposure model using an incorrect logistic model\n  e2 &lt;- predict(glm(`T` ~ H, family = \"binomial\"), type = \"response\")\n  \n  # compute the weights\n  w0 &lt;- (1 - `T`) / (1 - e)\n  w1 &lt;- `T` / e\n  w02 &lt;- (1 - `T`) / (1 - e2)\n  w12 &lt;- T / e2\n  \n  # fit an overspecified (saturated) outcome model\n  mod.out &lt;- lm(Y ~ `T` * H)\n  \n  # Estimate the expected potential outcomes using the various methods\n  dat &lt;- data.frame(\"Y\" = Y, \"T\" = `T`)\n  dat0 &lt;- dat\n  dat0$`T` &lt;- 0\n  dat1 &lt;- dat\n  dat1$`T` &lt;- 1\n  \n  # the predicted data\n  preds0 &lt;- predict(mod.out, newdata = dat0)\n  preds1 &lt;- predict(mod.out, newdata = dat1)\n  \n  # calculate the estimates\n  EYT0 &lt;- mean(Y * (1 - `T`))\n  EYT1 &lt;- mean(Y * `T`)\n  EY0exp &lt;- weighted.mean(Y, w = w0)\n  EY1exp &lt;- weighted.mean(Y, w = w1)\n  EY0exp2 &lt;- weighted.mean(Y, w = w02)\n  EY1exp2 &lt;- weighted.mean(Y, w = w12)\n  EY0out &lt;- mean(preds0)\n  EY1out &lt;- mean(preds1)\n  EY0dr &lt;- mean(w0 * Y + preds0 * (`T` - e) / (1 - e))\n  EY1dr &lt;- mean(w1 * Y - preds1 * (`T` - e) / e)\n  \n  list(\"EYT0\" = EYT0,\n       \"EYT1\" = EYT1,\n       \"EY0exp\" = EY0exp,\n       \"EY1exp\" = EY1exp,\n       \"EY0exp2\" = EY0exp2,\n       \"EY1exp2\" = EY1exp2,\n       \"EY0out\" = EY0out,\n       \"EY1out\" = EY1out,\n       \"EY0dr\" = EY0dr,\n       \"EY1dr\" = EY1dr)\n}\n\n#' Compute Statistics from \\code{standdr_sim}.\n#'\n#' @param x Vector of numeric values.\n#'\n#' @return list of statistics: \\code{sun(x), mean(x), min(x), max(x)}.\n#'\n#' @examples\n#' standdr_est(runif(20))\n#' @export\nstanddr_stats &lt;- function(x) {\n  list(\"sum\" = sum(x), \"mean\" = mean(x), \"min\" = min(x), \"max\" = max(x))\n}"
  }
]