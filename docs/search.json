[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "",
    "text": "Preface\nThis is a study project of the book Fundamentals of Causal Inference With R by Babette A. Brumback. Ms Brumback uses base R for all the code. This study project is coding with the tidyverse way. The motivation stems from the opinion that, in practice, the code is as important as the theory and learning better coding practice should be started as early as possible."
  },
  {
    "objectID": "index.html#where-to-find",
    "href": "index.html#where-to-find",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Where-to-find",
    "text": "Where-to-find\n\nThe online version of this document can be found at FCI.\nThe companion package for this project is fciR and can be found at fciR.\nThe exercises can be found at FCI exercises. This repo could be set as a private to avoid frustrating the publisher."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany thanks to Babette A. Brumback for a book full of amazing observations, tricks and tips."
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Packages",
    "text": "Packages\nFor data processing and analysis the following packages are used\n\n\n\nPackage\nComment\n\n\n\n\nfciR\nThis companion R package for this book\n\n\nconflicted\nManage conflict resolution amongst packages\n\n\ntidyverse\nTidyverse is the favored coding way\n\n\nskimr\nSummary statistics\n\n\nmodelr\nCreate elegant pipelines when modelling\n\n\nsimpr\nGenerate simulated data\n\n\nMonteCarlo\nMonte Carlo simulation\n\n\ngeepack\nGeneralized estimating equations solver\n\n\nrsample\nResampling and bootstraps\n\n\n\nFor plotting, graphs and tables these packages are used\n\n\n\nPackage\nComment\n\n\n\n\nggplot\nCreate graphics based on the grammar of graphics\n\n\nggdag\nCausal directed acyclic graphs\n\n\ntidygraph\nGraphs and networks manipulation\n\n\nggvenn\nVenn diagrams by ggplot2. Used in chap. 4\n\n\ngt\nNice-looking tables"
  },
  {
    "objectID": "part01.html",
    "href": "part01.html",
    "title": "Part I Basics",
    "section": "",
    "text": "This part covers the basics of probability theory, effect-measure and causal directed acyclic graphs"
  },
  {
    "objectID": "ch01_intro.html#a-brief-history",
    "href": "ch01_intro.html#a-brief-history",
    "title": "1  Introduction",
    "section": "1.1 A Brief History",
    "text": "1.1 A Brief History"
  },
  {
    "objectID": "ch01_intro.html#data-examples",
    "href": "ch01_intro.html#data-examples",
    "title": "1  Introduction",
    "section": "1.2 Data Examples",
    "text": "1.2 Data Examples\n\n1.2.1 Mortality Rates by Country\nThis dataset is available with fciR::mortality. The summary table is\n\ndata(\"mortality\", package = \"fciR\")\nmortality |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.1:  Mortality Rates by Age and Country \n  \n    \n    \n      T\n      H\n      deaths\n      population\n      Y\n    \n  \n  \n    TRUE\nFALSE\n756340\n282305227\n0.002679157\n    TRUE\nTRUE\n2152660\n48262955\n0.044602739\n    FALSE\nFALSE\n2923480\n1297258493\n0.002253583\n    FALSE\nTRUE\n7517520\n133015479\n0.056516129\n  \n  \n  \n\n\n\n\n\n\n\n1.2.2 National Center for Education Statistics\nThis dataset is available with fciR::nces.\nThe statistical summary is\n\ndata(\"nces\", package = \"fciR\")\nnces |&gt;\n  skimr::skim()\n\n\nTable 1.2: ?(caption)\n\n\n\n\n(a) Data summary\n\n\nName\nnces\n\n\nNumber of rows\n1217\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nselective\n0\n1\n0.20\n0.40\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nfemale\n0\n1\n0.44\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▆\n\n\nhighmathsat\n0\n1\n0.21\n0.41\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\n\n\n\n\n\nand the frequency table is\n\nnces |&gt;\n  dplyr::count(selective, female, highmathsat) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.3:  NCES Data \n  \n    \n    \n      selective\n      female\n      highmathsat\n      n\n    \n  \n  \n    0\n0\n0\n435\n    0\n0\n1\n87\n    0\n1\n0\n420\n    0\n1\n1\n37\n    1\n0\n0\n50\n    1\n0\n1\n104\n    1\n1\n0\n55\n    1\n1\n1\n29\n  \n  \n  \n\n\n\n\n\n\n\n1.2.3 Reducing Alcohol Consumption\n\n1.2.3.1 The What-If? Study\nThis dataset is available with fciR::whatifdat.\nThe statistical summary is\n\ndata(\"whatifdat\", package = \"fciR\")\nwhatifdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\nwhatifdat\n\n\nNumber of rows\n165\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nT\n0\n1\n0.48\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nA\n0\n1\n0.64\n0.48\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nH\n0\n1\n0.36\n0.48\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nY\n0\n1\n0.32\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\n\n\n\nThe frequency table is\n\nwhatifdat |&gt;\n  dplyr::count(`T`, A, H, Y) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.4:  The What-If? Study \n  \n    \n    \n      T\n      A\n      H\n      Y\n      n\n    \n  \n  \n    0\n0\n0\n0\n15\n    0\n0\n0\n1\n3\n    0\n0\n1\n0\n3\n    0\n0\n1\n1\n11\n    0\n1\n0\n0\n36\n    0\n1\n0\n1\n4\n    0\n1\n1\n0\n4\n    0\n1\n1\n1\n9\n    1\n0\n0\n0\n15\n    1\n0\n0\n1\n3\n    1\n0\n1\n0\n3\n    1\n0\n1\n1\n7\n    1\n1\n0\n0\n27\n    1\n1\n0\n1\n3\n    1\n1\n1\n0\n9\n    1\n1\n1\n1\n13\n  \n  \n  \n\n\n\n\n\n\n1.2.3.1.1 The Double What-If? Study\nThis dataset is available with fciR::doublewhatifdat.\nThe statistical summary is\n\ndata(\"doublewhatifdat\", package = \"fciR\")\ndoublewhatifdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ndoublewhatifdat\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAD0\n0\n1\n0.50\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nVL0\n0\n1\n0.62\n0.49\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nU\n0\n1\n0.49\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nT\n0\n1\n0.50\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nA\n0\n1\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nAD1\n0\n1\n0.28\n0.45\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\nVL1\n0\n1\n0.58\n0.49\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\nThe DAG for the Double What-If? study in the dagitty version is\n\nscm &lt;- list()\nscm &lt;- within(scm, {\n  the_nodes &lt;- c(\"U\" = \"Unmeasured, healthy behavior (U=1)\", \n                 \"AD0\" = \"Adherence time 0\", \n                 \"VL0\" = \"Viral Load time 0\", \n                 \"T\" = \"Naltrexone (T=1)\", \n                 \"A\" = \"Reduced drinking (A=1)\", \n                 \"AD1\" = \"Adherence time 1\", \n                 \"VL1\" = \"Viral Load time 1\")\n  coords &lt;- data.frame(\n    name = names(the_nodes),\n    x = c(2, 3, 4, 1, 2, 3, 4),\n    y = c(2, 2, 2, 1, 1, 1, 1)\n  )\n  dag &lt;- dagify(\n    AD0 ~ U,\n    VL0 ~ AD0,\n    A ~ `T` + U,\n    AD1 ~ A,\n    VL1 ~ AD0 + AD1 + U,\n  outcome = \"VL1\",\n  exposure = \"T\",\n  latent = \"U\",\n  coords = coords,\n  labels = the_nodes)\n  \n  # this is the only technique known to have a subscript in a DAG\n  # IMPORTANT: the expression must be exactly in alphabetical order\n  the_text_labels &lt;- c(\n    expression(bold(A)), expression(bold(AD[0])),expression(bold(AD[1])),\n    expression(bold(T)), expression(bold(U)), expression(bold(VL[0])), \n    expression(bold(VL[1])))\n  \n  # status' colors\n  colrs &lt;- c(\"latent\" = \"palevioletred\", \"exposure\" = \"mediumspringgreen\", \n             \"outcome\" = \"cornflowerblue\")\n  # plot the DAG\n  plot &lt;- dag |&gt; \n    tidy_dagitty() |&gt;\n    ggdag_status(color = status, text = FALSE) +\n    geom_dag_text(size = 5, color = \"white\", fontface = \"bold\",\n      parse = TRUE, label = the_text_labels) +\n    scale_color_manual(values = colrs, na.value = \"honeydew3\") +\n    scale_fill_manual(values = colrs, na.value = \"honeydew3\") +\n    ggdag::theme_dag_blank(panel.background = \n                             element_rect(fill=\"snow\", color=\"snow\")) +\n    theme(title = element_text(color = \"darkblue\"),\n          legend.position = \"bottom\",\n          legend.title = element_blank()) +\n    labs(title = \"The Double What-If? Study\")\n})\nscm$plot\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\nFigure 1.1: The Double What-If? Study\n\n\n\n\nand the code for doublewhatifsim.R is\n\n#' \\code{doublewhatifsim} script rewritten\n#' \n#' \\code{doublewhatifsim} script rewritten.\n#' \n#' Simulate the What-If study data.\n#'\n#' @param n Nb of observations.\n#' @param seed Integer, the seed used for random numbers.\n#'\n#' @return Dataframe\ndoublewhatifsim &lt;- function(n = 1000, seed = 444) {\n  \n  set.seed(seed)\n  \n  # variables each with probability 0.5\n  U &lt;- rbinom(n, size = 1, prob = 0.5)\n  # probability of AD0 depends on U\n  AD0prob &lt;- 0.2 + 0.6 * U\n  # generate independent bernoulli variables with varying probabilities\n  AD0 &lt;- rbinom(n, size = 1, prob = AD0prob)\n  VL0prob &lt;- 0.8 - 0.4 * AD0\n  VL0 &lt;- rbinom(n, size = 1, prob = VL0prob)\n  `T` &lt;- rbinom(n, size = 1, prob = 0.5)\n  Aprob &lt;- 0.05 + `T` * U * 0.8\n  A &lt;- rbinom(n, size = 1, prob = Aprob)\n  AD1prob &lt;- 0.1 + 0.8 * A\n  AD1 &lt;- rbinom(n, size = 1, prob = AD1prob)\n  VL1prob &lt;- VL0prob + 0.1 - 0.45 * AD1\n  VL1 &lt;- rbinom(n, size =1 , prob = VL1prob)\n  \n  data.frame(\n    \"AD0\" = AD0,\n    \"VL0\" = VL0,\n    \"T\" = `T`,\n    \"A\" = A,\n    \"AD1\" = AD1,\n    \"VL1\" = VL1\n  )\n}\n\n\n\n\n\n1.2.4 General Social Survey\nThis dataset is available with fciR::gss.\nThe statistical summary is\n\ndata(\"gss\", package = \"fciR\")\ngss |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ngss\n\n\nNumber of rows\n2348\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n0\n1.00\n49.12\n18.24\n18\n34\n48\n63\n99\n▇▇▇▅▁\n\n\ngt65\n0\n1.00\n0.22\n0.41\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nattend\n16\n0.99\n0.34\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\ngthsedu\n0\n1.00\n0.39\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nmagthsedu\n180\n0.92\n0.24\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\npagthsedu\n583\n0.75\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nfair\n798\n0.66\n0.49\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nowngun\n818\n0.65\n0.35\n0.48\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nconservative\n101\n0.96\n0.33\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\ntrump\n0\n1.00\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nwhite\n0\n1.00\n0.72\n0.45\n0\n0\n1\n1\n1\n▃▁▁▁▇\n\n\nfemale\n0\n1.00\n0.55\n0.50\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\n\n\n1.2.5 A Cancer Clinical Trial\nThis dataset is available with fciR::cogdat.\nThe statistical summary is\n\ndata(\"cogdat\", package = \"fciR\")\ncogdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ncogdat\n\n\nNumber of rows\n1190\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nA1\n0\n1\n0.47\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nH2\n0\n1\n0.38\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nA2\n0\n1\n0.13\n0.33\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nY\n0\n1\n0.26\n0.44\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\n\n\n\nand the frequency table is\n\ndf &lt;- cogdat |&gt;\n  filter(Y == 1) |&gt;\n  count(A1, H2, A2, name = \"nY\")\ncogdat |&gt;\n  count(A1, H2, A2) |&gt;\n  left_join(df) |&gt;\n  mutate(nY = dplyr::if_else(is.na(nY), 0, nY),\n         prop = round(nY / n, 2)) |&gt;\n  gt::gt()\n\nJoining with `by = join_by(A1, H2, A2)`\n\n\n\n\n\n\nTable 1.5:  A Hypothetical Cancer Clinical Trial \n  \n    \n    \n      A1\n      H2\n      A2\n      n\n      nY\n      prop\n    \n  \n  \n    0\n0\n0\n410\n120\n0.29\n    0\n0\n1\n30\n0\n0.00\n    0\n1\n0\n160\n30\n0.19\n    0\n1\n1\n30\n20\n0.67\n    1\n0\n0\n280\n30\n0.11\n    1\n0\n1\n20\n10\n0.50\n    1\n1\n0\n190\n80\n0.42\n    1\n1\n1\n70\n20\n0.29\n  \n  \n  \n\n\n\n\nrm(df)"
  },
  {
    "objectID": "ch01_intro.html#exercises",
    "href": "ch01_intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.3 Exercises",
    "text": "1.3 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch02_probability.html#conditional-probability",
    "href": "ch02_probability.html#conditional-probability",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.1 Conditional Probability",
    "text": "2.1 Conditional Probability\n\n2.1.1 Law of total probability\nIt is important to note that \\(\\sum_i{H_i} = H\\), that is \\(H\\) can be partitioned in \\(i\\) non-overlapping partitions.\nThen the law of total probabilities is\n\\[\n\\begin{align*}\nP(A) &= \\sum_i{P(A \\cap H_i)}= \\sum_i{P(A \\mid H_i) P(H_i)} \\\\\n&\\text{and we condition the whole expression with B} \\\\\nP(A \\mid B) &= \\sum_i{P(A \\cap H_i \\mid B)}= \\sum_i{P(A \\mid B, H_i) P(B,H_i)} \\\\\n\\end{align*}\n\\]\nand the multiplication rule is\n\\[\n\\begin{align*}\nP(A, B \\mid C) &= \\frac{P(A, B, C)}{P(C)} \\\\\n&= \\frac{P(A \\mid B, C) P(B, C)}{P(C)} \\\\\n&= \\frac{P(A \\mid B, C) P(B \\mid C) P(C)}{P(C)} \\\\\n&= P(A \\mid B, C) P(B \\mid C)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch02_probability.html#conditional-expectation-and-the-law-of-total-expectation",
    "href": "ch02_probability.html#conditional-expectation-and-the-law-of-total-expectation",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.2 Conditional Expectation and the Law of Total expectation",
    "text": "2.2 Conditional Expectation and the Law of Total expectation\nThe conditional expectation is defined as\n\\[\nE(Y \\mid T) = \\sum_y y P(Y=y \\mid T)\n\\]\nand one way that helps me usually understand it well is that conditioning is the same as filtering the data.\nFor example the conditional expectation of mortality of US resident at the beginning of 2019 \\(E(Y \\mid T = 1) = 0.0088\\)\n\ndata(\"mortality_long\", package = \"fciR\")\nmortality_long |&gt;\n  # condition on T = 1\n  filter(`T` == 1) |&gt;\n  # compute probabilities\n  mutate(prob = n / sum(n)) |&gt;\n  # compute expectation for each possible value of Y\n  group_by(Y) |&gt;\n  summarize(EYT1 = sum(Y * prob)) |&gt;\n  # output results in a named vector\n  pull() |&gt;\n  setNames(nm = c(\"EY0T1\", \"EY1T1\"))\n\n EY0T1  EY1T1 \n0.0000 0.0088 \n\n\nwhere \\(E(Y=0 \\mid T=1) = 0\\) because when \\(Y=0 \\implies 0 \\cdot P(Y=0) = 0\\) and since \\(Y\\) is binary \\(E(Y=1 \\mid T=1) = P(Y=1 \\mid T=1)\\).\n\nAnalogous to the law of total probability is the law of ttal expectation, also called double expectation theorem.\n\nThis law is used extensively in this textbook.\n\\[\n\\begin{align*}\nE(Y \\mid T) &= E_{H \\mid T}(E(Y \\mid H, T)) \\\\\n&= \\sum_h \\left[ \\sum_y y P(Y=y \\mid H=h, T) \\right] P(H=h \\mid T)\n\\end{align*}\n\\]\nAlso the following equivalence is used very often in this book.\n\\[\n\\begin{align*}\nE(H \\mid T) = \\sum_h h P(H=h \\mid T) = E_{H \\mid T} (H)\n\\end{align*}\n\\]\n\n2.2.1 Mean independence and conditional mean independence\n\nThe random variable \\(Y\\) is mean independent of \\(T\\) if\n\n\\[\nE(Y \\mid T) = E(Y)\n\\]\n\nand is conditionally mean independent of \\(T\\) given \\(H\\) is\n\n\\[\nE(Y \\mid T, H) = E(Y \\mid H)\n\\]\nand the conditional uncorrelation and uncorrelation are\n\\[\nE(YT \\mid H) = E(Y \\mid H)E(T \\mid H) \\implies \\text{conditionally uncorrelated} \\\\\nE(YT) = E(Y)E(T) \\implies \\text{uncorrelated}\n\\]\n\nIt happens that conditional mean independence implies conditional uncorrelation, but not the other way around.\n\nWe prove it as follows\n\\[\n\\begin{align*}\n\\text{assume Y is conditionally independent of T given H then} \\\\\nE(Y \\mid T, H) &= E(Y \\mid H) \\\\ \\\\\n\\text{using double expectation theorem} \\\\\nE(TY \\mid H) &= E_{T \\mid H}(E(TY \\mid H, T)) \\\\\n\\text{expectation is a linear operator} \\\\\n&= E_{T \\mid H}(TE(Y \\mid H, T)) \\\\\n\\text{by conditional mean independence from above} \\\\\n&= E_{T \\mid H}(TE(Y \\mid H)) \\\\\n\\text{expectation is a linear operator} \\\\\n&= E_{T \\mid H}(T)E_{T \\mid H}(E(Y \\mid H)) \\\\\n\\text{which proves the conditional uncorrelation} \\\\\n&= E(T \\mid H)E(Y \\mid H)\n\\end{align*}\n\\]\n\n\n2.2.2 Regression model\n\nA statistical model for a conditional expectation is called a regression model.\n\nFor a binary dataset the regression model is said to be saturated or nonparametric because the 4 proportions, or coefficients, cover all possibilities.\n\\[\nE(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T + \\beta_3 H * T\n\\]\nWhen unsaturated or parametric the model makes an assumption. For example the following model assumes no interaction.\n\\[\nE(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T\n\\]\n\n\n2.2.3 Nonlinear parametric models\nThe three parametric models, also the most well-known, used in the book are\n\\[\n\\begin{align*}\n\\text{linear: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p \\\\\n\\text{loglinear: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\exp{(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)} \\\\\n\\text{logistic: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\text{expit}(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)\n\\end{align*}\n\\]\nThe function expit() used by the author is actually the same as gtools::inv.logit(), boot::inv.logit() or stats::plogis(). In this project we use stats::plogis() to minimize dependencies since it is in base R."
  },
  {
    "objectID": "ch02_probability.html#estimation",
    "href": "ch02_probability.html#estimation",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.3 Estimation",
    "text": "2.3 Estimation\nThis section is extremely important as it is used extensively, especially from the chapter 6 on.\nLets use the What-if example to illustrate the mathematics of it. In that case, we have 3 covariates, \\(T\\) for naltrexone, \\(A\\) for reduced drinking and \\(H\\) for unsuppressed viral load. In the data set we have 165 observations, therefore \\(i=165\\).\nLet \\(X_i\\) denote the collection of \\(X_{ij}\\) for \\(j=1, \\ldots, p\\) where \\(p\\) is the number of covariates which is 3 in the What-if dataset. \\(X_i\\) is a horizontal vector.\nSo for the What-if study\n\\[\nX_i = \\begin{bmatrix}A_i & T_i & H_1 \\end{bmatrix}\n\\]\nand \\(\\beta\\) is a vertical vector.\n\\[\n\\beta = \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix}\n\\]\ntherefore\n\\[\n\\begin{align*}\nX_i \\beta &= \\begin{bmatrix}A_i & T_i & H_1 \\end{bmatrix} \\times \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} \\\\\n&= \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i\n\\end{align*}\n\\]\nwe also have\n\\[\n\\begin{align*}\nX_i^T (Y_i - X_i \\beta) &= \\begin{bmatrix}A_i \\\\ T_i \\\\ H_1 \\end{bmatrix}(Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\n&= \\begin{bmatrix}\nA_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\nT_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\nH_1 (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i)\n\\end{bmatrix}\n\\end{align*}\n\\]\nAlso, we can estimate the unconditional expectation of the binary outcome \\(Y\\) as\n\\[\n\\hat{E}(Y) = \\frac{1}{n} \\sum_{i=1}^n Y_i\n\\]\nwhich returns the proportion of participants with unsuppressed viral load after 4 months\n\n#label: ch02_whatifdat\ndata(\"whatifdat\", package = \"fciR\")\nmean(whatifdat$Y)\n\n[1] 0.3212121\n\n\nThis is an unbiased estimator for \\(E(Y)\\) because \\(E(\\hat{E}(Y)) = E(Y)\\)\nNow let the saturated model for \\(E(Y)\\) be\n\\[\nE(Y) = \\beta\n\\]\nwith the following estimating equation\n\\[\nU(\\beta) = \\sum_{i=1}^n (Y_i - \\beta) = 0\n\\]\nand simple algebra, using the estimate of \\(\\hat{E}(Y)\\) from above gives \\(\\beta = \\hat{E}(Y)\\) and we use the notation \\(\\hat{E}(Y) = E(Y \\mid X, \\beta)\\) to show its dependency on the data and \\(\\beta\\).\n\\[\n\\begin{align*}\nU(\\beta) &= \\sum_{i=1}^n (Y_i - \\beta) \\\\\n&= \\sum_{i=1}^n \\left[ Y_i - E(Y \\mid X, \\beta) \\right] \\\\\n&= 0\n\\end{align*}\n\\]\nand to do the actual calculations we use a variation of the estimating equation as follows\n\\[\n\\begin{align*}\nU(\\beta) = \\sum_{i=1}^n X_i^T \\left[ Y_i - E(Y \\mid X, \\beta) \\right] = 0\n\\end{align*}\n\\]\nAs an example, we fit the logistic regression model with the What-if dataset\n\\[\nE(Y \\mid A,T,H) = expit(\\beta_0 + \\beta_A A + \\beta_T T + \\beta_H H)\n\\]\n\nwhatif.mod &lt;- glm(Y ~ A + `T` + H, family = \"binomial\", data = whatifdat)\nsummary(whatif.mod)\n\n\nCall:\nglm(formula = Y ~ A + T + H, family = \"binomial\", data = whatifdat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.5234     0.4125  -3.693 0.000222 ***\nA            -0.5647     0.4214  -1.340 0.180248    \nT            -0.2254     0.4147  -0.543 0.586790    \nH             2.7438     0.4158   6.600 4.12e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.17  on 164  degrees of freedom\nResidual deviance: 150.83  on 161  degrees of freedom\nAIC: 158.83\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe coefficients, \\(\\beta_0, \\beta_A, \\beta_T, \\beta_H\\) are obtained with coef() which is an alias for coefficient(). The example in the book uses lmod$coef which is not a recommended coding practice. The data should be obtained with an extractor function such as coef().\n\ncoef(whatif.mod)\n\n(Intercept)           A           T           H \n -1.5233530  -0.5647068  -0.2254112   2.7438245 \n\n\nand since the model is binomial then the inverse function to convert from the logit scale to the natural scale is stats::plogis().\nTherefore the parametric estimate is\n\\[\nE(Y \\mid A=1, T=1, H=1) = \\beta_0 + \\beta_A \\cdot 1 + \\beta_T \\cdot 1 + \\beta_H \\cdot 1\n\\]\n\nstats::plogis(sum(coef(whatif.mod)))\n\n[1] 0.6059581\n\n\nwe can also use the linear model directly, thus avoiding a call to plogis and making the code a little more robust. This technique is used quite often in the book, especially starting from chapter 6 on.\n\nwhatif.newdata &lt;- whatifdat |&gt;\n  filter(A == 1, `T` == 1, H == 1)\nwhatif.EYT1 &lt;- predict(whatif.mod, newdata = whatif.newdata, \n                            type = \"response\") |&gt;\n  mean()\nwhatif.EYT1\n\n[1] 0.6059581\n\n\nand we can validate the result directly using the dataset\n\nwhatifdat.est &lt;- whatifdat |&gt;\n  filter(A == 1, `T` == 1, H == 1) |&gt;\n  pull(Y) |&gt;\n  mean()\nwhatifdat.est\n\n[1] 0.5909091\n\n\nwhich is close to the parametric estimate."
  },
  {
    "objectID": "ch02_probability.html#sampling-distributions-and-the-bootstrap",
    "href": "ch02_probability.html#sampling-distributions-and-the-bootstrap",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.4 Sampling Distributions and the Bootstrap",
    "text": "2.4 Sampling Distributions and the Bootstrap\nThe previous section prived and estimate. But how variable is this estimator? One way would be to report the standard error \\(\\sigma_{\\hat{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Therefore, since \\(\\sigma^2 = p(1-p)\\) for a binomial distribution\n\n# the sample size\nwhatifdat.n &lt;- sum(whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H == 1)\n# whatifdat.n\n\n# the standard deviation\nwhatifdat.sd &lt;- sqrt(whatifdat.est * (1- whatifdat.est))\n# whatifdat.sd\n\n# the standard error\nwhatifdat.se &lt;- whatifdat.sd / sqrt(whatifdat.n)\nwhatifdat.se\n\n[1] 0.1048236\n\n\nNote that the formula used by B. Brumback is using the standard deviation of the population \\(p(1-p)\\) which, maybe, is not entirely the right way since the standard error of the mean is defined using the standard deviation of the sample.\nThat is Brumback uses\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) * sqrt(n-1) / n\n\n[1] 0.1048236\n\n\nwhen the correct definition is\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H == 1]\nsd(x) / sqrt(length(x))\n\n[1] 0.1072903\n\n\n\nAnother way would be by reporting the sampling distribution of our estimator.\n\n\nthe sampling distribution is centered at the true value of our estimand and it has a standard deviation equal to the true standard error of our estimator.\n\nThe section 2.4 will not be repeated here, yet it is very instructive and cover a topic that we end to forget. We will just repeat the calculations below. They\nThe sim() function in section 2.4, p. 31 is coded in fciR::sim_intervals() and its alias fciR::sim().\nRun and verify with author’s results on p. 32\n\nd &lt;- fciR::sim_intervals(nsim = 500, n = 500, seed = 1013)\nstopifnot(abs(d$bad - 0.8374) &lt; 0.01, abs(d$good - 0.948) &lt; 0.01)\nd\n\n$bad\n[1] 0.84\n\n$good\n[1] 0.946\n\n\n\n2.4.1 Bootstrapping\n\nAs we have seen, replacing \\(\\mu\\) and \\(\\sigma\\) with estimates leads to some problems, but we can still form an interpretable 95% confidence interval. In many situations, we do not have a good estimate of \\(\\sigma\\). In those cases, we often can turn to bootstrap.\n\nBootstrapping in base R is done with the boot from the boot package. Another option is the rsample package which isthe tidyverse way of doing it. Both methods will be used in this project. In the package fciR, the functions boot_run and boot_est use the classic R package boot. The functions bootidy_run and bootidy_est are similar but use the rsample package.\n\n2.4.1.1 boot package\nWe use boot::boot() for boostrapping and boot::boot.ci() to compute the confidence interval of the estimand using the logistic model as done by the lmodboot function in section 2.4. We only run 100 boot samples since it is only an example. The default value is 1000.\nThe first 3 arguments data, statistic and R are specific to boot, the ... is for extra arguments used by lmodboot. The details of lmdboots can be found in the package fciR or in its alias fciR::prob_lmod.\n\nprob_lmod(whatifdat, formula = Y ~ `T` + A + H)\n\n    term  estimate std.err\n1 logitP 0.4303535      NA\n\n\nand we show the details on how to do the bootstrapping with the boot package as follows.\n\n# define the function used by boot\nwhatifdat.fnc &lt;- function(data, ids, ...) {\n  dat &lt;- data[ids, ]\n  df &lt;- prob_lmod(data = dat, ...)\n  # create the named vector\n  out &lt;- c(df$estimate)\n  names(out) &lt;- df$term\n  out\n}\n# run the bootstrapping\nwhatifdat.boot &lt;- boot::boot(data = whatifdat, statistic = whatifdat.fnc, \n                             R = 100, formula = Y ~ `T` + A + H)\n# get the confidence interval for every term in the bootstrap object\nwhatifdat.out &lt;- lapply(X = seq_along(whatifdat.boot$t0), FUN = function(i, alpha = 0.05) {\n  est &lt;- whatifdat.boot$t0[i]\n  # the method used to find the interval\n  the_method &lt;- \"norm\"\n  # extract the interval from the boot object\n  ci &lt;- boot::boot.ci(whatifdat.boot, conf = 1 - alpha, type = the_method, \n                      index = i)$normal\n  # the dataframe of results, follow the format from rsample package\n  data.frame(\"term\" = names(est),\n    \".lower\" = plogis(ci[2]),\n    \".estimate\" = plogis(unname(est)),\n    \".upper\" = plogis(ci[3]),\n    \".alpha\" = alpha,\n    \".method\" = the_method)\n  })\n# bind the data.frame in one.\n# In this case it makes no difference as there is only one.\n# and we just want to show how it works in case of several items\nwhatifdat.out &lt;- do.call(rbind, whatifdat.out)\nwhatifdat.out\n\n    term    .lower .estimate    .upper .alpha .method\n1 logitP 0.4023362 0.6059581 0.7694102   0.05    norm\n\n\n\n\n2.4.1.2 rsample package\nAnother way package that could be used for boostrapping is rsample::bootstrap which uses the tidyverse way of R programming.\nTo be fully tidyverse-like we recode the prob_lmod function from above and call it prob_lmod_td where the td suffix stands for tidyverse. Note the use of a formula as an argument to the function and the rlang package which is a great tool to learn for quality code in R.\n\nprob_lmod_td &lt;- function(data, formula = Y ~ `T` + A + H,\n                         condition.names = NULL) {\n  # independent variables from the formula\n  f_vars &lt;- all.vars(rlang::f_rhs(formula))\n  # if condition.names is NULL then use all independent variables\n  # which is the same as saying there is no condition\n  if (is.null(condition.names)) condition.names &lt;- f_vars\n  stopifnot(all(condition.names %in% f_vars))\n\n  # add intercept to conditions\n  x0 &lt;- \"(Intercept)\"  # name of intercept used by lm, glm, etc.\n  condition.names &lt;- c(x0, condition.names)\n  \n  \n  fit &lt;- glm(formula = formula, family = \"binomial\", data = data) |&gt;\n    tidy()\n  fit |&gt;\n    filter(term %in% condition.names) |&gt;\n    summarize(term = \"logitP\",\n              estimate = sum(estimate),\n              # don't know the std.err so no t-intervals\n              std.err = NA_real_)\n}\n\n\nprob_lmod_td(whatifdat, formula = Y ~ `T` + A + H)\n\n# A tibble: 1 × 3\n  term   estimate std.err\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 logitP    0.430      NA\n\n\nThe following function is inspired from rsample bootstrap.\nWe obtain the confidence interval using rsample::int_pctl which uses a percentile interval instead of assuming a normal distribution as Ms Brumback does. The result are not materially different in the current case.\n\nwhatifdat |&gt;\n  rsample::bootstraps(times = 1000, apparent = FALSE) |&gt;\n  mutate(results = map(splits, function(x) {\n    dat &lt;- analysis(x)\n    prob_lmod_td(dat, formula = Y ~ `T` + A + H)})) |&gt;\n  rsample::int_pctl(statistics = results, alpha = 0.05) |&gt;\n  mutate(term = \"P\",\n         .lower = plogis(.lower),\n         .estimate = plogis(.estimate),\n         .upper = plogis(.upper))\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 P      0.440     0.610  0.771   0.05 percentile\n\n\nand lmodboot() is fciR::boot_lmod() with the alias fciR::lmodboot()\nFinally we run fciR::boot_lmod() and verify against the author’s results on p.34\n\nfciR::boot_est(whatifdat, func = fciR::prob_lmod, times = 500,\n                 alpha = 0.05, seed = 123, transf = \"expit\", terms = NULL,\n                 formula = Y ~ `T` + A + H)\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 P      0.427     0.608  0.750   0.05 percentile"
  },
  {
    "objectID": "ch02_probability.html#exercises",
    "href": "ch02_probability.html#exercises",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch03_outcomes.html#potential-outcomes-and-the-consistency-assumption",
    "href": "ch03_outcomes.html#potential-outcomes-and-the-consistency-assumption",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.1 Potential Outcomes and the Consistency Assumption",
    "text": "3.1 Potential Outcomes and the Consistency Assumption\n\nThe utility of the potential outcome framework hinges on the validity of a consistency assumption that links potential outcome to obe=served outcomes.\n\n\nIt is worth emphasizing that the consistency assumption requires the potential outcomes to be well-defined.\n\n\nA major problem for causal inference is that we can only observe one potential outcome per participant. This is the Fundamental Problem of Causal Inference (FPCI).\n\nWe can therefore classify 4 causal types\n\\[\n\\text{Doomed:} \\: Y(0) = Y(1) = 0 \\\\\n\\text{Responsive:} \\: Y(0) = 0,  Y(1) = 1 \\\\\n\\text{Harmed:} \\: Y(0) = 1,  Y(1) = 0 \\\\\n\\text{Immune:} \\: Y(0) = Y(1) = 1 \\\\\n\\]\nIf we consider that the treatment cannot be harmful, in that case a patient with \\(Y=0\\) must be doomed and an untreated patient with \\(Y=0\\) must be immune.\n\\[\n\\text{Assuming the treatment cannot be harmful} \\\\ \\\\\n\\text{Doomed:} \\: Y(0) = Y(1) = 0 \\\\\n\\text{Responsive:} \\: Y(0) = 0,  Y(1) = 1 \\\\\n\\text{Immune:} \\: Y(0) = 1 \\\\\n\\]"
  },
  {
    "objectID": "ch03_outcomes.html#circumventing-the-fundamental-problem-of-causal-inference",
    "href": "ch03_outcomes.html#circumventing-the-fundamental-problem-of-causal-inference",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.2 Circumventing the Fundamental Problem of Causal Inference",
    "text": "3.2 Circumventing the Fundamental Problem of Causal Inference\nThere are 2 solutions to the FPCI\n\nScientific solution: We use scientific theory to measure both potential outcomes. For example by using 2 different participants that are considered identical for the purpose of the scientific study.\nStatistical solution: The treatment is randomized independently of any existing data. As a result we obtain mean independence and the mean can be used to draw inference. This approach has some issues\n\nThe difference between the outcome with or without treatment can still be caused by chance.\nThe study populaiton may not be relevant anymore.\nExtrapolating an average result to an individdual might be problematic."
  },
  {
    "objectID": "ch03_outcomes.html#effect-measures",
    "href": "ch03_outcomes.html#effect-measures",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.3 Effect Measures",
    "text": "3.3 Effect Measures\nTo estimate the association measures we use the parametric method with the glm function.\n\nWe certainly do not believe that our binary outcomes the distributional assumtptions for the gaussian and poisson models, but we are only using the glm function to solve estimating equations.\n\n\ndata(\"gss\", package = \"fciR\")\ngssrcc &lt;- gss[, c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")]\ngssrcc &lt;- gss[complete.cases(gssrcc), ]\nstopifnot(nrow(gssrcc) == 2348 - 180)  # see comment on p. 46\n\n\nuncond &lt;- fciR::boot_est(gssrcc, func = fciR::meas_effect_uncond, times = 500,\n                 alpha = 0.05, transf = \"exp\",\n                 terms = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"), \n                 formula = trump ~ gthsedu)\n\n\ngt_measures(uncond, \n            title = \"Table 3.2\", \n            subtitle = paste(\"4 Association Measures Relating\", \n            \"&lt;em&gt;More than High School Education&lt;/em&gt; to &lt;em&gt;Voting for Trump&lt;/em&gt;\", \n            sep = \"&lt;br&gt;\"))\n\n\n\n\n\n  \n    \n      Table 3.2\n    \n    \n      4 Association Measures RelatingMore than High School Education to Voting for Trump\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    P0\n0.234\n(0.213, 0.259)\n    P1\n0.272\n(0.239, 0.302)\n    RD\n0.038\n(0.001, 0.077)\n    RR\n1.162\n(1.005, 1.358)\n    RR*\n1.052\n(1.002, 1.109)\n    OR\n1.222\n(1.007, 1.504)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nverify with author’s results, p. 48-49\n\nbb &lt;- data.frame(\n  term = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n  .estimate = c(0.23338, 0.27117, 0.037782, 1.1619, 1.0518, 1.221),\n  .lower = c(0.21030, 0.24095, 0.00078085, 1.0047, 1.0006, 1.0056),\n  .upper = c(0.25647, 0.30139, 0.07478354, 1.3437, 1.1057, 1.4853))\nstopifnot(\n  all((uncond$.estimate - bb$.estimate) &lt; 0.01),\n  all((uncond$.lower - bb$.lower) &lt; 0.1),\n  all((uncond$.upper - bb$.upper) &lt; 0.1))\n\nNow we estimate the four effect measures using conditional association measures.\nTo use the function with conditioning on variables, i.e. filtering on variables, we note that\n\\[\n\\begin{align*}\nY &= \\text{trump} \\\\\nT &= \\text{gthsedu} \\\\\nH_1 &= \\text{magthsedu} \\\\\nH_2 &= \\text{white} \\\\\nH_3 &= \\text{female} \\\\\nH_4 &= \\text{gt65} \\\\\n\\end{align*}\n\\]\nTherefore\n\\[\n\\begin{align*}\nE(Y &\\mid T = 0, H_1 = 1, H_2 = 1, H_3 = 1, H_4 = 1) \\\\\n&\\therefore \\text{because T = 0} \\\\\nY &\\sim H_1 + H_2 + H_3 + H_4 \\\\\n\\text{trump} &\\sim \\text{magthsedu} + \\text{white} + \\text{female} + \\text{gt65} \\\\\n\\end{align*}\n\\]\n\na_formula &lt;- trump ~ gthsedu + magthsedu + white + female + gt65\ncondit &lt;- fciR::boot_est(gssrcc, func = fciR::meas_effect_cond,\n                 times = 500, alpha = 0.05, seed = 1234, transf = \"exp\",\n                 terms = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n                 formula = a_formula, exposure.name = \"gthsedu\",\n                 confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"))\n\n\ngt_measures(condit, \n            title = \"Table 3.3\", \n            subtitle = paste(\"4 Conditional Association or Effect Measures\",\n            \"Relating &lt;em&gt;More than High School Education&lt;/em&gt; \n            to &lt;em&gt;Voting for Trump&lt;/em&gt;\", sep = \"&lt;br&gt;\"))\n\n\n\n\n\n  \n    \n      Table 3.3\n    \n    \n      4 Conditional Association or Effect MeasuresRelating More than High School Education \n            to Voting for Trump\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    P0\n0.257\n(0.191, 0.332)\n    P1\n0.302\n(0.234, 0.372)\n    RD\n0.045\n(0.006, 0.092)\n    RR\n1.180\n(1.023, 1.425)\n    RR*\n1.065\n(1.008, 1.138)\n    OR\n1.257\n(1.031, 1.618)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nverify with author’s results, p.52\n\nbb &lt;- data.frame(\n  term = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n  .estimate = c(0.25458, 0.30101, 0.046438, 1.1824, 1.0664, 1.261),\n  .lower = c(0.18518, 0.22995, 0.0022706, 1.0039, 1.0018, 1.0091),\n  .upper = c(0.32397, 0.37208, 0.0906057, 1.3927, 1.1352, 1.5758))\nstopifnot(\n  all(abs(condit$.estimate - bb$.estimate) &lt; 0.01),\n  all(abs(condit$.lower - bb$.lower) &lt; 0.1),\n  all(abs(condit$.upper - bb$.upper) &lt; 0.1))"
  },
  {
    "objectID": "ch03_outcomes.html#exercises",
    "href": "ch03_outcomes.html#exercises",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch04_measures.html#effect-measure-modification-and-statistical-interaction",
    "href": "ch04_measures.html#effect-measure-modification-and-statistical-interaction",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.1 Effect-Measure Modification and Statistical Interaction",
    "text": "4.1 Effect-Measure Modification and Statistical Interaction\n\n4.1.1 RECOVERY trial\nGet the data.frame for RECOVERY trial\n\ndata(\"recovery\", package = \"fciR\")\n\nrun fciR::boot(), alias meas_effect_modif() with the RECOVERY data set\n\nthe_terms &lt;- c(\n  \"EYT0.M0\", \"EYT0.M1\", \"EYT0.diff\", \"EYT1.M0\", \"EYT1.M1\", \"EYT1.diff\", \n  \"RD.M0\", \"RD.M1\", \"RD.diff\", \"RR.M0\", \"RR.M1\", \"RR.diff\",\n  \"RR*.M0\", \"RR*.M1\", \"RR*.diff\", \"OR.diff\", \"OR.M0\", \"OR.M1\")\nrecovery.out &lt;- fciR::boot_est(data = recovery, func = meas_effect_modif,\n                         times = 100, alpha = 0.05, \n                         terms = the_terms, transf = \"exp\",\n                         formula = Y ~ `T` + M, exposure.name = \"T\",\n                         modifier.name = \"M\")\nrecovery.out\n\n# A tibble: 18 × 6\n   term       .lower .estimate  .upper .alpha .method   \n   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n 1 EYT0.M0    0.767    0.784    0.797    0.05 percentile\n 2 EYT0.M1    0.554    0.590    0.628    0.05 percentile\n 3 EYT0.diff -0.231   -0.194   -0.149    0.05 percentile\n 4 EYT1.M0    0.774    0.793    0.809    0.05 percentile\n 5 EYT1.M1    0.691    0.735    0.777    0.05 percentile\n 6 EYT1.diff -0.102   -0.0588  -0.0135   0.05 percentile\n 7 RD.M0     -0.0159   0.00947  0.0314   0.05 percentile\n 8 RD.M1      0.0978   0.144    0.197    0.05 percentile\n 9 RD.diff    0.0854   0.135    0.186    0.05 percentile\n10 RR.M0      0.980    1.01     1.04     0.05 percentile\n11 RR.M1      1.16     1.24     1.35     0.05 percentile\n12 RR.diff    1.14     1.23     1.33     0.05 percentile\n13 RR*.M0     0.930    1.05     1.16     0.05 percentile\n14 RR*.M1     1.33     1.55     1.86     0.05 percentile\n15 RR*.diff   1.26     1.48     1.77     0.05 percentile\n16 OR.diff    1.45     1.82     2.35     0.05 percentile\n17 OR.M0      0.911    1.06     1.21     0.05 percentile\n18 OR.M1      1.55     1.93     2.49     0.05 percentile\n\n\nverify the results with the author’s on p. 65.\n\nbb &lt;- data.frame(\n  term = c(\"EYT0\", \"EYT0\", \"EYT1\", \"EYT1\", \"RD\", \"RD\",\n           \"EYT0\", \"EYT1\", \"RD\", \"RR\", \"RR\", \"RR\",\n           \"RR*\", \"RR*\", \"RR*\", \"OR\", \"OR\", \"OR\"),\n  group = c(\"M0\", \"M1\", \"M0\", \"M1\", \"M0\", \"M1\",\n           \"diff\", \"diff\", \"diff\", \"M0\", \"M1\", \"diff\",\n           \"M0\", \"M1\", \"diff\", \"M0\", \"M1\", \"diff\"),\n  estimate = c(0.784, 0.593, 0.793, 0.735, 0.01, 0.142,\n          -0.191, -0.059, 0.132, 1.012, 1.239, 1.224,\n          1.046, 1.533, 1.466, 1.059, 1.9, 1.794)\n  )\nrecovery.out$term\n\n [1] \"EYT0.M0\"   \"EYT0.M1\"   \"EYT0.diff\" \"EYT1.M0\"   \"EYT1.M1\"   \"EYT1.diff\"\n [7] \"RD.M0\"     \"RD.M1\"     \"RD.diff\"   \"RR.M0\"     \"RR.M1\"     \"RR.diff\"  \n[13] \"RR*.M0\"    \"RR*.M1\"    \"RR*.diff\"  \"OR.diff\"   \"OR.M0\"     \"OR.M1\"    \n\nids &lt;- match(paste(bb$term, bb$group, sep = \".\"), recovery.out$term)\n# ids\ncomp &lt;- data.frame(bb = bb,\n                   d = recovery.out$.estimate[ids])\n# comp\ncomp$dev &lt;- abs(comp$bb.estimate - comp$d)\n# comp$dev\nstopifnot(all(comp$dev &lt; 0.1))\n\nand we communicate the results in a table\n\n# reformat to use for table and plot\nrecovery.out &lt;- recovery.out %&gt;%\n  separate(col = \"term\", into = c(\"term\", \"group\"), sep = \"[.]\", remove = TRUE)\nfciR::gt_measures_colgrp(recovery.out, var_grp = \"group\",\n                   title = \"Table 4.2 RECOVERY Trial\",\n                   subtitle = \"Effect-measure Modification\")\n\n\n\n\n\n  \n    \n      Table 4.2 RECOVERY Trial\n    \n    \n      Effect-measure Modification\n    \n    \n      Estimator\n      \n        M0\n      \n      \n        M1\n      \n      \n        diff\n      \n    \n    \n      Estimate\n      CI1\n      Estimate\n      CI1\n      Estimate\n      CI1\n    \n  \n  \n    EYT0\n0.784\n(0.767, 0.797)\n0.590\n(0.554, 0.628)\n-0.194\n(-0.231, -0.149)\n    EYT1\n0.793\n(0.774, 0.809)\n0.735\n(0.691, 0.777)\n-0.059\n(-0.102, -0.013)\n    RD\n0.009\n(-0.016, 0.031)\n0.144\n(0.098, 0.197)\n0.135\n(0.085, 0.186)\n    RR\n1.012\n(0.980, 1.040)\n1.245\n(1.159, 1.347)\n1.230\n(1.138, 1.330)\n    RR*\n1.046\n(0.930, 1.158)\n1.547\n(1.330, 1.862)\n1.479\n(1.262, 1.775)\n    OR\n1.059\n(0.911, 1.205)\n1.926\n(1.545, 2.492)\n1.819\n(1.452, 2.350)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nplotting the results makes it easier to see the measures vary among the strata. We can clearly see here significant difference in effect measures between the 2 strata.\nIt supports the observation in the text concerning the lack of effect of dexamethasone without intrusive mechanical ventilation (M0) vs its use with intrusive mechanical ventilation (M1) which is significant.\n\nfciR::ggp_measures_modif(recovery.out, title = \"RECOVERY trial\")\n\n\n\n\nThe gee::gee() function is used to find information on the coefficients and see if they are statistically significant. The same could be done withe glm::glm() but gee offers results with robust statistics which is very useful in practical terms.\nLuckily, the gee() works exactly like the glm() functions, with the same extractor functions coefficients(), effects(), etc. See the documentation of lm with ?lm for more details.\nWe are going through an example just below to illustrate how tthe extractor functions are used which is not shown in the textbook.\n\nlinmod &lt;- gee::gee(Y ~ `T` + M + `T` * M,\n                   id = id,\n                   data = recovery,\n                   family = gaussian)\n\nBeginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27\n\n\nrunning glm to get initial regression estimate\n\n\n(Intercept)           T           M         T:M \n 0.78367235  0.00958608 -0.19070017  0.13200964 \n\nsummary(linmod)\n\n\n GEE:  GENERALIZED LINEAR MODELS FOR DEPENDENT DATA\n gee S-function, version 4.13 modified 98/01/27 (1998) \n\nModel:\n Link:                      Identity \n Variance to Mean Relation: Gaussian \n Correlation Structure:     Independent \n\nCall:\ngee::gee(formula = Y ~ T + M + T * M, id = id, data = recovery, \n    family = gaussian)\n\nSummary of Residuals:\n       Min         1Q     Median         3Q        Max \n-0.7932584  0.2067416  0.2163277  0.2163277  0.4070278 \n\n\nCoefficients:\n               Estimate  Naive S.E.     Naive z Robust S.E.    Robust z\n(Intercept)  0.78367235 0.006975704 112.3431253 0.006826403 114.8001855\nT            0.00958608 0.012170195   0.7876686 0.011778554   0.8138588\nM           -0.19070017 0.017545664 -10.8687918 0.019999408  -9.5352906\nT:M          0.13200964 0.030881745   4.2746820 0.033074076   3.9913327\n\nEstimated Scale Parameter:  0.1770267\nNumber of Iterations:  1\n\nWorking Correlation\n     [,1]\n[1,]    1\n\n\nTo extract the coefficients from the gee object we use the extractor function coefficients() or its alias coef()\n\ncoef(linmod)\n\n(Intercept)           T           M         T:M \n 0.78367235  0.00958608 -0.19070017  0.13200964 \n\n\nand to extract the entire coefficient data to work with it, just use coefficients() with summary()\n\ncoef(summary(linmod))\n\n               Estimate  Naive S.E.     Naive z Robust S.E.    Robust z\n(Intercept)  0.78367235 0.006975704 112.3431253 0.006826403 114.8001855\nT            0.00958608 0.012170195   0.7876686 0.011778554   0.8138588\nM           -0.19070017 0.017545664 -10.8687918 0.019999408  -9.5352906\nT:M          0.13200964 0.030881745   4.2746820 0.033074076   3.9913327\n\n\nand in this case we are concerned about how significant the interaction is. Therefore the Robust z is extracted with coefficients() alias coef()\n\ncoef(summary(linmod))[, \"Robust z\"]\n\n(Intercept)           T           M         T:M \n114.8001855   0.8138588  -9.5352906   3.9913327 \n\n\nwe see that \\(T:M\\) is 3.99 standard deviations away from zero which will give us the 2-sided p-value that is significant\n\nz &lt;- coef(summary(linmod))[\"T:M\", \"Robust z\"]\n2 * (1 - pnorm(z))\n\n[1] 6.570304e-05\n\n\n\n\n4.1.2 NCES\nWe process the NCES data the same way we did for the RECOVERY trial.\nRun the bootsrap\n\nnces.out &lt;- fciR::boot_est(data = nces, func = meas_effect_modif,\n                     times = 100, alpha = 0.05, transf = \"exp\",\n                     terms = the_terms,\n                     formula = highmathsat ~ female + selective, \n                     exposure.name = \"female\", modifier.name = \"selective\")\n\nverify the results with the author’s on p. 70.\n\nbb &lt;- data.frame(\n  term = c(\"EYT0\", \"EYT0\", \"EYT1\", \"EYT1\", \"RD\", \"RD\",\n           \"EYT0\", \"EYT1\", \"RD\", \"RR\", \"RR\", \"RR\",\n           \"RR*\", \"RR*\", \"RR*\", \"OR\", \"OR\", \"OR\"),\n  group = c(\"M0\", \"M1\", \"M0\", \"M1\", \"M0\", \"M1\",\n           \"diff\", \"diff\", \"diff\", \"M0\", \"M1\", \"diff\",\n           \"M0\", \"M1\", \"diff\", \"M0\", \"M1\", \"diff\"),\n  estimate = c(0.167, 0.675, 0.081, 0.345, -0.086, -0.33,\n          0.509, 0.264, -0.244, 0.486, 0.511, 1.052,\n          0.907, 0.496, 0.547, 0.44, 0.254, 0.576)\n  )\nids &lt;- match(paste(bb$term, bb$group, sep = \".\"), nces.out$term)\ncomp &lt;- data.frame(bb = bb, d = nces.out$.estimate[ids])\ncomp$dev &lt;- abs(comp$bb.estimate - comp$d)\n# comp$dev\nstopifnot(all(comp$dev &lt; 0.1))\n\nand the table is\n\n# reformat to use for table and plot\nnces.out &lt;- nces.out %&gt;%\n  separate(col = \"term\", into = c(\"term\", \"group\"), sep = \"[.]\", remove = TRUE)\nfciR::gt_measures_colgrp(nces.out, var_grp = \"group\",\n                   title = \"Table 4.3 NCES data\",\n                   subtitle = \"Effect-measure Modification\")\n\n\n\n\n\n  \n    \n      Table 4.3 NCES data\n    \n    \n      Effect-measure Modification\n    \n    \n      Estimator\n      \n        M0\n      \n      \n        M1\n      \n      \n        diff\n      \n    \n    \n      Estimate\n      CI1\n      Estimate\n      CI1\n      Estimate\n      CI1\n    \n  \n  \n    EYT0\n0.165\n(0.136, 0.199)\n0.678\n(0.611, 0.749)\n0.513\n(0.443, 0.588)\n    EYT1\n0.082\n(0.054, 0.103)\n0.337\n(0.245, 0.433)\n0.256\n(0.163, 0.357)\n    RD\n-0.083\n(-0.134, -0.044)\n-0.340\n(-0.460, -0.227)\n-0.257\n(-0.382, -0.135)\n    RR\n0.491\n(0.299, 0.677)\n0.493\n(0.361, 0.655)\n1.005\n(0.654, 1.726)\n    RR*\n0.909\n(0.859, 0.951)\n0.484\n(0.367, 0.616)\n0.533\n(0.404, 0.677)\n    OR\n0.446\n(0.259, 0.644)\n0.239\n(0.136, 0.396)\n0.536\n(0.280, 1.037)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nand we plot the results\n\nfciR::ggp_measures_modif(nces.out, title = \"NCES data\")\n\n\n\n\nWe observe that\n\nRD: Risk difference shows that using selection to accept more women seem to decrease the % of school with hogh math SAT\nOR and RRstar: Show the same results as RD\nRR: Indicates that selection, in relative terms has no significant effect"
  },
  {
    "objectID": "ch04_measures.html#qualitative-agreement-of-effect-measures-in-modification",
    "href": "ch04_measures.html#qualitative-agreement-of-effect-measures-in-modification",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.2 Qualitative Agreement of Effect Measures in Modification",
    "text": "4.2 Qualitative Agreement of Effect Measures in Modification\nThis section relies heavily on the paper from Shannin and Brumback (2021) Jake Shannin (2021). It used a Monte-Carlo simulation in java by jake running 1000000 times for six effect measures (the 4 in this chapter, the hazard ratio HR and the recovery ration HR*).\nFor the purpose of this project we only simulate the 4 effect measures discussed so far (RD, RR, RR* and OR). We use R package MonteCarlo with 5000 repetitions. The distribution used for simulation is the beta distribution which is generally used for values in [0,1]. It is also used as a prior of binomial regression in Bayes analysis which is the subject covered a little later in this section. Regardless if the 6 effects measures from Jake Shannin (2021) or the 4 from Brumback (2022) are used, the process and conclusion are the same.\nWe point out that using the distribution \\(Beta(1, 1) \\sim Uniform(0, 1)\\) is equivalent to running a grid search. It is also equivalent to the uniform distribution used in Jake Shannin (2021).\n\n4.2.1 Simulate the effect measures\nWe run the Monte-Carlo simulation without constraint and with \\(Beta(1,1)\\) which is equivalent to \\(Uniform(0,1)\\) used by Jake Shannin (2021).\n\ngridsim &lt;- fciR::mc_beta_effect_measures(shape1 = 1, shape2 = 1, nrep = 5000)\n\nGrid of  1  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n# gridsim\n\nwhich gives the vector of percentages\n\nunlist(gridsim)\n\n             RD_RR          RD_RRstar              RD_OR          RR_RRstar \n            0.0000             0.0000             0.0000             0.0000 \n             RR_OR          RRstar_OR RD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR \n            0.0000             0.0000             0.0258             0.0246 \nRD_OR_vs_RR_RRstar       RD_RR_RRstar           RD_RR_OR       RD_RRstar_OR \n            0.0000             0.0000             0.0534             0.0518 \n      RR_RRstar_OR    RD_RR_RRstar_OR               NONE \n            0.0000             0.8444             0.0000 \n\n\nFor the following discussion, we must note the following about the vector of percentages returned by the simulation.\n\nPairwise events: Some measures move as 1 pair in the same direction while all the other pairs move in different direction between each other. These are 6 possibilities named RD_RR,RD_RRstar, RD_OR, RR_RRstar, RR_OR, RRstar_OR.\nOpposite pairwise events: Some measures move as 2 pairs but each of the 2 pairs does not move in the same direction. There are 3 possibilities, called RD_RR_vs_RRstar_OR, RD_RRstar_vs_RR_OR and RD_OR_vs_RR_RRstar. These are the problematic ones as they cannot be represented in the Venn diagram of section 4.2. However we can distribute them to ensure probabilities add up to 1. For example RD_RR_vs_RRstar_OR will be split 50% between pairwise events RD_RR and 50% to pairwise eventRRstar_OR. This enforces the very important rule that probabilities must add up to 1 without consequences on the conclusions reached.\n3-wise events: Some 3 measures move in the same direction together There are 4 possibilities called RD_RR_RRstar, RD_RR_OR, RD_RRstar_OR and RR_RRstar_OR.\nAll events: Sometimes all measures move together. This is the possibility of interest discussed by Shannin and Brumback (2021). This possibility is called ALL.\nNo event: The possibility NONE concerns the event that no pair of measures move in the same direction. It is impossible and represents the empty set \\(\\emptyset\\) which is one of the 3 conditions of a \\(\\sigma-field\\).\nValidation: The sum of the vector’s elements must be one.\n\nThe event definitions above ensure that the sample space is actually a \\(\\sigma-field\\). See Geoffrey R. Grimmet (2001), section 1.2.\nthen we compare with the author’s results\n\nbb &lt;- c(\"RD_RR\" = 0.026, \"RD_RRstar\" = 0.026, \"RD_OR\" = 0, \n        \"RR_RRstar\" = 0, \"RR_OR\" = 0.026, \"RRstar_OR\" = 0.026,\n        \"RD_RR_vs_RRstar_OR\" = 0,\n        \"RD_RRstar_vs_RR_OR\" = 0,\n        \"RD_OR_vs_RR_RRstar\" = 0,\n        \"RD_RR_RRstar\" = 0, \"RD_RR_OR\" = 0.057, \n        \"RD_RRstar_OR\" = 0.057, \"RR_RRstar_OR\" = 0,\n        \"RD_RR_RRstar_OR\" = 0.833, \"NONE\" = 0)\ncomp &lt;- data.frame(bb = round(bb, 4), sim = round(unlist(gridsim), 4))\ncomp\n\n                      bb    sim\nRD_RR              0.026 0.0000\nRD_RRstar          0.026 0.0000\nRD_OR              0.000 0.0000\nRR_RRstar          0.000 0.0000\nRR_OR              0.026 0.0000\nRRstar_OR          0.026 0.0000\nRD_RR_vs_RRstar_OR 0.000 0.0258\nRD_RRstar_vs_RR_OR 0.000 0.0246\nRD_OR_vs_RR_RRstar 0.000 0.0000\nRD_RR_RRstar       0.000 0.0000\nRD_RR_OR           0.057 0.0534\nRD_RRstar_OR       0.057 0.0518\nRR_RRstar_OR       0.000 0.0000\nRD_RR_RRstar_OR    0.833 0.8444\nNONE               0.000 0.0000\n\n\nThe results from the Monte Carlo simulation above confirm the main conclusion from Jake Shannin (2021) that all effect measures move together 84% of the time.\n\nc(\"simulation\" = unname(unlist(gridsim[\"RD_RR_RRstar_OR\"])), \n  \"author's\" = unname(bb[\"RD_RR_RRstar_OR\"]))\n\nsimulation   author's \n    0.8444     0.8330 \n\n\nWe note that the sim adds up correctly to 1 but not the author’s which adds up to 1.051. This is explained on p. 72 as a bit of arbitrary allocations. The Jake Shannin (2021) paper (caption figure 1) mentions that they do not add to 1 because they include events that are not mutually exclusive.\n\nc(\"simulation\" = sum(unlist(gridsim)), \"author's\" = sum(bb))\n\nsimulation   author's \n     1.000      1.051 \n\n\nActually, this is caused by the Opposite pairwise events which cannot be represented in the 4-set Venn diagram. For the purpose of this books we will simply split them between the events that makes them up without consequence on the conclusion.\nThe Opposite pairwise events are the following\n\n# The events that pair of measures move together but in opposite\n# direction of another pair who also move together\nunlist(gridsim[c(\"RD_RR_vs_RRstar_OR\", \"RD_RRstar_vs_RR_OR\", \"RD_OR_vs_RR_RRstar\")])\n\nRD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR RD_OR_vs_RR_RRstar \n            0.0258             0.0246             0.0000 \n\n\nand when we split them between their 2 sub-events we can generate the Venn diagram as follows\n\nggp_venn_sim(gridsim, n = 1000, \n         fill_colr = c(\"blue\", \"yellow\", \"green\", \"red\"),\n         title = \"Venn diagram of effect measure modifications\")\n\n\n\n\nnow for constrained data\n\ngridsim_const &lt;- fciR::mc_beta_effect_measures(shape1 = 1, shape2 = 1, nrep = 5000,\n                                constrained = TRUE)\n\nGrid of  1  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n\nwhich gives the vector of percentages\n\nunlist(gridsim_const)\n\n             RD_RR          RD_RRstar              RD_OR          RR_RRstar \n        0.00000000         0.00000000         0.00000000         0.00000000 \n             RR_OR          RRstar_OR RD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR \n        0.00000000         0.00000000         0.05117790         0.05199025 \nRD_OR_vs_RR_RRstar       RD_RR_RRstar           RD_RR_OR       RD_RRstar_OR \n        0.00000000         0.00000000         0.11291633         0.11129163 \n      RR_RRstar_OR    RD_RR_RRstar_OR               NONE \n        0.00000000         0.67262388         0.00000000 \n\n\nand comparing to the author’s results\n\nbb_const &lt;- c(\"RD_RR\" = 0.053, \"RD_RRstar\" = 0.053, \"RD_OR\" = 0.053, \n        \"RR_RRstar\" = 0, \"RR_OR\" = 0.053, \"RRstar_OR\" = 0.0,\n        \"RD_RR_vs_RRstar_OR\" = 0,\n        \"RD_RRstar_vs_RR_OR\" = 0,\n        \"RD_OR_vs_RR_RRstar\" = 0,\n        \"RD_RR_RRstar\" = 0, \"RD_RR_OR\" = 0.114, \n        \"RD_RRstar_OR\" = 0.114, \"RR_RRstar_OR\" = 0,\n        \"RD_RR_RRstar_OR\" = 0.667, \"NONE\" = 0)\nsum(bb_const)\n\n[1] 1.107\n\ncomp &lt;- data.frame(bb = round(bb_const, 4), \n                   sim = round(unlist(gridsim_const), 4))\ncomp\n\n                      bb    sim\nRD_RR              0.053 0.0000\nRD_RRstar          0.053 0.0000\nRD_OR              0.053 0.0000\nRR_RRstar          0.000 0.0000\nRR_OR              0.053 0.0000\nRRstar_OR          0.000 0.0000\nRD_RR_vs_RRstar_OR 0.000 0.0512\nRD_RRstar_vs_RR_OR 0.000 0.0520\nRD_OR_vs_RR_RRstar 0.000 0.0000\nRD_RR_RRstar       0.000 0.0000\nRD_RR_OR           0.114 0.1129\nRD_RRstar_OR       0.114 0.1113\nRR_RRstar_OR       0.000 0.0000\nRD_RR_RRstar_OR    0.667 0.6726\nNONE               0.000 0.0000\n\n\nThe results from the Monte Carlo simulation with the constrained data agree the author’s result.\n\nc(\"simulation\" = unname(unlist(gridsim_const[\"RD_RR_RRstar_OR\"])),\n  \"author's\" = unname(bb_const[\"RD_RR_RRstar_OR\"]))\n\nsimulation   author's \n 0.6726239  0.6670000 \n\n\nAgain the Opposite pairwise events cannot be represented in the 4-set Venn diagram.\n\n# The events that pair of measures move together but in opposite\n# direction of another pair who also move together\nunlist(gridsim_const[c(\"RD_RR_vs_RRstar_OR\", \"RD_RRstar_vs_RR_OR\", \"RD_OR_vs_RR_RRstar\")])\n\nRD_RR_vs_RRstar_OR RD_RRstar_vs_RR_OR RD_OR_vs_RR_RRstar \n        0.05117790         0.05199025         0.00000000 \n\n\nbut if we do it as mentioned above then we can show a Venn diagram as follows\n\n# This is a custom function using the venn package\nggp_venn_sim(gridsim_const,\n         fill_colr = c(\"cyan\", \"gold\", \"springgreen\", \"hotpink\"),\n         title = \"Venn diagram for constrained data\")\n\n\n\n\n\n\n4.2.2 Applications\n\n4.2.2.1 Simulation of distribution of effect measures\nWe use a parametric Monte Carlo simulation using the beta distribution to evaluate the effect of the distribution assumption on the effect-measure modifications.\nWe run the Monte-Carlo simulation with 5000 repetitions, a grid of shape1 and shape2 parameters for the Beta distribution and no constraint. Namely mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7), shape2 = c(0.5, 1, 3, 5, 7), nrep = 5000)\n\npriorsim &lt;- fciR::mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7),\n                                  shape2 = c(0.5, 1, 3, 5, 7),\n                                  nrep = 5000)\n\nGrid of  25  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===                                                                   |   4%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======                                                                |   8%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========                                                              |  12%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========                                                           |  16%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============                                                        |  20%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=================                                                     |  24%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================                                                  |  28%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================                                                |  32%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=========================                                             |  36%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |============================                                          |  40%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===============================                                       |  44%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================                                    |  48%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================================                                  |  52%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=======================================                               |  56%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==========================================                            |  60%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=============================================                         |  64%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================                      |  68%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================================                    |  72%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=====================================================                 |  76%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========================================================              |  80%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========================================================           |  84%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============================================================        |  88%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================================      |  92%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===================================================================   |  96%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n\nand we look at the matrix for the event RD_RR_RRstar_OR which represents the event that all effect measures move in the same direction.\nThe matrix elements correspond to the percentage frequency of the event given the beta distribution with shape parameters shape1 = s1 with s1 indicated as column names, and shape2 = s1 parameter with s2 indicated in the row names.\nThe beta distribution with shape1 = 1 and shape2 = 1 is similar to the uniform distribution on \\([0, 1]\\). Therefore this element is the one simulated by Shannon and Brumback, see Jake Shannin (2021) and, in fact, with 5000 repetitions nrep = 5000the measure is almost always very close to what is mentioned in Jake Shannin (2021).\nAlso note that for large \\(shape1 = shape2 = \\text{large number}\\) the beta distribution is similar to the normal distribution with a mean of \\(shape1 / (shape1 + shape2)\\). The simulation shows that in that case the percentage of measures moving in the same directions is nearing 100%. See for example when \\(shape1 = shape2 = 7\\) in the matrix of results from the sim.\nhere is the matrix of percentage of times that all measures move in the same direction, i.e. the event RD_RR_RRstar_OR\n\nround(priorsim[[\"RD_RR_RRstar_OR\"]], 2)\n\n       s1=0.5 s1=1 s1=3 s1=5 s1=7\ns2=0.5   0.78 0.81 0.83 0.83 0.83\ns2=1     0.80 0.84 0.85 0.87 0.87\ns2=3     0.82 0.86 0.90 0.91 0.92\ns2=5     0.83 0.87 0.92 0.92 0.93\ns2=7     0.83 0.86 0.91 0.93 0.93\n\n\nand to show as a heatmap\n\nggp_betasim(priorsim, var = \"RD_RR_RRstar_OR\", \n             colr = list(\"low\" = \"deepskyblue1\", \"high\" = \"deepskyblue4\"))\n\n\n\n\nand comparing with the author\n\nround(c(\"simulation\" = priorsim[[\"RD_RR_RRstar_OR\"]][\"s2=1\", \"s1=1\"], \n  \"author's\" = unname(bb[\"RD_RR_RRstar_OR\"])), 3)\n\nsimulation   author's \n     0.837      0.833 \n\n\nWe can see that the range is wide and should be considered.\n\nrange(priorsim[[\"RD_RR_RRstar_OR\"]])\n\n[1] 0.7848 0.9302\n\n\nNow we do it with constrained data\n\npriorsim_const &lt;- fciR::mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7),\n                                        shape2 = c(0.5, 1, 3, 5, 7),\n                                        nrep = 5000, constrained = TRUE)\n\nGrid of  25  parameter constellations to be evaluated. \n \nProgress: \n \n\n  |                                                                            \n  |                                                                      |   0%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===                                                                   |   4%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======                                                                |   8%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========                                                              |  12%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========                                                           |  16%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============                                                        |  20%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=================                                                     |  24%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================                                                  |  28%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================                                                |  32%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=========================                                             |  36%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |============================                                          |  40%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===============================                                       |  44%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================                                    |  48%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |====================================                                  |  52%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=======================================                               |  56%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==========================================                            |  60%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=============================================                         |  64%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================                      |  68%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==================================================                    |  72%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |=====================================================                 |  76%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |========================================================              |  80%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===========================================================           |  84%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |==============================================================        |  88%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |================================================================      |  92%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |===================================================================   |  96%\n\n\nWarning in searchCommandline(parallel, cpus = cpus, type = type, socketHosts =\nsocketHosts, : Unknown option on commandline: --file\n\n\n\n  |                                                                            \n  |======================================================================| 100%\n \n\n\n\nggp_betasim(priorsim_const, var = \"RD_RR_RRstar_OR\",\n             colr = list(\"low\" = \"lightsalmon1\", \"high\" = \"lightsalmon4\"),\n             title = \"Monte Carlo simulation. Constrained data.\")\n\n\n\n\nand comparing with the author\n\nround(c(\"simulation\" = priorsim_const[[\"RD_RR_RRstar_OR\"]][\"s2=1\", \"s1=1\"], \n  \"author's\" = unname(bb_const[\"RD_RR_RRstar_OR\"])), 3)\n\nsimulation   author's \n     0.670      0.667 \n\n\nand the range is even larger and therefore more significant.\n\nrange(priorsim_const[[\"RD_RR_RRstar_OR\"]])\n\n[1] 0.5811623 0.8613429\n\n\n\n\n4.2.2.2 Application: Data pre-processing (data cleaning)\nUnless the \\(p_0, p_1\\) obtained are uniformly distributed (not the most common scenario for sure) it seems from the results above that the likelihood of not having all effect measures moving in the same direction is low. Thus, it could be a good hint to uncover hidden processes in a data pre-processing routine.\nWe note the important rule mentioned at the beginning of section 4.2\n\nwhen relative risk \\(RR\\) and other relative risk \\(RR^*\\) both change in the same direction […] then so must the difference the risk difference and odds ratio.\n\nAnother rule from section 4.2 can help in data cleaning\n\nWhen \\(RR_0\\) and \\(RR_1\\) are on opposite side of 1, that is, when in one stratum the treatment is helpful and in the other it is harmful, then all measures will automatically change together.\n\nThus a quick, easy way to clean up potential data problems and obtain relevant details on outliers and hidden processes might be\n\nExclude cases when \\(RR\\) and \\(RR^*\\) change in the same direction to reduce the data load.\nExclude cases when \\(RR_0\\) and \\(RR_1\\) are on opposite side of 1\nInvestigate the remining cases asthey are good candidates for hidden processes\n\n\n\n4.2.2.3 Application: Bayesian prior in Beta-binomial model\nIf we use information from the population, or from expert knowledge, that effect measures should move in the same direction then a beta distribution with \\(shape1 &gt; 1, shape2 &gt; 1\\) would make sense and provide a better prior for the Beta-binomial model.\nOn the contrary, if we wish that the model look into the unlikely events that effect measure do not move in the same directions, then a beta distribution with \\(shape1 \\leq 1, shape2 \\leq 1\\) could be supported by the results from above."
  },
  {
    "objectID": "ch04_measures.html#causal-interaction",
    "href": "ch04_measures.html#causal-interaction",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.3 Causal Interaction",
    "text": "4.3 Causal Interaction"
  },
  {
    "objectID": "ch04_measures.html#exercises",
    "href": "ch04_measures.html#exercises",
    "title": "4  Effect-Measure Modification and Causal Interaction",
    "section": "4.4 Exercises",
    "text": "4.4 Exercises\nThe exercises are located in a separate project.\n\n\n\n\nBrumback, Babette A. 2022. Fundamentals of Causal Inference with r. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and Random Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP: Oxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brumback, Babette A. 2022. Fundamentals of Causal Inference with\nr. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and\nRandom Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP:\nOxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning\nEffect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app01_errata.html#preface",
    "href": "app01_errata.html#preface",
    "title": "Appendix A — Errata",
    "section": "A.1 Preface",
    "text": "A.1 Preface\npage xi, last word of first paragraph is standaridzation, s/b standardization"
  },
  {
    "objectID": "app01_errata.html#chapter-1",
    "href": "app01_errata.html#chapter-1",
    "title": "Appendix A — Errata",
    "section": "A.2 Chapter 1",
    "text": "A.2 Chapter 1\n\nA.2.1 Section 1.2.3.2, p. 11\nThe sentence of the 6th line on top of the page is\n\nWe simulated the data according to the hyothetical\n\nShould be hypothetical."
  },
  {
    "objectID": "app01_errata.html#chapter-2",
    "href": "app01_errata.html#chapter-2",
    "title": "Appendix A — Errata",
    "section": "A.3 Chapter 2",
    "text": "A.3 Chapter 2\n\nA.3.1 section 2.4 Compute the standard error\nThe standard error formula used is with the standard deviation of the population and equivalent to\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) * sqrt(n-1) / n\n\nwhen the correct definition is with the standard deviation of the sample\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) / sqrt(n)\n\n\n\nA.3.2 Figure 2.1, p. 30\nThis is really a small detail. The caption of the bottom plot is \\(\\hat{E_{np}}(Y \\mid A= 1, H =1, T = 1)\\), s/b \\(\\hat{E}_{np}\\)"
  },
  {
    "objectID": "app01_errata.html#chapter-3",
    "href": "app01_errata.html#chapter-3",
    "title": "Appendix A — Errata",
    "section": "A.4 Chapter 3",
    "text": "A.4 Chapter 3\n\nA.4.1 Page 37\nThe p-value found using a chi-square test is 3.207e-11. Using a t-test of the mean difference gives a p-value of 2.269e-9. It is not explained how the p-value of 0.032 is arrived at in the book.\n\n\nA.4.2 Typography: section 3.2 p. 40, equation 3.1\nThe current latex expression of conditional independence used seems to be (Y(0), Y(1)) \\ \\text{II} \\ T with the output\n\\[\n(Y(0), Y(1)) \\ \\text{II} \\ T\n\\]\na better typography would be \\perp\\!\\!\\!\\perp for the symbol \\(\\perp\\!\\!\\!\\perp\\). When used for equation 3.1 as (Y(0), Y(1)) \\perp\\!\\!\\!\\perp T we obtain\n\\[\n(Y(0), Y(1)) \\perp\\!\\!\\!\\perp T\n\\]\nIn the case when we want to show dependence, that is no independence then the latex expression is \\not\\!\\perp\\!\\!\\!\\perp for the symbol \\(\\not\\!\\perp\\!\\!\\!\\perp\\). For example equation 3.1 would become\n\\[\n(Y(0), Y(1)) \\not\\!\\perp\\!\\!\\!\\perp T\n\\]"
  },
  {
    "objectID": "app01_errata.html#chapter-4",
    "href": "app01_errata.html#chapter-4",
    "title": "Appendix A — Errata",
    "section": "A.5 Chapter 4",
    "text": "A.5 Chapter 4\n\nA.5.1 Section 4.1 p. 67 (on top)\nThe line is\n\nwhich is statistically signficant\n\nshould be significant"
  },
  {
    "objectID": "app01_errata.html#chapter-5",
    "href": "app01_errata.html#chapter-5",
    "title": "Appendix A — Errata",
    "section": "A.6 Chapter 5",
    "text": "A.6 Chapter 5\n\n\nnothing found"
  },
  {
    "objectID": "app01_errata.html#chapter-6",
    "href": "app01_errata.html#chapter-6",
    "title": "Appendix A — Errata",
    "section": "A.7 Chapter 6",
    "text": "A.7 Chapter 6\n\nA.7.1 Section 6.1 p. 100, first paragraph\nThe second sentence says\n\nMistakingly equation \\(E_H E(Y \\mid T=t \\mid H)\\) with […]\n\nShould it be \\(E_H (E(Y \\mid T=t) \\mid H)\\)? See extra \\()\\) before the last \\(\\mid\\).\n\n\nA.7.2 Section 6.3 p. 126, the script of simdr\nThe last paragraph of p. 126 says\n\nWe simulated \\(T\\) […] such that approximaly 600 individuals had \\(T=1\\)\n\nThe simdr gives an incorrect result of 540 with the constant 0.13. That constant should be 0.15 to obtain 600. See the mathematical proof and proof by simulation in the appendix Doubly Robust Simulation at Analyse \\(T\\)."
  },
  {
    "objectID": "app01_errata.html#chapter-7",
    "href": "app01_errata.html#chapter-7",
    "title": "Appendix A — Errata",
    "section": "A.8 Chapter 7",
    "text": "A.8 Chapter 7\n\nA.8.1 Section 7.2, equation (7.11), p. 139\n\n\\[\n\\begin{align*}\nE(Y_1 \\mid A=1) - (E(Y_1\\mid A=0, Y_0=1) - E(Y_1\\mid A=0, Y_0=0)) - E(Y_0 \\mid A=0) - (E(Y_1\\mid A=0, Y_0=0) - E(Y_1\\mid A=0, Y_0=0))\n\\end{align*}\n\\]\n\n\n\nA.8.2 Exercise 2\nIn the last paragraph of the exercise\n\nIn addition, use exsim.r to simulate […]\n\nIt should be ex2sim.r"
  },
  {
    "objectID": "app01_errata.html#chapter-8",
    "href": "app01_errata.html#chapter-8",
    "title": "Appendix A — Errata",
    "section": "A.9 Chapter 8",
    "text": "A.9 Chapter 8\n\nA.9.1 Section 8.2, p. 150\nThe very first sentence of section 8.2 says\n\n[…] the front-door theorm of Pearl […]\n\nIt should be theorem"
  },
  {
    "objectID": "app01_errata.html#chapter-9",
    "href": "app01_errata.html#chapter-9",
    "title": "Appendix A — Errata",
    "section": "A.10 Chapter 9",
    "text": "A.10 Chapter 9\n\nA.10.1 Beginning of chapter, p. 158\nMissing parentheses in the equation\n\\[\nITT = E(Y(1, A(1)) - E(Y(0, A(0))\n\\] but is missing parentheses and should be\n\\[\nITT = E(Y(1, A(1))) - E(Y(0, A(0)))\n\\]\nBased on the notation of mentioned in the second paragraph of p. 158, that we let \\(Y(t,a)\\) be the potential outcome of \\(Y\\) assuming we set \\(T=t\\) and then \\(A=a\\). Then the equation could be written more simply as\n\\[\nITT = E(Y(1, 1)) - E(Y(0, 0))\n\\]\n\n\nA.10.2 Section 9.3, p. 165\nIn the code for the example the problem is caused by the fact that IV &lt;- ITT / denom does not work when denom is too small. What about setting the result to NA when denom &lt; tolerance so the bootstrap will skip it and increase the number of bootstraps?\n\n\nA.10.3 Section 9.3, p. 169, 170, table 9.1\nMy results seem to be more consistent than the textbook’s. Is this a mistake, how to test these results which can possibly be too good to be true."
  },
  {
    "objectID": "app01_errata.html#chapter-10",
    "href": "app01_errata.html#chapter-10",
    "title": "Appendix A — Errata",
    "section": "A.11 Chapter 10",
    "text": "A.11 Chapter 10\n\nA.11.1 Section 10.3, code for equartiles.r\nThe following coding line is superfluous\nquartiles &lt;- quantile(eb, c(0, .25, .5, .75, 1))"
  },
  {
    "objectID": "app01_errata.html#chapter-11",
    "href": "app01_errata.html#chapter-11",
    "title": "Appendix A — Errata",
    "section": "A.12 Chapter 11",
    "text": "A.12 Chapter 11\n\nA.12.1 Section 11.2, p. 190\nThe data set i17dat is not in the material provided."
  },
  {
    "objectID": "app01_errata.html#chapter-12",
    "href": "app01_errata.html#chapter-12",
    "title": "Appendix A — Errata",
    "section": "A.13 Chapter 12",
    "text": "A.13 Chapter 12\n\nA.13.1 section 12.1, p. 198\nAt the bottom of the page, th first sentence of the paragraph says\n“by substituting parametric or nonparmetric. s/b nonparametric.\n\n\nA.13.2 section 12.3, p. 206\nJust before the start of the exercise, beneath table 12.3\n“is helpful of terms of teasing apart …”, s/b tearing"
  },
  {
    "objectID": "app02_notes.html#chapter-2",
    "href": "app02_notes.html#chapter-2",
    "title": "Appendix B — Notes",
    "section": "B.1 Chapter 2",
    "text": "B.1 Chapter 2\n\nB.1.1 section 2.4 p. 31\nThe second sentence of the last paragraph on p. 33 says\n\nWe also need the car package in order for the summary() function to operate on boot objects the way we describe.\n\nThis sentence is not required if we use the boot::boot.ci() which simplifies lmodboot.r() and does not require the car package. See the code in this document for lmodboot.r in chapter 2."
  },
  {
    "objectID": "app02_notes.html#chapter-3",
    "href": "app02_notes.html#chapter-3",
    "title": "Appendix B — Notes",
    "section": "B.2 Chapter 3",
    "text": "B.2 Chapter 3\nThe chi-square test on p. 37 gives a different result. Not clear how the author arrive at her result."
  },
  {
    "objectID": "app02_notes.html#chapter-4",
    "href": "app02_notes.html#chapter-4",
    "title": "Appendix B — Notes",
    "section": "B.3 Chapter 4",
    "text": "B.3 Chapter 4\n\nB.3.1 Section 4.1\nSee the plots in section 4.2. They could be helpful to visualize the changes in effect measures from one level of modifier to the other.\n\n\nB.3.2 Section 4.2\n\nB.3.2.1 Monte Carlo Simulation\nA Monte Carlo is provided in section 4.2 and coded in a function called betasim_effect_measures(). It uses the \\(Beta\\) distribution. It is helpful in that it\n\nconfirms the same results as in Jake Shannin (2021)\nis less CPU intensive as it needs only 5000 iterations to confirm Jake Shannin (2021)\nis easier to code than java and uses R which is the declared language of Brumback (2022)\nallows some extra flexibility with the shape parameters of \\(Beta\\) to investigate the conclusion with diffferent curves. See the suggestion for applications below.\n\n\n\nB.3.2.2 page 72, Figure 4.1\n\nThe probabilites shown in the Venn diagram do not add up to 100% because, for example, the event that RR changes in the same direction as RD but not in the same direction as the other two measures […]. It would akward to arbitrarily one of those 2 chances as zero.\n\nJake Shannin (2021) mentions that it is the result of not mutually exclusive events. That is true. Yet, these events, properly grouped are actually mutually exclusive. In section 4.2 they are called Opposite pairwise events. Using these definitions then yes, they are mutually exclusive but cannot be properly shown in the Venn diagram. This can be easily solved by splitting the probabilities. See section 4.2 for details.\nThe end result a proper partitioning of the sample space \\(\\Omega\\) and is, in fact, a \\(\\sigma-field\\) (See Geoffrey R. Grimmet (2001), section 1.2). Yet it does not change the conclusions reached in Jake Shannin (2021).\n\n\nB.3.2.3 Applications\nSee my sub-section 4.2 called Applications where 2 possible applications are mentioned.\n\nData pre-processing (data cleaning)\nBayesian prior for Beta-binomial model\n\n\n\n\nB.3.3 Exercises\n\nB.3.3.1 Exercise 1\nUsing the causal power, the conclusion is different than the official answer. It is not obvious why the official solution does not make use of the causal power.\n\n\nB.3.3.2 Exercise 5\nThe official solution uses gee with the default family, that is gaussian.\nSince the outcome \\(attend\\) is binary isn’t it better to use the binomial family?\nWe quote p. 50 from chapter 3 in that respect\n\nBecause our outcome is binary, we choose to fit the logistic parametric model"
  },
  {
    "objectID": "app02_notes.html#chapter-6",
    "href": "app02_notes.html#chapter-6",
    "title": "Appendix B — Notes",
    "section": "B.4 Chapter 6",
    "text": "B.4 Chapter 6\n\nB.4.1 Section 6.1.1 ATT\nThe function bootstandatt is not necessary. The small change is taken care of in bootstand with the argument att.\n\n\nB.4.2 Section 6.3\nThe conclusions with the simulation are the same as Brumback’s. However her sd for the estimators \\(EY1exp\\) and \\(EY1dr\\) for both \\(ss=40\\) and \\(ss=100\\) don’t agree with my results. They are so large that they raise questions."
  },
  {
    "objectID": "app02_notes.html#chapter-9",
    "href": "app02_notes.html#chapter-9",
    "title": "Appendix B — Notes",
    "section": "B.5 Chapter 9",
    "text": "B.5 Chapter 9\n\nB.5.1 Section 9.3\nReplace IV with NA when its value is too close to zero"
  },
  {
    "objectID": "app02_notes.html#chapter-10",
    "href": "app02_notes.html#chapter-10",
    "title": "Appendix B — Notes",
    "section": "B.6 Chapter 10",
    "text": "B.6 Chapter 10"
  },
  {
    "objectID": "app02_notes.html#section-10.1",
    "href": "app02_notes.html#section-10.1",
    "title": "Appendix B — Notes",
    "section": "B.7 Section 10.1",
    "text": "B.7 Section 10.1\nAdd subsection to highlight the checking of overlap.\n\n\n\n\nBrumback, Babette A. 2022. Fundamentals of Causal Inference with r. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and Random Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP: Oxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app03_linreg.html#model",
    "href": "app03_linreg.html#model",
    "title": "Appendix C — Linear Regression with R",
    "section": "C.1 Model",
    "text": "C.1 Model\nWe will use the model from Jake Shannin (2021). in chapter 5, section 5.2, on page 89. Its DAG is illustrated in figure 5.7.\n\nsim2 &lt;- function(n = 1000, seed = 888) {\n  set.seed(seed)\n  # Generate the observed confounder\n  H &lt;- rbinom(n, size = 1, prob = 0.4)\n  # Let the treatment depend on the confounder\n  probA &lt;- H * 0.8 + (1 - H) * 0.3\n  A &lt;- rbinom(n, size = 1, probA)\n  # Let the outcome depend on the treatment and the confounder\n  probY &lt;- A * (H * 0.5 + (1 - H) * 0.7) + (1 - A) * (H * 0.3 + (1 - H) * 0.5)\n  Y &lt;- rbinom(n, size = 1, prob = probY)\n  data.frame(\"H\" = H, \"A\" = A, \"Y\" = Y)\n}\ndf &lt;- sim2()\ndf |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nH\n0\n1\n0.40\n0.49\n0\n0\n0.0\n1\n1\n▇▁▁▁▆\n\n\nA\n0\n1\n0.50\n0.50\n0\n0\n0.5\n1\n1\n▇▁▁▁▇\n\n\nY\n0\n1\n0.51\n0.50\n0\n0\n1.0\n1\n1\n▇▁▁▁▇"
  },
  {
    "objectID": "app03_linreg.html#fitted-vs-predict",
    "href": "app03_linreg.html#fitted-vs-predict",
    "title": "Appendix C — Linear Regression with R",
    "section": "C.2 fitted() vs predict()",
    "text": "C.2 fitted() vs predict()\n\nC.2.1 fitted()\nfitted() returns the expected values of the response after the link function is applied, i.e. \\(E(Y_I)\\). Also fitted() uses only the original data.\nFor example\n\nn &lt;- 5\nx &lt;- rnorm(n)\ny &lt;- rpois(n, lambda = exp(x))\ndf &lt;- data.frame(\"x\" = x, \"y\" = y)\nfit &lt;- glm(y ~ x, data = df, family = \"poisson\")\n\nand fitted() give the \\(E(Y_i)\\)\n\nfitted.out &lt;- fitted(fit)\nfitted.out\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\nand if you try to fit with new data, nothing will happen. It doesn’t use the new data and returns the same output, no warning is given about the fact that newdata is not used.\n\nnewdf &lt;- df\nnewdf$x &lt;- rnorm(n, mean = 1, sd = 0.5)\nfitted(fit, newdata = newdf)\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\n\n\nC.2.2 predict()\nOn the other hand predict gives the result before the link function is applied and uses newdata\nFirst lets use predict() with the existing data\n\npredict.out &lt;- predict(fit)\npredict.out\n\n         1          2          3          4          5 \n-0.0600878  1.6274351 -0.4642585 -2.3923702 -1.3963289 \n\n\nwhich is before the link function is applied and so\n\nexp(predict.out) == fitted.out\n\n   1    2    3    4    5 \nTRUE TRUE TRUE TRUE TRUE \n\n\nand there is an option, type = \"response\" that tells predict to applied the link function so you don’t have to do it\n\npredict(fit, type = \"response\")\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\n\n\nC.2.3 Conclusion\nIn the context of Jake Shannin (2021) we use fitted() when we use the original data to get \\(E(E(Y_i))= \\hat{Y}\\) and predict()..., type = \"response) when we ave a counterfactual in the data, that is \\(E(E(Y_i \\mid T = t))= \\hat{Y} \\mid T=t\\)\n\nmean(fitted.out)\n\n[1] 1.4\n\n\n\n\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app04_simdr.html#errata6a",
    "href": "app04_simdr.html#errata6a",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.1 Analyse \\(T = \\sum_{i=1}^J{T_i}\\) (Errata)",
    "text": "D.1 Analyse \\(T = \\sum_{i=1}^J{T_i}\\) (Errata)\nSection 6.3 claims that \\(T=600\\) and \\(P(T=1 \\mid H) \\in [0.041, 0.0468]\\) but the results obtained from running simdr as shown in the book are different. Not a lot but enough to puzzle the avid reader (like me for example).\nSo lets go through simdr to explain the differences.\n\nsimdr.out &lt;- simdr(seed = 1009)$stats\n\n\nOne might think that standardization with the exposure model would be preferable when the outcome indicates a rare condition. To see this, first suppose the condition is not rare. We might have 3000 individuals and 50%, or 1500, with the condition. Using the rule of thumb for logistic regression of Peduzzi et al. presented in chapter 2, we should be able to include 150 covariates in the outcome model.\n\nThe Peduzzi rule can be found at the end of section 2.3, on top of p. 29. where it states that\n\nboth the numbers of individuals with \\(Y=0\\) and \\(Y=1\\) need to be larger than ten times the number of parameters.\n\nand therefore\n\nNow suppose the exposure, \\(T\\), is divided more evenly: that is, we have 600 with \\(T=1\\). This would suggest we can include 60 covariates in the exposure model.\n\nBrumback uses a linear function of \\(H\\) to create a distribution of \\(T_i\\) that makes it dependent on \\(H\\) and which should give \\(\\sum T_i \\approx 600\\). However, the simulation with simdr returns \\(\\sum T_i \\approx 540\\).\n\n# sum of T\nsimdr.out$`T`$sum\n\n[1] 541\n\n\n\nWe observe that \\(\\sum_i T_i \\approx 540 \\not \\approx 600\\).\n\nThis is the part that needs explaining. The difference seems too large to be explained by the usual culprit, random process.\nTo be able to do that lets use some notations and go through the mechanics of simdr.\nLet \\(H_{j}\\) be the covariate \\(j\\)\n\\[\n\\begin{align*}\nH_j \\text{ i.i.d. } \\mathcal{Bernoulli}(p), \\, j = 1, \\ldots, J\n\\end{align*}\n\\]\nwhere \\(J\\) is the same variable as \\(ss\\) from Brumback, i.e. \\(J = ss = 100\\). Also \\(H_{i, j}\\) is the value of covariate \\(H_j\\) for individual \\(i\\).\nand let \\(T_i\\) be the treatment of individual \\(i\\)\n\\[\n\\begin{align*}\nT_i \\sim \\mathcal{Bernoulli}(prob = P_i), \\, i = 1, \\ldots, I\n\\end{align*}\n\\]\nwhere \\(I\\) is set at \\(I=3000\\) by Brumback.\nIn addition, we let\n\\[\nS_i = \\sum_{j=1}^{J}H_{i, j} \\\\\n\\text{where } J = ss \\text{ as mentioned above}\n\\]\nand simdr defines \\(P_i\\) as a random variable\n\\[\n\\begin{align*}\n&P_i = \\alpha \\frac{\\beta}{J} S_i + p \\times X_i \\\\\n\\\\\n&\\text{where} \\\\\n&\\alpha = 0.13 \\\\\n&\\beta = 20 \\\\\n&p = 0.05 \\\\\n&X_i \\sim \\mathcal{Normal}(mean = 1, sd = 0.1)\n\\end{align*}\n\\]\nnow, since\n\\[\n\\begin{align*}\nH_i \\text{ i.i.d. } \\mathcal{Bernoulli}(p) &\\implies E(H_i)=p \\\\\nT_i \\sim \\mathcal{Bernoulli}(p_{i}) &\\implies E(T_i) = E(P_i) \\\\\nX_i \\sim \\mathcal{Normal}(mean = 1, sd = 0.1) &\\implies E(X_i) = 1\n\\end{align*}\n\\]\nand since \\(E()\\) is a linear function then\n\\[\n\\begin{align*}\nE(T_i) &= E(P_i) \\\\\n&= E(\\alpha \\cdot \\frac{\\beta}{J} S_i + p \\cdot X_i) \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\sum_{j=1}^{J}E(H_{i, j}) + p \\cdot E(X_i) \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\sum_{j=1}^{J}p + p \\cdot 1 \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\cdot J \\cdot p + p \\\\\n&= \\alpha \\cdot \\beta \\cdot p + p\n\\end{align*}\n\\]\nand the sum of the number of people treated is \\(T = \\sum_i^IT_i\\)\n\\[\n\\begin{align*}\nE(T)  &= \\sum_{i=1}^I{E(T_i)} \\\\\n&= \\sum_{i=1}^I{(\\alpha \\cdot \\beta \\cdot p + p)} \\\\\n&= I (\\alpha \\cdot \\beta \\cdot p + p) \\\\\n\\\\\n&\\text{and since} \\\\\n&I = 3000, \\, \\alpha = 0.13, \\, \\beta = 20, \\, p = 0.05 \\\\\n\\\\\n&\\text{then} \\\\\nE(T) &= 3000 \\cdot (0.13 \\cdot 20 \\cdot 0.05 + 0.05) \\\\\n&= 540\n\\end{align*}\n\\]\nwhich proves that the expected number of \\(T\\) is not really close to 600 but rather 540 which is also the result of the simulation. Therefore we could use 54 covariates rather than 60 by the Peduzzi rule.\nWe note that this does not change the conclusions drawn from the simulation significantly.\nFinally, we note that the formula \\(\\alpha \\cdot \\beta \\cdot p + p\\) could be modified to get \\(T=600\\) by varying the coefficient \\(\\alpha=0.13\\) and/or \\(\\beta=20\\). If we change only \\(\\alpha\\) and keep \\(\\beta=20\\) we could use the result from above and with simple algebra\n\\[\n\\begin{align*}\nE(T)&= I (\\alpha \\cdot \\beta \\cdot p + p) \\\\\n\\\\\n&\\text{and since we want } E(T) = 600 \\\\\n&\\text{and that} \\\\\n&\\beta = 20, \\, I = 3000, \\, p = 0.05 \\\\\n\\\\\n\\text{then} \\\\\n600 &= 3000 \\cdot (\\alpha \\cdot 20 \\cdot 0.05 + 0.05) \\\\\n\\alpha &= \\frac{600 - 0.05 \\cdot 3000}{3000 \\cdot 20 \\cdot0.05} = \\frac{450}{3000} = 0.15\n\\end{align*}\n\\]\nand we simulate using \\(\\alpha = 0.15\\) to validate\n\nsimdr600.out &lt;- simdr(alpha = 0.15, seed = 1009)$stats\nsimdr600.out$`T`$sum\n\n[1] 593\n\n\n\nWe suggest that simdr be modified to use 0.15 instead of 0.13."
  },
  {
    "objectID": "app04_simdr.html#analyse-y-sum_i1i-y_i",
    "href": "app04_simdr.html#analyse-y-sum_i1i-y_i",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.2 Analyse \\(Y = \\sum_{i=1}^I Y_i\\)",
    "text": "D.2 Analyse \\(Y = \\sum_{i=1}^I Y_i\\)\nFirst we analyse \\(Y = \\sum Y_i\\) mathematically using the same notation as above.\n\\[\n\\begin{align*}\n&\\text{as defined in simdr} \\\\\nY_i &= 0.1 T_i + 0.1 \\frac{\\beta}{J} S_i \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y_i) &=0.1 E(T_i) + 0.1 \\frac{\\beta}{J} E(S_i) \\\\\n&= 0.01 E(P_i) + 0.01 \\frac{\\beta}{J} \\sum_{j=1}^J{E(H_{ij})} \\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\sum_{j=1}^J{p} \\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\frac{\\beta}{J} \\cdot J \\cdot p\\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\cdot \\beta \\cdot p \\\\\n\\\\\n&\\text{and given} \\\\\n&\\alpha = 0.13, \\, \\beta = 20, \\, p = 0.05 \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y_i) &= 0.01 \\cdot(0.13 \\cdot 20 \\cdot 0.05 + 0.05) + 0.01 \\cdot 20 \\cdot 0.05 \\\\\n&=0.0013 + 0.0005 + 0.01 \\\\\n&= 0.0118\n\\end{align*}\n\\]\nand since \\(Y = \\sum_{i=1}^I Y_i\\) then\n\\[\n\\begin{align*}\nY &= \\sum_{i=1}^I Y_i \\\\\nE(Y) &= \\sum_{i=1}^I E(Y_i) = I \\cdot E(Y_i) \\\\\n\\\\\n&\\text{and from above} \\\\\nE(Y_i) &= 0.0118 \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y) &= I \\cdot E(Y_i) = 3000 \\cdot 0.0118 = 35.4 \\\\\n&\\approx 35\n\\end{align*}\n\\]\nwhich mathematically proves the following\n\nWe simulated \\(Y_i\\) as a function of \\(T_i\\) and \\(\\sum_{k=1}^{ss}H_{i,k}\\), such that approximately 35 individuals had \\(Y=1\\).\n\nand the result we get from both simdr is 38, close enough.\n\nsimdr.out$Y$sum\n\n[1] 38\n\n\n\nThe mean of \\(\\sum_{k=1}^{ss}{H_{ik}}\\) was fixed at one, but when \\(ss\\) was set to 100, it ranged from 0.00 to 2.80.\n\nThat is exactly what we get.\n\nc(\"min\" = simdr.out$sumH$min, \"max\" = simdr.out$sumH$max)\n\nmin max \n0.0 2.8 \n\n\nthen\n\n\\(P(T = 1 \\mid H)\\) ranged from 0.041 to 0.468.\n\n\nThe results are different.\n\n\nc(\"min\" = simdr.out$probT$min, \"max\" = simdr.out$probT$max)\n\n       min        max \n0.03873516 0.41876534 \n\n\nProbably because the original simulation came up with \\(T=600\\) rather than \\(T=540\\) as discussed above. Indeed if we verify with the simulation with \\(T=600\\) the results are now reasonably close.\n\nc(\"min\" = simdr600.out$probT$min, \"max\" = simdr600.out$probT$max)\n\n       min        max \n0.03873516 0.47476534 \n\n\n\n\\(E(Y \\mid T, H)\\) ranged from 0.000 to 0.036.\n\nand the results are almost identical\n\nc(\"min\" = simdr.out$probY$min, \"max\" = simdr.out$probY$max)\n\n  min   max \n0.000 0.038"
  },
  {
    "objectID": "app04_simdr.html#functions",
    "href": "app04_simdr.html#functions",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.3 Functions",
    "text": "D.3 Functions\nThe function simdr is found on p. 127-128. See the script in the last section of this appendix. We just added some arguments and and output to facilitate the analysis. See the next section Analysis.\nThe Monte Carlo simulation is done with a non-parametric Monte Carlo function mc_standdr with the following script. See the section Simulation blow.\nNote that simdr was rewritten as function mc_standdr to\n\nfacilitate the analysis of the algorithm\nperform a Monte Carlo simulation with the MonteCarlo package\nseparate the data simulation from the measurement of estimates in 2 sub functions\n\nstanddr_sim: Simulate the data\nstanddr_est: Calculate the estimates using the simulated data from standdr_sim\n\n\nmc_standdr and simdr give exactly the same results for the simulatted data\n\nsimdr.out &lt;- simdr(seed = 1009)\nnew_simdr.out &lt;- standdr_sim(seed = 1009)\nstopifnot(identical(simdr.out$stats, new_simdr.out$stats))\n\nas well as the measurements\n\nnew_simdr.est &lt;- standdr_est(Y = new_simdr.out$data$Y,\n                             `T` = new_simdr.out$data$`T`,\n                             H = new_simdr.out$data$H)\nstopifnot(identical(simdr.out$est, new_simdr.est))"
  },
  {
    "objectID": "app04_simdr.html#scripts",
    "href": "app04_simdr.html#scripts",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.4 Scripts",
    "text": "D.4 Scripts\n\nD.4.1 simdr\n\n#' Doubly Robust Standardization Simulation\n#' \n#' Doubly robust standardization simulation.\n#' \n#' This is the function used in \\emph{Fundamentals of Causal Inference} by\n#' B. Brumback in section 6.3 of chapter 6, p.127-128. \\code{standdr_stats} is\n#' used in the output to give more statistics. Also the arguments \n#' \\code{beta = 0.13} and \\code{gamma = 20} were necessary to analyse the\n#' algorithm, they don't change anything.\n#'\n#' @param ss Number of covariates i.i.d with \\code{rbinom(n, size=1, prob=probH)}\n#' @param alpha coefficient used to compute the distribution of \\code{`T`}.\n#' @param beta coefficient used to compute the distribution of \\code{`T`}.\n#' @param seed Seed used for random number generation, default is \\code{NULL}.\n#'\n#' @return List of statistics for thesimulated data and estimates using\n#' different merhods.\n#'\n#' @examples\n#' \\dontrun{\n#' simdr()\n#' }\n#' @export\nsimdr &lt;- function(ss = 100, alpha = 0.13, beta = 20, seed = NULL) {\n  \n  set.seed(seed)\n  \n  # ss is the number of confounders\n  # i.e. the number of columns of H\n  H &lt;- matrix(0, 3000, ss)\n  # Let all components of H be independent Bernoulli variables with p=0.05\n  probH &lt;- rep(0.05, 3000)\n  for (i in 1:ss) {\n    H[, i] &lt;- rbinom(n = 3000, size = 1, prob = probH)\n  }\n  # Let the treatment depend on a function of H\n  sumH &lt;- apply(H, 1, sum) * beta / ss\n  # make sure P(T=1) is between 0 and 1, i.e. positivity assumption\n  probT &lt;- alpha * sumH + 0.05 * rnorm(n = 3000, mean = 1, sd = 0.1)\n  # validate the positivity assumption\n  stopifnot(probT &gt; 0, probT &lt; 1)\n  \n  `T` &lt;- rbinom(n = 3000, size = 1, prob = probT)\n  \n  # Generate the outcome depend on T and H\n  probY &lt;- 0.01 * `T` + 0.01 * sumH\n  # positivity assumption is not required for the outcome\n  # see intro to chapter 6 p. 99\n  stopifnot(probY &gt;= 0, probY &lt;= 1)\n  \n  Y &lt;- rbinom(n = 3000, size = 1, prob = probY)\n  \n  # put the simulated resuts in a list\n  stats &lt;- list(\"sumH\" = simdr_stats(sumH),\n              \"probT\" = simdr_stats(probT),\n              \"T\" = simdr_stats(`T`),\n              \"probY\" = simdr_stats(probY),\n              \"Y\" = simdr_stats(Y))\n  \n  # fit the exposure model\n  e &lt;- fitted(lm(`T` ~ H))\n  \n  # refit the exposure model using an incorrect logistic model\n  e2 &lt;- predict(glm(`T` ~ H, family = \"binomial\"), type = \"response\")\n  \n  # compute the weights\n  w0 &lt;- (1 - `T`) / (1 - e)\n  w1 &lt;- `T` / e\n  w02 &lt;- (1 - `T`) / (1 - e2)\n  w12 &lt;- T / e2\n  \n  # fit an overspecified (saturated) outcome model\n  mod.out &lt;- lm(Y ~ `T` * H)\n  \n  # Estimate the expected potential outcomes using the various methods\n  dat &lt;- data.frame(\"Y\" = Y, \"T\" = `T`)\n  dat0 &lt;- dat\n  dat0$`T` &lt;- 0\n  dat1 &lt;- dat\n  dat1$`T` &lt;- 1\n  \n  # the predicted data\n  preds0 &lt;- predict(mod.out, newdata = dat0)\n  preds1 &lt;- predict(mod.out, newdata = dat1)\n  \n  # calculate the estimates\n  EYT0 &lt;- mean(Y * (1 - `T`))\n  EYT1 &lt;- mean(Y * `T`)\n  EY0exp &lt;- weighted.mean(Y, w = w0)\n  EY1exp &lt;- weighted.mean(Y, w = w1)\n  EY0exp2 &lt;- weighted.mean(Y, w = w02)\n  EY1exp2 &lt;- weighted.mean(Y, w = w12)\n  EY0out &lt;- mean(preds0)\n  EY1out &lt;- mean(preds1)\n  EY0dr &lt;- mean(w0 * Y + preds0 * (`T` - e) / (1 - e))\n  EY1dr &lt;- mean(w1 * Y - preds1 * (`T` - e) / e)\n  \n  est &lt;- list(\n    \"EYT0\" = EYT0,\n    \"EYT1\" = EYT1,\n    \"EY0exp\" = EY0exp,\n    \"EY1exp\" = EY1exp,\n    \"EY0exp2\" = EY0exp2,\n    \"EY1exp2\" = EY1exp2,\n    \"EY0out\" = EY0out,\n    \"EY1out\" = EY1out,\n    \"EY0dr\" = EY0dr,\n    \"EY1dr\" = EY1dr\n    )\n  \n  list(\"stats\" = stats, \"est\" = est)\n}\n\n#' Compute statistics from \\code{simdr}. Sames as \\code{standdr_est}\n#'\n#' @param x Vector of numeric values.\n#'\n#' @return list of statistics: \\code{sun(x), mean(x), min(x), max(x)}.\n#' \n#' @seealso standdr_stats\n#'\n#' @examples\n#' simdr_stats(runif(20))\n#' @export\nsimdr_stats &lt;- function(x) {\n  list(\"sum\" = sum(x), \"mean\" = mean(x), \"min\" = min(x), \"max\" = max(x))\n}\n\n\n\nD.4.2 mc_standdr\n\n#' Monte Carlo Simulation of Doubly Robust Standardization\n#'\n#' @param ss Integer(). Number of covariates.\n#' @param nrep Number of Monte Carlo repetitions.\n#' @param width Width of interval. e.g. 0.95 will give interval c(0.025, 0.975).\n#' Default is 0.95.\n#' \n#' @seealso standdr_sim standdr_est\n#'\n#' @return Dataframe of results.\n#' @export\nmc_standdr &lt;- function(ss = c(40, 100), nrep = 1000, width = 0.95) {\n  stopifnot(all(ss &gt;= 1), nrep &gt;= 1, width &gt; 0, width &lt; 1)\n  \n  # We use alpha = 0.15 to match results with the books\n  ms_standdr_func &lt;- function(n = 3000, ss = 100, alpha = 0.13, beta = 20, \n                              probH = 0.05, seed = NULL) {\n    \n    # first we simulate the data\n    dat &lt;- standdr_sim(n = n, ss = ss, alpha = alpha, beta = beta, \n                       probH = probH, seed = seed)$data\n    # then output the estimates in a list\n    standdr_est(Y = dat$Y, `T` = dat$`T`, H = dat$H)\n  }\n  \n  params &lt;- list(\"ss\" = ss)\n  mc.out &lt;- MonteCarlo::MonteCarlo(func = ms_standdr_func,\n                                   nrep = nrep, param_list = params)\n  \n  # output results in a dataframe\n  out &lt;- suppressWarnings(MonteCarlo::MakeFrame(mc.out))\n  out %&gt;%\n    pivot_longer(cols = -ss, names_to = \"estimator\", values_to = \"value\") %&gt;%\n    group_by(ss, estimator) %&gt;%\n    summarize(n = n(), \n              mean = mean(value),\n              sd = sd(value),\n              lower = quantile(value, probs = (1 - width) / 2),\n              upper = quantile(value, probs = 1 - (1 - width) / 2)) %&gt;%\n    ungroup()\n}\n\n#' Data Simulation for Doubly Robust Standardization\n#'\n#' @param n Number of individuals/observations.\n#' @param ss Number of covariates.\n#' @param alpha coefficient used to compute the distribution of \\code{`T`}.\n#' @param beta coefficient used to compute the distribution of \\code{`T`}.\n#' @param probH probability of H.\n#' @param seed Seed value. default is \\code{NULL}.\n#'\n#' @return List with a dataframe of Y, T and H and summary statitics.\n#' @export\n#'\n#' @examples\n#' \\dontrun{\n#' standdr_sim()\n#' }\nstanddr_sim &lt;- function(n = 3000, ss = 100, alpha = 0.13, beta = 20, \n                        probH = 0.05, seed = NULL) {\n  set.seed(seed)\n  \n  # matrix of independent Bernoulli vector with prob = 0.05\n  # \"The columns of H were independent indicator variables each\n  #  with probability 0.05\"\n  H &lt;- cbind(replicate(n = ss, rbinom(n = n, size = 1, prob = probH)))\n  \n  # let the treatment depend on a function of H\n  # \"We simulated T  as indicator variables with probabilities that varied as\n  # a linear function  of H such that approximately 600 individuals had T=1\"\n  sumH &lt;- apply(H, MARGIN = 1, FUN = sum) * beta / ss\n  probT &lt;- alpha * sumH + probH * rnorm(n = n, mean = 1, sd = 0.1)\n  # validate the positivity assumption\n  stopifnot(probT &gt; 0, probT &lt; 1)\n  \n  `T` &lt;- rbinom(n = n, size = 1, prob = probT)\n  \n  \n  # generate the outcome depend on T and H\n  # \"We simulated Y as a function T ans sumH such hat approximatey 35 \n  # individuals had Y = 1\"\n  probY &lt;- 0.01 * `T` + 0.01 * sumH\n  # positivity assumption is not required for the outcome\n  # see intro to chapter 6 p. 99\n  stopifnot(probY &gt;= 0, probY &lt;= 1)\n  \n  Y &lt;- rbinom(n = n, size = 1, prob = probY)\n  \n  # put the data in a data.frame\n  df &lt;- data.frame(\"Y\" = Y, \"T\" = `T`, \"H\" = H)\n  \n  # output results in a list\n  list(\n    \"stats\" = list(\"sumH\" = standdr_stats(sumH),\n                 \"probT\" = standdr_stats(probT),\n                 \"T\" = standdr_stats(`T`),\n                 \"probY\" = standdr_stats(probY),\n                 \"Y\" = standdr_stats(Y)),\n    \"data\" = list(\"Y\" = Y, \"T\" = `T`, \"H\" = H)\n    )\n}\n\n\n#' Estimates from Doubly Robust Standardization Simulation\n#'\n#' @param Y Vector of outcomes\n#' @param `T` Vector of treatments\n#' @param H Matrix of covariates\n#'\n#' @return List of estimates\n#' @export\nstanddr_est &lt;- function(Y, `T`, H) {\n  \n  # fit the exposure model\n  e &lt;- fitted(lm(`T` ~ H))\n  \n  # refit the exposure model using an incorrect logistic model\n  e2 &lt;- predict(glm(`T` ~ H, family = \"binomial\"), type = \"response\")\n  \n  # compute the weights\n  w0 &lt;- (1 - `T`) / (1 - e)\n  w1 &lt;- `T` / e\n  w02 &lt;- (1 - `T`) / (1 - e2)\n  w12 &lt;- T / e2\n  \n  # fit an overspecified (saturated) outcome model\n  mod.out &lt;- lm(Y ~ `T` * H)\n  \n  # Estimate the expected potential outcomes using the various methods\n  dat &lt;- data.frame(\"Y\" = Y, \"T\" = `T`)\n  dat0 &lt;- dat\n  dat0$`T` &lt;- 0\n  dat1 &lt;- dat\n  dat1$`T` &lt;- 1\n  \n  # the predicted data\n  preds0 &lt;- predict(mod.out, newdata = dat0)\n  preds1 &lt;- predict(mod.out, newdata = dat1)\n  \n  # calculate the estimates\n  EYT0 &lt;- mean(Y * (1 - `T`))\n  EYT1 &lt;- mean(Y * `T`)\n  EY0exp &lt;- weighted.mean(Y, w = w0)\n  EY1exp &lt;- weighted.mean(Y, w = w1)\n  EY0exp2 &lt;- weighted.mean(Y, w = w02)\n  EY1exp2 &lt;- weighted.mean(Y, w = w12)\n  EY0out &lt;- mean(preds0)\n  EY1out &lt;- mean(preds1)\n  EY0dr &lt;- mean(w0 * Y + preds0 * (`T` - e) / (1 - e))\n  EY1dr &lt;- mean(w1 * Y - preds1 * (`T` - e) / e)\n  \n  list(\"EYT0\" = EYT0,\n       \"EYT1\" = EYT1,\n       \"EY0exp\" = EY0exp,\n       \"EY1exp\" = EY1exp,\n       \"EY0exp2\" = EY0exp2,\n       \"EY1exp2\" = EY1exp2,\n       \"EY0out\" = EY0out,\n       \"EY1out\" = EY1out,\n       \"EY0dr\" = EY0dr,\n       \"EY1dr\" = EY1dr)\n}\n\n#' Compute Statistics from \\code{standdr_sim}.\n#'\n#' @param x Vector of numeric values.\n#'\n#' @return list of statistics: \\code{sun(x), mean(x), min(x), max(x)}.\n#'\n#' @examples\n#' standdr_est(runif(20))\n#' @export\nstanddr_stats &lt;- function(x) {\n  list(\"sum\" = sum(x), \"mean\" = mean(x), \"min\" = min(x), \"max\" = max(x))\n}"
  }
]