[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "",
    "text": "Preface\nThis is a study project of the book Fundamentals of Causal Inference With R by Babette A. Brumback. Ms Brumback uses base R for all the code. This study project is coding with the tidyverse way. The motivation stems from the opinion that, in practice, the code is as important as the theory and learning better coding practice should be started as early as possible."
  },
  {
    "objectID": "index.html#where-to-find",
    "href": "index.html#where-to-find",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Where-to-find",
    "text": "Where-to-find\n\nThe online version of this document can be found at FCI.\nThe companion package for this project is fciR and can be found at fciR.\nThe exercises can be found at FCI exercises. This repo could be set as a private to avoid frustrating the publisher."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany thanks to Babette A. Brumback for a book full of amazing observations, tricks and tips."
  },
  {
    "objectID": "index.html#packages",
    "href": "index.html#packages",
    "title": "Fundamentals of Causal Inference 2022 - Study Project",
    "section": "Packages",
    "text": "Packages\nFor data processing and analysis the following packages are used\n\n\n\nPackage\nComment\n\n\n\n\nfciR\nThis companion R package for this book\n\n\nconflicted\nManage conflict resolution amongst packages\n\n\ntidyverse\nTidyverse is the favored coding way\n\n\nskimr\nSummary statistics\n\n\nmodelr\nCreate elegant pipelines when modelling\n\n\nsimpr\nGenerate simulated data\n\n\nMonteCarlo\nMonte Carlo simulation\n\n\ngeepack\nGeneralized estimating equations solver\n\n\nrsample\nResampling and bootstraps\n\n\n\nFor plotting, graphs and tables these packages are used\n\n\n\nPackage\nComment\n\n\n\n\nggplot\nCreate graphics based on the grammar of graphics\n\n\nggdag\nCausal directed acyclic graphs\n\n\ntidygraph\nGraphs and networks manipulation\n\n\ngt\nNice-looking tables"
  },
  {
    "objectID": "part01.html",
    "href": "part01.html",
    "title": "Part I Basics",
    "section": "",
    "text": "This part covers the basics of probability theory, effect-measure and causal directed acyclic graphs"
  },
  {
    "objectID": "ch01_intro.html#a-brief-history",
    "href": "ch01_intro.html#a-brief-history",
    "title": "1  Introduction",
    "section": "1.1 A Brief History",
    "text": "1.1 A Brief History"
  },
  {
    "objectID": "ch01_intro.html#data-examples",
    "href": "ch01_intro.html#data-examples",
    "title": "1  Introduction",
    "section": "1.2 Data Examples",
    "text": "1.2 Data Examples\n\n1.2.1 Mortality Rates by Country\nThis dataset is available with fciR::mortality. The summary table is\n\ndata(\"mortality\", package = \"fciR\")\nmortality |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.1:  Mortality Rates by Age and Country \n  \n    \n    \n      T\n      H\n      deaths\n      population\n      Y\n    \n  \n  \n    TRUE\nFALSE\n756340\n282305227\n0.002679157\n    TRUE\nTRUE\n2152660\n48262955\n0.044602739\n    FALSE\nFALSE\n2923480\n1297258493\n0.002253583\n    FALSE\nTRUE\n7517520\n133015479\n0.056516129\n  \n  \n  \n\n\n\n\n\n\n\n1.2.2 National Center for Education Statistics\nThis dataset is available with fciR::nces.\nThe statistical summary is\n\ndata(\"nces\", package = \"fciR\")\nnces |&gt;\n  skimr::skim()\n\n\nTable 1.2: ?(caption)\n\n\n\n\n(a) Data summary\n\n\nName\nnces\n\n\nNumber of rows\n1217\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nselective\n0\n1\n0.20\n0.40\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nfemale\n0\n1\n0.44\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▆\n\n\nhighmathsat\n0\n1\n0.21\n0.41\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\n\n\n\n\n\nand the frequency table is\n\nnces |&gt;\n  dplyr::count(selective, female, highmathsat) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.3:  NCES Data \n  \n    \n    \n      selective\n      female\n      highmathsat\n      n\n    \n  \n  \n    0\n0\n0\n435\n    0\n0\n1\n87\n    0\n1\n0\n420\n    0\n1\n1\n37\n    1\n0\n0\n50\n    1\n0\n1\n104\n    1\n1\n0\n55\n    1\n1\n1\n29\n  \n  \n  \n\n\n\n\n\n\n\n1.2.3 Reducing Alcohol Consumption\n\n1.2.3.1 The What-If? Study\nThis dataset is available with fciR::whatifdat.\nThe statistical summary is\n\ndata(\"whatifdat\", package = \"fciR\")\nwhatifdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\nwhatifdat\n\n\nNumber of rows\n165\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nT\n0\n1\n0.48\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nA\n0\n1\n0.64\n0.48\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nH\n0\n1\n0.36\n0.48\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nY\n0\n1\n0.32\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\n\n\n\nThe frequency table is\n\nwhatifdat |&gt;\n  dplyr::count(`T`, A, H, Y) |&gt;\n  gt::gt()\n\n\n\n\n\nTable 1.4:  The What-If? Study \n  \n    \n    \n      T\n      A\n      H\n      Y\n      n\n    \n  \n  \n    0\n0\n0\n0\n15\n    0\n0\n0\n1\n3\n    0\n0\n1\n0\n3\n    0\n0\n1\n1\n11\n    0\n1\n0\n0\n36\n    0\n1\n0\n1\n4\n    0\n1\n1\n0\n4\n    0\n1\n1\n1\n9\n    1\n0\n0\n0\n15\n    1\n0\n0\n1\n3\n    1\n0\n1\n0\n3\n    1\n0\n1\n1\n7\n    1\n1\n0\n0\n27\n    1\n1\n0\n1\n3\n    1\n1\n1\n0\n9\n    1\n1\n1\n1\n13\n  \n  \n  \n\n\n\n\n\n\n1.2.3.1.1 The Double What-If? Study\nThis dataset is available with fciR::doublewhatifdat.\nThe statistical summary is\n\ndata(\"doublewhatifdat\", package = \"fciR\")\ndoublewhatifdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ndoublewhatifdat\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nAD0\n0\n1\n0.50\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nVL0\n0\n1\n0.62\n0.49\n0\n0\n1\n1\n1\n▅▁▁▁▇\n\n\nU\n0\n1\n0.49\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nT\n0\n1\n0.50\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nA\n0\n1\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nAD1\n0\n1\n0.28\n0.45\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\nVL1\n0\n1\n0.58\n0.49\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\nThe DAG for the Double What-If? study in the dagitty version is\n\nscm &lt;- list()\nscm &lt;- within(scm, {\n  the_nodes &lt;- c(\"U\" = \"Unmeasured, healthy behavior (U=1)\", \n                 \"AD0\" = \"Adherence time 0\", \n                 \"VL0\" = \"Viral Load time 0\", \n                 \"T\" = \"Naltrexone (T=1)\", \n                 \"A\" = \"Reduced drinking (A=1)\", \n                 \"AD1\" = \"Adherence time 1\", \n                 \"VL1\" = \"Viral Load time 1\")\n  coords &lt;- data.frame(\n    name = names(the_nodes),\n    x = c(2, 3, 4, 1, 2, 3, 4),\n    y = c(2, 2, 2, 1, 1, 1, 1)\n  )\n  dag &lt;- dagify(\n    AD0 ~ U,\n    VL0 ~ AD0,\n    A ~ `T` + U,\n    AD1 ~ A,\n    VL1 ~ AD0 + AD1 + U,\n  outcome = \"VL1\",\n  exposure = \"T\",\n  latent = \"U\",\n  coords = coords,\n  labels = the_nodes)\n  \n  # this is the only technique known to have a subscript in a DAG\n  # IMPORTANT: the expression must be exactly in alphabetical order\n  the_text_labels &lt;- c(\n    expression(bold(A)), expression(bold(AD[0])),expression(bold(AD[1])),\n    expression(bold(T)), expression(bold(U)), expression(bold(VL[0])), \n    expression(bold(VL[1])))\n  \n  # status' colors\n  colrs &lt;- c(\"latent\" = \"palevioletred\", \"exposure\" = \"mediumspringgreen\", \n             \"outcome\" = \"cornflowerblue\")\n  # plot the DAG\n  plot &lt;- dag |&gt; \n    tidy_dagitty() |&gt;\n    ggdag_status(color = status, text = FALSE) +\n    geom_dag_text(size = 5, color = \"white\", fontface = \"bold\",\n      parse = TRUE, label = the_text_labels) +\n    scale_color_manual(values = colrs, na.value = \"honeydew3\") +\n    scale_fill_manual(values = colrs, na.value = \"honeydew3\") +\n    ggdag::theme_dag_blank(panel.background = \n                             element_rect(fill=\"snow\", color=\"snow\")) +\n    theme(title = element_text(color = \"darkblue\"),\n          legend.position = \"bottom\",\n          legend.title = element_blank()) +\n    labs(title = \"The Double What-If? Study\")\n})\nscm$plot\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\nFigure 1.1: The Double What-If? Study\n\n\n\n\nand the code for doublewhatifsim.R is\n\n#' \\code{doublewhatifsim} script rewritten\n#' \n#' \\code{doublewhatifsim} script rewritten.\n#' \n#' Simulate the What-If study data.\n#'\n#' @param n Nb of observations.\n#' @param seed Integer, the seed used for random numbers.\n#'\n#' @return Dataframe\ndoublewhatifsim &lt;- function(n = 1000, seed = 444) {\n  \n  set.seed(seed)\n  \n  # variables each with probability 0.5\n  U &lt;- rbinom(n, size = 1, prob = 0.5)\n  # probability of AD0 depends on U\n  AD0prob &lt;- 0.2 + 0.6 * U\n  # generate independent bernoulli variables with varying probabilities\n  AD0 &lt;- rbinom(n, size = 1, prob = AD0prob)\n  VL0prob &lt;- 0.8 - 0.4 * AD0\n  VL0 &lt;- rbinom(n, size = 1, prob = VL0prob)\n  `T` &lt;- rbinom(n, size = 1, prob = 0.5)\n  Aprob &lt;- 0.05 + `T` * U * 0.8\n  A &lt;- rbinom(n, size = 1, prob = Aprob)\n  AD1prob &lt;- 0.1 + 0.8 * A\n  AD1 &lt;- rbinom(n, size = 1, prob = AD1prob)\n  VL1prob &lt;- VL0prob + 0.1 - 0.45 * AD1\n  VL1 &lt;- rbinom(n, size =1 , prob = VL1prob)\n  \n  data.frame(\n    \"AD0\" = AD0,\n    \"VL0\" = VL0,\n    \"T\" = `T`,\n    \"A\" = A,\n    \"AD1\" = AD1,\n    \"VL1\" = VL1\n  )\n}\n\n\n\n\n\n1.2.4 General Social Survey\nThis dataset is available with fciR::gss.\nThe statistical summary is\n\ndata(\"gss\", package = \"fciR\")\ngss |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ngss\n\n\nNumber of rows\n2348\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n0\n1.00\n49.12\n18.24\n18\n34\n48\n63\n99\n▇▇▇▅▁\n\n\ngt65\n0\n1.00\n0.22\n0.41\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nattend\n16\n0.99\n0.34\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\ngthsedu\n0\n1.00\n0.39\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nmagthsedu\n180\n0.92\n0.24\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\npagthsedu\n583\n0.75\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nfair\n798\n0.66\n0.49\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nowngun\n818\n0.65\n0.35\n0.48\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nconservative\n101\n0.96\n0.33\n0.47\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\ntrump\n0\n1.00\n0.25\n0.43\n0\n0\n0\n0\n1\n▇▁▁▁▂\n\n\nwhite\n0\n1.00\n0.72\n0.45\n0\n0\n1\n1\n1\n▃▁▁▁▇\n\n\nfemale\n0\n1.00\n0.55\n0.50\n0\n0\n1\n1\n1\n▆▁▁▁▇\n\n\n\n\n\n\n\n1.2.5 A Cancer Clinical Trial\nThis dataset is available with fciR::cogdat.\nThe statistical summary is\n\ndata(\"cogdat\", package = \"fciR\")\ncogdat |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ncogdat\n\n\nNumber of rows\n1190\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nA1\n0\n1\n0.47\n0.50\n0\n0\n0\n1\n1\n▇▁▁▁▇\n\n\nH2\n0\n1\n0.38\n0.49\n0\n0\n0\n1\n1\n▇▁▁▁▅\n\n\nA2\n0\n1\n0.13\n0.33\n0\n0\n0\n0\n1\n▇▁▁▁▁\n\n\nY\n0\n1\n0.26\n0.44\n0\n0\n0\n1\n1\n▇▁▁▁▃\n\n\n\n\n\nand the frequency table is\n\ndf &lt;- cogdat |&gt;\n  filter(Y == 1) |&gt;\n  count(A1, H2, A2, name = \"nY\")\ncogdat |&gt;\n  count(A1, H2, A2) |&gt;\n  left_join(df) |&gt;\n  mutate(nY = dplyr::if_else(is.na(nY), 0, nY),\n         prop = round(nY / n, 2)) |&gt;\n  gt::gt()\n\nJoining with `by = join_by(A1, H2, A2)`\n\n\n\n\n\n\nTable 1.5:  A Hypothetical Cancer Clinical Trial \n  \n    \n    \n      A1\n      H2\n      A2\n      n\n      nY\n      prop\n    \n  \n  \n    0\n0\n0\n410\n120\n0.29\n    0\n0\n1\n30\n0\n0.00\n    0\n1\n0\n160\n30\n0.19\n    0\n1\n1\n30\n20\n0.67\n    1\n0\n0\n280\n30\n0.11\n    1\n0\n1\n20\n10\n0.50\n    1\n1\n0\n190\n80\n0.42\n    1\n1\n1\n70\n20\n0.29\n  \n  \n  \n\n\n\n\nrm(df)"
  },
  {
    "objectID": "ch01_intro.html#exercises",
    "href": "ch01_intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.3 Exercises",
    "text": "1.3 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch02_probability.html#conditional-probability",
    "href": "ch02_probability.html#conditional-probability",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.1 Conditional Probability",
    "text": "2.1 Conditional Probability\n\n2.1.1 Law of total probability\nIt is important to note that \\(\\sum_i{H_i} = H\\), that is \\(H\\) can be partitioned in \\(i\\) non-overlapping partitions.\nThen the law of total probabilities is\n\\[\n\\begin{align*}\nP(A) &= \\sum_i{P(A \\cap H_i)}= \\sum_i{P(A \\mid H_i) P(H_i)} \\\\\n&\\text{and we condition the whole expression with B} \\\\\nP(A \\mid B) &= \\sum_i{P(A \\cap H_i \\mid B)}= \\sum_i{P(A \\mid B, H_i) P(B,H_i)} \\\\\n\\end{align*}\n\\]\nand the multiplication rule is\n\\[\n\\begin{align*}\nP(A, B \\mid C) &= \\frac{P(A, B, C)}{P(C)} \\\\\n&= \\frac{P(A \\mid B, C) P(B, C)}{P(C)} \\\\\n&= \\frac{P(A \\mid B, C) P(B \\mid C) P(C)}{P(C)} \\\\\n&= P(A \\mid B, C) P(B \\mid C)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "ch02_probability.html#conditional-expectation-and-the-law-of-total-expectation",
    "href": "ch02_probability.html#conditional-expectation-and-the-law-of-total-expectation",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.2 Conditional Expectation and the Law of Total expectation",
    "text": "2.2 Conditional Expectation and the Law of Total expectation\nThe conditional expectation is defined as\n\\[\nE(Y \\mid T) = \\sum_y y P(Y=y \\mid T)\n\\]\nand one way that helps me usually understand it well is that conditioning is the same as filtering the data.\nFor example the conditional expectation of mortality of US resident at the beginning of 2019 \\(E(Y \\mid T = 1) = 0.0088\\)\n\ndata(\"mortality_long\", package = \"fciR\")\nmortality_long |&gt;\n  # condition on T = 1\n  filter(`T` == 1) |&gt;\n  # compute probabilities\n  mutate(prob = n / sum(n)) |&gt;\n  # compute expectation for each possible value of Y\n  group_by(Y) |&gt;\n  summarize(EYT1 = sum(Y * prob)) |&gt;\n  # output results in a named vector\n  pull() |&gt;\n  setNames(nm = c(\"EY0T1\", \"EY1T1\"))\n\n EY0T1  EY1T1 \n0.0000 0.0088 \n\n\nwhere \\(E(Y=0 \\mid T=1) = 0\\) because when \\(Y=0 \\implies 0 \\cdot P(Y=0) = 0\\) and since \\(Y\\) is binary \\(E(Y=1 \\mid T=1) = P(Y=1 \\mid T=1)\\).\n\nAnalogous to the law of total probability is the law of ttal expectation, also called double expectation theorem.\n\nThis law is used extensively in this textbook.\n\\[\n\\begin{align*}\nE(Y \\mid T) &= E_{H \\mid T}(E(Y \\mid H, T)) \\\\\n&= \\sum_h \\left[ \\sum_y y P(Y=y \\mid H=h, T) \\right] P(H=h \\mid T)\n\\end{align*}\n\\]\nAlso the following equivalence is used very often in this book.\n\\[\n\\begin{align*}\nE(H \\mid T) = \\sum_h h P(H=h \\mid T) = E_{H \\mid T} (H)\n\\end{align*}\n\\]\n\n2.2.1 Mean independence and conditional mean independence\n\nThe random variable \\(Y\\) is mean independent of \\(T\\) if\n\n\\[\nE(Y \\mid T) = E(Y)\n\\]\n\nand is conditionally mean independent of \\(T\\) given \\(H\\) is\n\n\\[\nE(Y \\mid T, H) = E(Y \\mid H)\n\\]\nand the conditional uncorrelation and uncorrelation are\n\\[\nE(YT \\mid H) = E(Y \\mid H)E(T \\mid H) \\implies \\text{conditionally uncorrelated} \\\\\nE(YT) = E(Y)E(T) \\implies \\text{uncorrelated}\n\\]\n\nIt happens that conditional mean independence implies conditional uncorrelation, but not the other way around.\n\nWe prove it as follows\n\\[\n\\begin{align*}\n\\text{assume Y is conditionally independent of T given H then} \\\\\nE(Y \\mid T, H) &= E(Y \\mid H) \\\\ \\\\\n\\text{using double expectation theorem} \\\\\nE(TY \\mid H) &= E_{T \\mid H}(E(TY \\mid H, T)) \\\\\n\\text{expectation is a linear operator} \\\\\n&= E_{T \\mid H}(TE(Y \\mid H, T)) \\\\\n\\text{by conditional mean independence from above} \\\\\n&= E_{T \\mid H}(TE(Y \\mid H)) \\\\\n\\text{expectation is a linear operator} \\\\\n&= E_{T \\mid H}(T)E_{T \\mid H}(E(Y \\mid H)) \\\\\n\\text{which proves the conditional uncorrelation} \\\\\n&= E(T \\mid H)E(Y \\mid H)\n\\end{align*}\n\\]\n\n\n2.2.2 Regression model\n\nA statistical model for a conditional expectation is called a regression model.\n\nFor a binary dataset the regression model is said to be saturated or nonparametric because the 4 proportions, or coefficients, cover all possibilities.\n\\[\nE(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T + \\beta_3 H * T\n\\]\nWhen unsaturated or parametric the model makes an assumption. For example the following model assumes no interaction.\n\\[\nE(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T\n\\]\n\n\n2.2.3 Nonlinear parametric models\nThe three parametric models, also the most well-known, used in the book are\n\\[\n\\begin{align*}\n\\text{linear: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p \\\\\n\\text{loglinear: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\exp{(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)} \\\\\n\\text{logistic: } \\: E(Y \\mid X_1, \\ldots, X_p) &= \\text{expit}(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)\n\\end{align*}\n\\]\nThe function expit() used by the author is actually the same as gtools::inv.logit(), boot::inv.logit() or stats::plogis(). In this project we use stats::plogis() to minimize dependencies since it is in base R."
  },
  {
    "objectID": "ch02_probability.html#estimation",
    "href": "ch02_probability.html#estimation",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.3 Estimation",
    "text": "2.3 Estimation\nThis section is extremely important as it is used extensively, especially from the chapter 6 on.\nLets use the What-if example to illustrate the mathematics of it. In that case, we have 3 covariates, \\(T\\) for naltrexone, \\(A\\) for reduced drinking and \\(H\\) for unsuppressed viral load. In the data set we have 165 observations, therefore \\(i=165\\).\nLet \\(X_i\\) denote the collection of \\(X_{ij}\\) for \\(j=1, \\ldots, p\\) where \\(p\\) is the number of covariates which is 3 in the What-if dataset. \\(X_i\\) is a horizontal vector.\nSo for the What-if study\n\\[\nX_i = \\begin{bmatrix}A_i & T_i & H_1 \\end{bmatrix}\n\\]\nand \\(\\beta\\) is a vertical vector.\n\\[\n\\beta = \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix}\n\\]\ntherefore\n\\[\n\\begin{align*}\nX_i \\beta &= \\begin{bmatrix}A_i & T_i & H_1 \\end{bmatrix} \\times \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} \\\\\n&= \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i\n\\end{align*}\n\\]\nwe also have\n\\[\n\\begin{align*}\nX_i^T (Y_i - X_i \\beta) &= \\begin{bmatrix}A_i \\\\ T_i \\\\ H_1 \\end{bmatrix}(Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\n&= \\begin{bmatrix}\nA_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\nT_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\\nH_1 (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i)\n\\end{bmatrix}\n\\end{align*}\n\\]\nAlso, we can estimate the unconditional expectation of the binary outcome \\(Y\\) as\n\\[\n\\hat{E}(Y) = \\frac{1}{n} \\sum_{i=1}^n Y_i\n\\]\nwhich returns the proportion of participants with unsuppressed viral load after 4 months\n\n#label: ch02_whatifdat\ndata(\"whatifdat\", package = \"fciR\")\nmean(whatifdat$Y)\n\n[1] 0.3212121\n\n\nThis is an unbiased estimator for \\(E(Y)\\) because \\(E(\\hat{E}(Y)) = E(Y)\\)\nNow let the saturated model for \\(E(Y)\\) be\n\\[\nE(Y) = \\beta\n\\]\nwith the following estimating equation\n\\[\nU(\\beta) = \\sum_{i=1}^n (Y_i - \\beta) = 0\n\\]\nand simple algebra, using the estimate of \\(\\hat{E}(Y)\\) from above gives \\(\\beta = \\hat{E}(Y)\\) and we use the notation \\(\\hat{E}(Y) = E(Y \\mid X, \\beta)\\) to show its dependency on the data and \\(\\beta\\).\n\\[\n\\begin{align*}\nU(\\beta) &= \\sum_{i=1}^n (Y_i - \\beta) \\\\\n&= \\sum_{i=1}^n \\left[ Y_i - E(Y \\mid X, \\beta) \\right] \\\\\n&= 0\n\\end{align*}\n\\]\nand to do the actual calculations we use a variation of the estimating equation as follows\n\\[\n\\begin{align*}\nU(\\beta) = \\sum_{i=1}^n X_i^T \\left[ Y_i - E(Y \\mid X, \\beta) \\right] = 0\n\\end{align*}\n\\]\nAs an example, we fit the logistic regression model with the What-if dataset\n\\[\nE(Y \\mid A,T,H) = expit(\\beta_0 + \\beta_A A + \\beta_T T + \\beta_H H)\n\\]\n\nwhatif.mod &lt;- glm(Y ~ A + `T` + H, family = \"binomial\", data = whatifdat)\nsummary(whatif.mod)\n\n\nCall:\nglm(formula = Y ~ A + T + H, family = \"binomial\", data = whatifdat)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.5234     0.4125  -3.693 0.000222 ***\nA            -0.5647     0.4214  -1.340 0.180248    \nT            -0.2254     0.4147  -0.543 0.586790    \nH             2.7438     0.4158   6.600 4.12e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 207.17  on 164  degrees of freedom\nResidual deviance: 150.83  on 161  degrees of freedom\nAIC: 158.83\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe coefficients, \\(\\beta_0, \\beta_A, \\beta_T, \\beta_H\\) are obtained with coef() which is an alias for coefficient(). The example in the book uses lmod$coef which is not a recommended coding practice. The data should be obtained with an extractor function such as coef().\n\ncoef(whatif.mod)\n\n(Intercept)           A           T           H \n -1.5233530  -0.5647068  -0.2254112   2.7438245 \n\n\nand since the model is binomial then the inverse function to convert from the logit scale to the natural scale is stats::plogis().\nTherefore the parametric estimate is\n\\[\nE(Y \\mid A=1, T=1, H=1) = \\beta_0 + \\beta_A \\cdot 1 + \\beta_T \\cdot 1 + \\beta_H \\cdot 1\n\\]\n\nstats::plogis(sum(coef(whatif.mod)))\n\n[1] 0.6059581\n\n\nwe can also use the linear model directly, thus avoiding a call to plogis and making the code a little more robust. This technique is used quite often in the book, especially starting from chapter 6 on.\n\nwhatif.newdata &lt;- whatifdat |&gt;\n  filter(A == 1, `T` == 1, H == 1)\nwhatif.EYT1 &lt;- predict(whatif.mod, newdata = whatif.newdata, \n                            type = \"response\") |&gt;\n  mean()\nwhatif.EYT1\n\n[1] 0.6059581\n\n\nand we can validate the result directly using the dataset\n\nwhatifdat.est &lt;- whatifdat |&gt;\n  filter(A == 1, `T` == 1, H == 1) |&gt;\n  pull(Y) |&gt;\n  mean()\nwhatifdat.est\n\n[1] 0.5909091\n\n\nwhich is close to the parametric estimate."
  },
  {
    "objectID": "ch02_probability.html#sampling-distributions-and-the-bootstrap",
    "href": "ch02_probability.html#sampling-distributions-and-the-bootstrap",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.4 Sampling Distributions and the Bootstrap",
    "text": "2.4 Sampling Distributions and the Bootstrap\nThe previous section prived and estimate. But how variable is this estimator? One way would be to report the standard error \\(\\sigma_{\\hat{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Therefore, since \\(\\sigma^2 = p(1-p)\\) for a binomial distribution\n\n# the sample size\nwhatifdat.n &lt;- sum(whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H == 1)\n# whatifdat.n\n\n# the standard deviation\nwhatifdat.sd &lt;- sqrt(whatifdat.est * (1- whatifdat.est))\n# whatifdat.sd\n\n# the standard error\nwhatifdat.se &lt;- whatifdat.sd / sqrt(whatifdat.n)\nwhatifdat.se\n\n[1] 0.1048236\n\n\nNote that the formula used by B. Brumback is using the standard deviation of the population \\(p(1-p)\\) which, maybe, is not entirely the right way since the standard error of the mean is defined using the standard deviation of the sample.\nThat is Brumback uses\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) * sqrt(n-1) / n\n\n[1] 0.1048236\n\n\nwhen the correct definition is\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H == 1]\nsd(x) / sqrt(length(x))\n\n[1] 0.1072903\n\n\n\nAnother way would be by reporting the sampling distribution of our estimator.\n\n\nthe sampling distribution is centered at the true value of our estimand and it has a standard deviation equal to the true standard error of our estimator.\n\nThe section 2.4 will not be repeated here, yet it is very instructive and cover a topic that we end to forget. We will just repeat the calculations below. They\nThe sim() function in section 2.4, p. 31 is coded in fciR::sim_intervals() and its alias fciR::sim().\nRun and verify with author’s results on p. 32\n\nd &lt;- fciR::sim_intervals(nsim = 500, n = 500, seed = 1013)\nstopifnot(abs(d$bad - 0.8374) &lt; 0.01, abs(d$good - 0.948) &lt; 0.01)\nd\n\n$bad\n[1] 0.84\n\n$good\n[1] 0.946\n\n\n\n2.4.1 Bootstrapping\n\nAs we have seen, replacing \\(\\mu\\) and \\(\\sigma\\) with estimates leads to some problems, but we can still form an interpretable 95% confidence interval. In many situations, we do not have a good estimate of \\(\\sigma\\). In those cases, we often can turn to bootstrap.\n\nBootstrapping in base R is done with the boot from the boot package. Another option is the rsample package which isthe tidyverse way of doing it. Both methods will be used in this project. In the package fciR, the functions boot_run and boot_est use the classic R package boot. The functions bootidy_run and bootidy_est are similar but use the rsample package.\n\n2.4.1.1 boot package\nWe use boot::boot() for boostrapping and boot::boot.ci() to compute the confidence interval of the estimand using the logistic model as done by the lmodboot function in section 2.4. We only run 100 boot samples since it is only an example. The default value is 1000.\nThe first 3 arguments data, statistic and R are specific to boot, the ... is for extra arguments used by lmodboot. The details of lmdboots can be found in the package fciR or in its alias fciR::prob_lmod.\n\nprob_lmod(whatifdat, formula = Y ~ `T` + A + H)\n\n    term  estimate std.err\n1 logitP 0.4303535      NA\n\n\nand we show the details on how to do the bootstrapping with the boot package as follows.\n\n# define the function used by boot\nwhatifdat.fnc &lt;- function(data, ids, ...) {\n  dat &lt;- data[ids, ]\n  df &lt;- prob_lmod(data = dat, ...)\n  # create the named vector\n  out &lt;- c(df$estimate)\n  names(out) &lt;- df$term\n  out\n}\n# run the bootstrapping\nwhatifdat.boot &lt;- boot::boot(data = whatifdat, statistic = whatifdat.fnc, \n                             R = 100, formula = Y ~ `T` + A + H)\n# get the confidence interval for every term in the bootstrap object\nwhatifdat.out &lt;- lapply(X = seq_along(whatifdat.boot$t0), FUN = function(i, alpha = 0.05) {\n  est &lt;- whatifdat.boot$t0[i]\n  # the method used to find the interval\n  the_method &lt;- \"norm\"\n  # extract the interval from the boot object\n  ci &lt;- boot::boot.ci(whatifdat.boot, conf = 1 - alpha, type = the_method, \n                      index = i)$normal\n  # the dataframe of results, follow the format from rsample package\n  data.frame(\"term\" = names(est),\n    \".lower\" = plogis(ci[2]),\n    \".estimate\" = plogis(unname(est)),\n    \".upper\" = plogis(ci[3]),\n    \".alpha\" = alpha,\n    \".method\" = the_method)\n  })\n# bind the data.frame in one.\n# In this case it makes no difference as there is only one.\n# and we just want to show how it works in case of several items\nwhatifdat.out &lt;- do.call(rbind, whatifdat.out)\nwhatifdat.out\n\n    term    .lower .estimate    .upper .alpha .method\n1 logitP 0.4023362 0.6059581 0.7694102   0.05    norm\n\n\n\n\n2.4.1.2 rsample package\nAnother way package that could be used for boostrapping is rsample::bootstrap which uses the tidyverse way of R programming.\nTo be fully tidyverse-like we recode the prob_lmod function from above and call it prob_lmod_td where the td suffix stands for tidyverse. Note the use of a formula as an argument to the function and the rlang package which is a great tool to learn for quality code in R.\n\nprob_lmod_td &lt;- function(data, formula = Y ~ `T` + A + H,\n                         condition.names = NULL) {\n  # independent variables from the formula\n  f_vars &lt;- all.vars(rlang::f_rhs(formula))\n  # if condition.names is NULL then use all independent variables\n  # which is the same as saying there is no condition\n  if (is.null(condition.names)) condition.names &lt;- f_vars\n  stopifnot(all(condition.names %in% f_vars))\n\n  # add intercept to conditions\n  x0 &lt;- \"(Intercept)\"  # name of intercept used by lm, glm, etc.\n  condition.names &lt;- c(x0, condition.names)\n  \n  \n  fit &lt;- glm(formula = formula, family = \"binomial\", data = data) |&gt;\n    tidy()\n  fit |&gt;\n    filter(term %in% condition.names) |&gt;\n    summarize(term = \"logitP\",\n              estimate = sum(estimate),\n              # don't know the std.err so no t-intervals\n              std.err = NA_real_)\n}\n\n\nprob_lmod_td(whatifdat, formula = Y ~ `T` + A + H)\n\n# A tibble: 1 × 3\n  term   estimate std.err\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 logitP    0.430      NA\n\n\nThe following function is inspired from rsample bootstrap.\nWe obtain the confidence interval using rsample::int_pctl which uses a percentile interval instead of assuming a normal distribution as Ms Brumback does. The result are not materially different in the current case.\n\nwhatifdat |&gt;\n  rsample::bootstraps(times = 1000, apparent = FALSE) |&gt;\n  mutate(results = map(splits, function(x) {\n    dat &lt;- analysis(x)\n    prob_lmod_td(dat, formula = Y ~ `T` + A + H)})) |&gt;\n  rsample::int_pctl(statistics = results, alpha = 0.05) |&gt;\n  mutate(term = \"P\",\n         .lower = plogis(.lower),\n         .estimate = plogis(.estimate),\n         .upper = plogis(.upper))\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 P      0.440     0.610  0.771   0.05 percentile\n\n\nand lmodboot() is fciR::boot_lmod() with the alias fciR::lmodboot()\nFinally we run fciR::boot_lmod() and verify against the author’s results on p.34\n\nfciR::boot_est(whatifdat, func = fciR::prob_lmod, times = 500,\n                 alpha = 0.05, seed = 123, transf = \"expit\", terms = NULL,\n                 formula = Y ~ `T` + A + H)\n\n# A tibble: 1 × 6\n  term  .lower .estimate .upper .alpha .method   \n  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 P      0.427     0.608  0.750   0.05 percentile"
  },
  {
    "objectID": "ch02_probability.html#exercises",
    "href": "ch02_probability.html#exercises",
    "title": "2  Conditional Probability and Expectation",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "ch03_outcomes.html#potential-outcomes-and-the-consistency-assumption",
    "href": "ch03_outcomes.html#potential-outcomes-and-the-consistency-assumption",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.1 Potential Outcomes and the Consistency Assumption",
    "text": "3.1 Potential Outcomes and the Consistency Assumption\n\nThe utility of the potential outcome framework hinges on the validity of a consistency assumption that links potential outcome to obe=served outcomes.\n\n\nIt is worth emphasizing that the consistency assumption requires the potential outcomes to be well-defined.\n\n\nA major problem for causal inference is that we can only observe one potential outcome per participant. This is the Fundamental Problem of Causal Inference (FPCI).\n\nWe can therefore classify 4 causal types\n\\[\n\\text{Doomed:} \\: Y(0) = Y(1) = 0 \\\\\n\\text{Responsive:} \\: Y(0) = 0,  Y(1) = 1 \\\\\n\\text{Harmed:} \\: Y(0) = 1,  Y(1) = 0 \\\\\n\\text{Immune:} \\: Y(0) = Y(1) = 1 \\\\\n\\]\nIf we consider that the treatment cannot be harmful, in that case a patient with \\(Y=0\\) must be doomed and an untreated patient with \\(Y=0\\) must be immune.\n\\[\n\\text{Assuming the treatment cannot be harmful} \\\\ \\\\\n\\text{Doomed:} \\: Y(0) = Y(1) = 0 \\\\\n\\text{Responsive:} \\: Y(0) = 0,  Y(1) = 1 \\\\\n\\text{Immune:} \\: Y(0) = 1 \\\\\n\\]"
  },
  {
    "objectID": "ch03_outcomes.html#circumventing-the-fundamental-problem-of-causal-inference",
    "href": "ch03_outcomes.html#circumventing-the-fundamental-problem-of-causal-inference",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.2 Circumventing the Fundamental Problem of Causal Inference",
    "text": "3.2 Circumventing the Fundamental Problem of Causal Inference\nThere are 2 solutions to the FPCI\n\nScientific solution: We use scientific theory to measure both potential outcomes. For example by using 2 different participants that are considered identical for the purpose of the scientific study.\nStatistical solution: The treatment is randomized independently of any existing data. As a result we obtain mean independence and the mean can be used to draw inference. This approach has some issues\n\nThe difference between the outcome with or without treatment can still be caused by chance.\nThe study populaiton may not be relevant anymore.\nExtrapolating an average result to an individdual might be problematic."
  },
  {
    "objectID": "ch03_outcomes.html#effect-measures",
    "href": "ch03_outcomes.html#effect-measures",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.3 Effect Measures",
    "text": "3.3 Effect Measures\nTo estimate the association measures we use the parametric method with the glm function.\n\nWe certainly do not believe that our binary outcomes the distributional assumtptions for the gaussian and poisson models, but we are only using the glm function to solve estimating equations.\n\n\ndata(\"gss\", package = \"fciR\")\ngssrcc &lt;- gss[, c(\"trump\", \"gthsedu\", \"magthsedu\", \"white\", \"female\", \"gt65\")]\ngssrcc &lt;- gss[complete.cases(gssrcc), ]\nstopifnot(nrow(gssrcc) == 2348 - 180)  # see comment on p. 46\n\n\nuncond &lt;- fciR::boot_est(gssrcc, func = fciR::meas_effect_uncond, times = 500,\n                 alpha = 0.05, transf = \"exp\",\n                 terms = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"), \n                 formula = trump ~ gthsedu)\n\n\ngt_measures(uncond, \n            title = \"Table 3.2\", \n            subtitle = paste(\"4 Association Measures Relating\", \n            \"&lt;em&gt;More than High School Education&lt;/em&gt; to &lt;em&gt;Voting for Trump&lt;/em&gt;\", \n            sep = \"&lt;br&gt;\"))\n\n\n\n\n\n  \n    \n      Table 3.2\n    \n    \n      4 Association Measures RelatingMore than High School Education to Voting for Trump\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    P0\n0.234\n(0.213, 0.259)\n    P1\n0.272\n(0.239, 0.302)\n    RD\n0.038\n(0.001, 0.077)\n    RR\n1.162\n(1.005, 1.358)\n    RR*\n1.052\n(1.002, 1.109)\n    OR\n1.222\n(1.007, 1.504)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nverify with author’s results, p. 48-49\n\nbb &lt;- data.frame(\n  term = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n  .estimate = c(0.23338, 0.27117, 0.037782, 1.1619, 1.0518, 1.221),\n  .lower = c(0.21030, 0.24095, 0.00078085, 1.0047, 1.0006, 1.0056),\n  .upper = c(0.25647, 0.30139, 0.07478354, 1.3437, 1.1057, 1.4853))\nstopifnot(\n  all((uncond$.estimate - bb$.estimate) &lt; 0.01),\n  all((uncond$.lower - bb$.lower) &lt; 0.1),\n  all((uncond$.upper - bb$.upper) &lt; 0.1))\n\nNow we estimate the four effect measures using conditional association measures.\nTo use the function with conditioning on variables, i.e. filtering on variables, we note that\n\\[\n\\begin{align*}\nY &= \\text{trump} \\\\\nT &= \\text{gthsedu} \\\\\nH_1 &= \\text{magthsedu} \\\\\nH_2 &= \\text{white} \\\\\nH_3 &= \\text{female} \\\\\nH_4 &= \\text{gt65} \\\\\n\\end{align*}\n\\]\nTherefore\n\\[\n\\begin{align*}\nE(Y &\\mid T = 0, H_1 = 1, H_2 = 1, H_3 = 1, H_4 = 1) \\\\\n&\\therefore \\text{because T = 0} \\\\\nY &\\sim H_1 + H_2 + H_3 + H_4 \\\\\n\\text{trump} &\\sim \\text{magthsedu} + \\text{white} + \\text{female} + \\text{gt65} \\\\\n\\end{align*}\n\\]\n\na_formula &lt;- trump ~ gthsedu + magthsedu + white + female + gt65\ncondit &lt;- fciR::boot_est(gssrcc, func = fciR::meas_effect_cond,\n                 times = 500, alpha = 0.05, seed = 1234, transf = \"exp\",\n                 terms = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n                 formula = a_formula, exposure.name = \"gthsedu\",\n                 confound.names = c(\"magthsedu\", \"white\", \"female\", \"gt65\"))\n\n\ngt_measures(condit, \n            title = \"Table 3.3\", \n            subtitle = paste(\"4 Conditional Association or Effect Measures\",\n            \"Relating &lt;em&gt;More than High School Education&lt;/em&gt; \n            to &lt;em&gt;Voting for Trump&lt;/em&gt;\", sep = \"&lt;br&gt;\"))\n\n\n\n\n\n  \n    \n      Table 3.3\n    \n    \n      4 Conditional Association or Effect MeasuresRelating More than High School Education \n            to Voting for Trump\n    \n    \n      Measure\n      Estimate\n      CI1\n    \n  \n  \n    P0\n0.257\n(0.191, 0.332)\n    P1\n0.302\n(0.234, 0.372)\n    RD\n0.045\n(0.006, 0.092)\n    RR\n1.180\n(1.023, 1.425)\n    RR*\n1.065\n(1.008, 1.138)\n    OR\n1.257\n(1.031, 1.618)\n  \n  \n    \n      Fundamentals of Causal Inference, Babette A. Brumback, 2022\n    \n  \n  \n    \n      1 95% confidence interval\n    \n  \n\n\n\n\nverify with author’s results, p.52\n\nbb &lt;- data.frame(\n  term = c(\"P0\", \"P1\", \"RD\", \"RR\", \"RR*\", \"OR\"),\n  .estimate = c(0.25458, 0.30101, 0.046438, 1.1824, 1.0664, 1.261),\n  .lower = c(0.18518, 0.22995, 0.0022706, 1.0039, 1.0018, 1.0091),\n  .upper = c(0.32397, 0.37208, 0.0906057, 1.3927, 1.1352, 1.5758))\nstopifnot(\n  all(abs(condit$.estimate - bb$.estimate) &lt; 0.01),\n  all(abs(condit$.lower - bb$.lower) &lt; 0.1),\n  all(abs(condit$.upper - bb$.upper) &lt; 0.1))"
  },
  {
    "objectID": "ch03_outcomes.html#exercises",
    "href": "ch03_outcomes.html#exercises",
    "title": "3  Potential Outcomes and the Fundamental Problem of Causal Inference",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nThe exercises are located in a separate project."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brumback, Babette A. 2022. Fundamentals of Causal Inference with\nr. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and\nRandom Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP:\nOxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning\nEffect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app01_errata.html#preface",
    "href": "app01_errata.html#preface",
    "title": "Appendix A — Errata",
    "section": "A.1 Preface",
    "text": "A.1 Preface\npage xi, last word of first paragraph is standaridzation, s/b standardization"
  },
  {
    "objectID": "app01_errata.html#chapter-1",
    "href": "app01_errata.html#chapter-1",
    "title": "Appendix A — Errata",
    "section": "A.2 Chapter 1",
    "text": "A.2 Chapter 1\n\nA.2.1 Section 1.2.3.2, p. 11\nThe sentence of the 6th line on top of the page is\n\nWe simulated the data according to the hyothetical\n\nShould be hypothetical."
  },
  {
    "objectID": "app01_errata.html#chapter-2",
    "href": "app01_errata.html#chapter-2",
    "title": "Appendix A — Errata",
    "section": "A.3 Chapter 2",
    "text": "A.3 Chapter 2\n\nA.3.1 section 2.4 Compute the standard error\nThe standard error formula used is with the standard deviation of the population and equivalent to\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) * sqrt(n-1) / n\n\nwhen the correct definition is with the standard deviation of the sample\n\nx &lt;- whatifdat$Y[whatifdat$A == 1 & whatifdat$`T` == 1 & whatifdat$H ==1]\nn &lt;- length(x)\nsd(x) / sqrt(n)\n\n\n\nA.3.2 Figure 2.1, p. 30\nThis is really a small detail. The caption of the bottom plot is \\(\\hat{E_{np}}(Y \\mid A= 1, H =1, T = 1)\\), s/b \\(\\hat{E}_{np}\\)"
  },
  {
    "objectID": "app01_errata.html#chapter-3",
    "href": "app01_errata.html#chapter-3",
    "title": "Appendix A — Errata",
    "section": "A.4 Chapter 3",
    "text": "A.4 Chapter 3\n\nA.4.1 Page 37\nThe p-value found using a chi-square test is 3.207e-11. Using a t-test of the mean difference gives a p-value of 2.269e-9. It is not explained how the p-value of 0.032 is arrived at in the book.\n\n\nA.4.2 Typography: section 3.2 p. 40, equation 3.1\nThe current latex expression of conditional independence used seems to be (Y(0), Y(1)) \\ \\text{II} \\ T with the output\n\\[\n(Y(0), Y(1)) \\ \\text{II} \\ T\n\\]\na better typography would be \\perp\\!\\!\\!\\perp for the symbol \\(\\perp\\!\\!\\!\\perp\\). When used for equation 3.1 as (Y(0), Y(1)) \\perp\\!\\!\\!\\perp T we obtain\n\\[\n(Y(0), Y(1)) \\perp\\!\\!\\!\\perp T\n\\]\nIn the case when we want to show dependence, that is no independence then the latex expression is \\not\\!\\perp\\!\\!\\!\\perp for the symbol \\(\\not\\!\\perp\\!\\!\\!\\perp\\). For example equation 3.1 would become\n\\[\n(Y(0), Y(1)) \\not\\!\\perp\\!\\!\\!\\perp T\n\\]"
  },
  {
    "objectID": "app01_errata.html#chapter-4",
    "href": "app01_errata.html#chapter-4",
    "title": "Appendix A — Errata",
    "section": "A.5 Chapter 4",
    "text": "A.5 Chapter 4\n\nA.5.1 Section 4.1 p. 67 (on top)\nThe line is\n\nwhich is statistically signficant\n\nshould be significant"
  },
  {
    "objectID": "app01_errata.html#chapter-5",
    "href": "app01_errata.html#chapter-5",
    "title": "Appendix A — Errata",
    "section": "A.6 Chapter 5",
    "text": "A.6 Chapter 5\n\n\nnothing found"
  },
  {
    "objectID": "app01_errata.html#chapter-6",
    "href": "app01_errata.html#chapter-6",
    "title": "Appendix A — Errata",
    "section": "A.7 Chapter 6",
    "text": "A.7 Chapter 6\n\nA.7.1 Section 6.1 p. 100, first paragraph\nThe second sentence says\n\nMistakingly equation \\(E_H E(Y \\mid T=t \\mid H)\\) with […]\n\nShould it be \\(E_H (E(Y \\mid T=t) \\mid H)\\)? See extra \\()\\) before the last \\(\\mid\\).\n\n\nA.7.2 Section 6.3 p. 126, the script of simdr\nThe last paragraph of p. 126 says\n\nWe simulated \\(T\\) […] such that approximaly 600 individuals had \\(T=1\\)\n\nThe simdr gives an incorrect result of 540 with the constant 0.13. That constant should be 0.15 to obtain 600. See the mathematical proof and proof by simulation in the appendix Doubly Robust Simulation at Analyse \\(T\\)."
  },
  {
    "objectID": "app01_errata.html#chapter-7",
    "href": "app01_errata.html#chapter-7",
    "title": "Appendix A — Errata",
    "section": "A.8 Chapter 7",
    "text": "A.8 Chapter 7\n\nA.8.1 Section 7.2, equation (7.11), p. 139\n\n\\[\n\\begin{align*}\nE(Y_1 \\mid A=1) - (E(Y_1\\mid A=0, Y_0=1) - E(Y_1\\mid A=0, Y_0=0)) - E(Y_0 \\mid A=0) - (E(Y_1\\mid A=0, Y_0=0) - E(Y_1\\mid A=0, Y_0=0))\n\\end{align*}\n\\]\n\n\n\nA.8.2 Exercise 2\nIn the last paragraph of the exercise\n\nIn addition, use exsim.r to simulate […]\n\nIt should be ex2sim.r"
  },
  {
    "objectID": "app01_errata.html#chapter-8",
    "href": "app01_errata.html#chapter-8",
    "title": "Appendix A — Errata",
    "section": "A.9 Chapter 8",
    "text": "A.9 Chapter 8\n\nA.9.1 Section 8.2, p. 150\nThe very first sentence of section 8.2 says\n\n[…] the front-door theorm of Pearl […]\n\nIt should be theorem"
  },
  {
    "objectID": "app01_errata.html#chapter-9",
    "href": "app01_errata.html#chapter-9",
    "title": "Appendix A — Errata",
    "section": "A.10 Chapter 9",
    "text": "A.10 Chapter 9\n\nA.10.1 Beginning of chapter, p. 158\nMissing parentheses in the equation\n\\[\nITT = E(Y(1, A(1)) - E(Y(0, A(0))\n\\] but is missing parentheses and should be\n\\[\nITT = E(Y(1, A(1))) - E(Y(0, A(0)))\n\\]\nBased on the notation of mentioned in the second paragraph of p. 158, that we let \\(Y(t,a)\\) be the potential outcome of \\(Y\\) assuming we set \\(T=t\\) and then \\(A=a\\). Then the equation could be written more simply as\n\\[\nITT = E(Y(1, 1)) - E(Y(0, 0))\n\\]\n\n\nA.10.2 Section 9.3, p. 165\nIn the code for the example the problem is caused by the fact that IV &lt;- ITT / denom does not work when denom is too small. What about setting the result to NA when denom &lt; tolerance so the bootstrap will skip it and increase the number of bootstraps?\n\n\nA.10.3 Section 9.3, p. 169, 170, table 9.1\nMy results seem to be more consistent than the textbook’s. Is this a mistake, how to test these results which can possibly be too good to be true."
  },
  {
    "objectID": "app01_errata.html#chapter-10",
    "href": "app01_errata.html#chapter-10",
    "title": "Appendix A — Errata",
    "section": "A.11 Chapter 10",
    "text": "A.11 Chapter 10\n\nA.11.1 Section 10.3, code for equartiles.r\nThe following coding line is superfluous\nquartiles &lt;- quantile(eb, c(0, .25, .5, .75, 1))"
  },
  {
    "objectID": "app01_errata.html#chapter-11",
    "href": "app01_errata.html#chapter-11",
    "title": "Appendix A — Errata",
    "section": "A.12 Chapter 11",
    "text": "A.12 Chapter 11\n\nA.12.1 Section 11.2, p. 190\nThe data set i17dat is not in the material provided."
  },
  {
    "objectID": "app01_errata.html#chapter-12",
    "href": "app01_errata.html#chapter-12",
    "title": "Appendix A — Errata",
    "section": "A.13 Chapter 12",
    "text": "A.13 Chapter 12\n\nA.13.1 section 12.1, p. 198\nAt the bottom of the page, th first sentence of the paragraph says\n“by substituting parametric or nonparmetric. s/b nonparametric.\n\n\nA.13.2 section 12.3, p. 206\nJust before the start of the exercise, beneath table 12.3\n“is helpful of terms of teasing apart …”, s/b tearing"
  },
  {
    "objectID": "app02_notes.html#chapter-2",
    "href": "app02_notes.html#chapter-2",
    "title": "Appendix B — Notes",
    "section": "B.1 Chapter 2",
    "text": "B.1 Chapter 2\n\nB.1.1 section 2.4 p. 31\nThe second sentence of the last paragraph on p. 33 says\n\nWe also need the car package in order for the summary() function to operate on boot objects the way we describe.\n\nThis sentence is not required if we use the boot::boot.ci() which simplifies lmodboot.r() and does not require the car package. See the code in this document for lmodboot.r in chapter 2."
  },
  {
    "objectID": "app02_notes.html#chapter-3",
    "href": "app02_notes.html#chapter-3",
    "title": "Appendix B — Notes",
    "section": "B.2 Chapter 3",
    "text": "B.2 Chapter 3\nThe chi-square test on p. 37 gives a different result. Not clear how the author arrive at her result."
  },
  {
    "objectID": "app02_notes.html#chapter-4",
    "href": "app02_notes.html#chapter-4",
    "title": "Appendix B — Notes",
    "section": "B.3 Chapter 4",
    "text": "B.3 Chapter 4\n\nB.3.1 Section 4.1\nSee the plots in section 4.2. They could be helpful to visualize the changes in effect measures from one level of modifier to the other.\n\n\nB.3.2 Section 4.2\n\nB.3.2.1 Monte Carlo Simulation\nA Monte Carlo is provided in section 4.2 and coded in a function called betasim_effect_measures(). It uses the \\(Beta\\) distribution. It is helpful in that it\n\nconfirms the same results as in Jake Shannin (2021)\nis less CPU intensive as it needs only 5000 iterations to confirm Jake Shannin (2021)\nis easier to code than java and uses R which is the declared language of Brumback (2022)\nallows some extra flexibility with the shape parameters of \\(Beta\\) to investigate the conclusion with diffferent curves. See the suggestion for applications below.\n\n\n\nB.3.2.2 page 72, Figure 4.1\n\nThe probabilites shown in the Venn diagram do not add up to 100% because, for example, the event that RR changes in the same direction as RD but not in the same direction as the other two measures […]. It would akward to arbitrarily one of those 2 chances as zero.\n\nJake Shannin (2021) mentions that it is the result of not mutually exclusive events. That is true. Yet, these events, properly grouped are actually mutually exclusive. In section 4.2 they are called Opposite pairwise events. Using these definitions then yes, they are mutually exclusive but cannot be properly shown in the Venn diagram. This can be easily solved by splitting the probabilities. See section 4.2 for details.\nThe end result a proper partitioning of the sample space \\(\\Omega\\) and is, in fact, a \\(\\sigma-field\\) (See Geoffrey R. Grimmet (2001), section 1.2). Yet it does not change the conclusions reached in Jake Shannin (2021).\n\n\nB.3.2.3 Applications\nSee my sub-section 4.2 called Applications where 2 possible applications are mentioned.\n\nData pre-processing (data cleaning)\nBayesian prior for Beta-binomial model\n\n\n\n\nB.3.3 Exercises\n\nB.3.3.1 Exercise 1\nUsing the causal power, the conclusion is different than the official answer. It is not obvious why the official solution does not make use of the causal power.\n\n\nB.3.3.2 Exercise 5\nThe official solution uses gee with the default family, that is gaussian.\nSince the outcome \\(attend\\) is binary isn’t it better to use the binomial family?\nWe quote p. 50 from chapter 3 in that respect\n\nBecause our outcome is binary, we choose to fit the logistic parametric model"
  },
  {
    "objectID": "app02_notes.html#chapter-6",
    "href": "app02_notes.html#chapter-6",
    "title": "Appendix B — Notes",
    "section": "B.4 Chapter 6",
    "text": "B.4 Chapter 6\n\nB.4.1 Section 6.1.1 ATT\nThe function bootstandatt is not necessary. The small change is taken care of in bootstand with the argument att.\n\n\nB.4.2 Section 6.3\nThe conclusions with the simulation are the same as Brumback’s. However her sd for the estimators \\(EY1exp\\) and \\(EY1dr\\) for both \\(ss=40\\) and \\(ss=100\\) don’t agree with my results. They are so large that they raise questions."
  },
  {
    "objectID": "app02_notes.html#chapter-9",
    "href": "app02_notes.html#chapter-9",
    "title": "Appendix B — Notes",
    "section": "B.5 Chapter 9",
    "text": "B.5 Chapter 9\n\nB.5.1 Section 9.3\nReplace IV with NA when its value is too close to zero"
  },
  {
    "objectID": "app02_notes.html#chapter-10",
    "href": "app02_notes.html#chapter-10",
    "title": "Appendix B — Notes",
    "section": "B.6 Chapter 10",
    "text": "B.6 Chapter 10"
  },
  {
    "objectID": "app02_notes.html#section-10.1",
    "href": "app02_notes.html#section-10.1",
    "title": "Appendix B — Notes",
    "section": "B.7 Section 10.1",
    "text": "B.7 Section 10.1\nAdd subsection to highlight the checking of overlap.\n\n\n\n\nBrumback, Babette A. 2022. Fundamentals of Causal Inference with r. Boca Raton, Florida: Chapman; Hall/CRC. https://www.crcpress.com.\n\n\nGeoffrey R. Grimmet, David R. Stirzacker. 2001. Probability and Random Processes. 3rd ed. Great Clarendon Street, oxford OX2 6DP: Oxford University Press.\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app03_linreg.html#model",
    "href": "app03_linreg.html#model",
    "title": "Appendix C — Linear Regression with R",
    "section": "C.1 Model",
    "text": "C.1 Model\nWe will use the model from Jake Shannin (2021). in chapter 5, section 5.2, on page 89. Its DAG is illustrated in figure 5.7.\n\nsim2 &lt;- function(n = 1000, seed = 888) {\n  set.seed(seed)\n  # Generate the observed confounder\n  H &lt;- rbinom(n, size = 1, prob = 0.4)\n  # Let the treatment depend on the confounder\n  probA &lt;- H * 0.8 + (1 - H) * 0.3\n  A &lt;- rbinom(n, size = 1, probA)\n  # Let the outcome depend on the treatment and the confounder\n  probY &lt;- A * (H * 0.5 + (1 - H) * 0.7) + (1 - A) * (H * 0.3 + (1 - H) * 0.5)\n  Y &lt;- rbinom(n, size = 1, prob = probY)\n  data.frame(\"H\" = H, \"A\" = A, \"Y\" = Y)\n}\ndf &lt;- sim2()\ndf |&gt;\n  skimr::skim()\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nH\n0\n1\n0.40\n0.49\n0\n0\n0.0\n1\n1\n▇▁▁▁▆\n\n\nA\n0\n1\n0.50\n0.50\n0\n0\n0.5\n1\n1\n▇▁▁▁▇\n\n\nY\n0\n1\n0.51\n0.50\n0\n0\n1.0\n1\n1\n▇▁▁▁▇"
  },
  {
    "objectID": "app03_linreg.html#fitted-vs-predict",
    "href": "app03_linreg.html#fitted-vs-predict",
    "title": "Appendix C — Linear Regression with R",
    "section": "C.2 fitted() vs predict()",
    "text": "C.2 fitted() vs predict()\n\nC.2.1 fitted()\nfitted() returns the expected values of the response after the link function is applied, i.e. \\(E(Y_I)\\). Also fitted() uses only the original data.\nFor example\n\nn &lt;- 5\nx &lt;- rnorm(n)\ny &lt;- rpois(n, lambda = exp(x))\ndf &lt;- data.frame(\"x\" = x, \"y\" = y)\nfit &lt;- glm(y ~ x, data = df, family = \"poisson\")\n\nand fitted() give the \\(E(Y_i)\\)\n\nfitted.out &lt;- fitted(fit)\nfitted.out\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\nand if you try to fit with new data, nothing will happen. It doesn’t use the new data and returns the same output, no warning is given about the fact that newdata is not used.\n\nnewdf &lt;- df\nnewdf$x &lt;- rnorm(n, mean = 1, sd = 0.5)\nfitted(fit, newdata = newdf)\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\n\n\nC.2.2 predict()\nOn the other hand predict gives the result before the link function is applied and uses newdata\nFirst lets use predict() with the existing data\n\npredict.out &lt;- predict(fit)\npredict.out\n\n         1          2          3          4          5 \n-0.0600878  1.6274351 -0.4642585 -2.3923702 -1.3963289 \n\n\nwhich is before the link function is applied and so\n\nexp(predict.out) == fitted.out\n\n   1    2    3    4    5 \nTRUE TRUE TRUE TRUE TRUE \n\n\nand there is an option, type = \"response\" that tells predict to applied the link function so you don’t have to do it\n\npredict(fit, type = \"response\")\n\n         1          2          3          4          5 \n0.94168185 5.09080045 0.62860104 0.09141276 0.24750390 \n\n\n\n\nC.2.3 Conclusion\nIn the context of Jake Shannin (2021) we use fitted() when we use the original data to get \\(E(E(Y_i))= \\hat{Y}\\) and predict()..., type = \"response) when we ave a counterfactual in the data, that is \\(E(E(Y_i \\mid T = t))= \\hat{Y} \\mid T=t\\)\n\nmean(fitted.out)\n\n[1] 1.4\n\n\n\n\n\n\nJake Shannin, Babette A. Brumback. 2021. Disagreement Concerning Effect-Measures Modification. https://arxiv.org/abs/2105.07285v1."
  },
  {
    "objectID": "app04_simdr.html#errata6a",
    "href": "app04_simdr.html#errata6a",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.1 Analyse \\(T = \\sum_{i=1}^J{T_i}\\) (Errata)",
    "text": "D.1 Analyse \\(T = \\sum_{i=1}^J{T_i}\\) (Errata)\nSection 6.3 claims that \\(T=600\\) and \\(P(T=1 \\mid H) \\in [0.041, 0.0468]\\) but the results obtained from running simdr as shown in the book are different. Not a lot but enough to puzzle the avid reader (like me for example).\nSo lets go through simdr to explain the differences.\n\nsimdr.out &lt;- simdr(seed = 1009)$stats\n\n\nOne might think that standardization with the exposure model would be preferable when the outcome indicates a rare condition. To see this, first suppose the condition is not rare. We might have 3000 individuals and 50%, or 1500, with the condition. Using the rule of thumb for logistic regression of Peduzzi et al. presented in chapter 2, we should be able to include 150 covariates in the outcome model.\n\nThe Peduzzi rule can be found at the end of section 2.3, on top of p. 29. where it states that\n\nboth the numbers of individuals with \\(Y=0\\) and \\(Y=1\\) need to be larger than ten times the number of parameters.\n\nand therefore\n\nNow suppose the exposure, \\(T\\), is divided more evenly: that is, we have 600 with \\(T=1\\). This would suggest we can include 60 covariates in the exposure model.\n\nBrumback uses a linear function of \\(H\\) to create a distribution of \\(T_i\\) that makes it dependent on \\(H\\) and which should give \\(\\sum T_i \\approx 600\\). However, the simulation with simdr returns \\(\\sum T_i \\approx 540\\).\n\n# sum of T\nsimdr.out$`T`$sum\n\n[1] 541\n\n\n\nWe observe that \\(\\sum_i T_i \\approx 540 \\not \\approx 600\\).\n\nThis is the part that needs explaining. The difference seems too large to be explained by the usual culprit, random process.\nTo be able to do that lets use some notations and go through the mechanics of simdr.\nLet \\(H_{j}\\) be the covariate \\(j\\)\n\\[\n\\begin{align*}\nH_j \\text{ i.i.d. } \\mathcal{Bernoulli}(p), \\, j = 1, \\ldots, J\n\\end{align*}\n\\]\nwhere \\(J\\) is the same variable as \\(ss\\) from Brumback, i.e. \\(J = ss = 100\\). Also \\(H_{i, j}\\) is the value of covariate \\(H_j\\) for individual \\(i\\).\nand let \\(T_i\\) be the treatment of individual \\(i\\)\n\\[\n\\begin{align*}\nT_i \\sim \\mathcal{Bernoulli}(prob = P_i), \\, i = 1, \\ldots, I\n\\end{align*}\n\\]\nwhere \\(I\\) is set at \\(I=3000\\) by Brumback.\nIn addition, we let\n\\[\nS_i = \\sum_{j=1}^{J}H_{i, j} \\\\\n\\text{where } J = ss \\text{ as mentioned above}\n\\]\nand simdr defines \\(P_i\\) as a random variable\n\\[\n\\begin{align*}\n&P_i = \\alpha \\frac{\\beta}{J} S_i + p \\times X_i \\\\\n\\\\\n&\\text{where} \\\\\n&\\alpha = 0.13 \\\\\n&\\beta = 20 \\\\\n&p = 0.05 \\\\\n&X_i \\sim \\mathcal{Normal}(mean = 1, sd = 0.1)\n\\end{align*}\n\\]\nnow, since\n\\[\n\\begin{align*}\nH_i \\text{ i.i.d. } \\mathcal{Bernoulli}(p) &\\implies E(H_i)=p \\\\\nT_i \\sim \\mathcal{Bernoulli}(p_{i}) &\\implies E(T_i) = E(P_i) \\\\\nX_i \\sim \\mathcal{Normal}(mean = 1, sd = 0.1) &\\implies E(X_i) = 1\n\\end{align*}\n\\]\nand since \\(E()\\) is a linear function then\n\\[\n\\begin{align*}\nE(T_i) &= E(P_i) \\\\\n&= E(\\alpha \\cdot \\frac{\\beta}{J} S_i + p \\cdot X_i) \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\sum_{j=1}^{J}E(H_{i, j}) + p \\cdot E(X_i) \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\sum_{j=1}^{J}p + p \\cdot 1 \\\\\n&= \\alpha \\cdot \\frac{\\beta}{J} \\cdot J \\cdot p + p \\\\\n&= \\alpha \\cdot \\beta \\cdot p + p\n\\end{align*}\n\\]\nand the sum of the number of people treated is \\(T = \\sum_i^IT_i\\)\n\\[\n\\begin{align*}\nE(T)  &= \\sum_{i=1}^I{E(T_i)} \\\\\n&= \\sum_{i=1}^I{(\\alpha \\cdot \\beta \\cdot p + p)} \\\\\n&= I (\\alpha \\cdot \\beta \\cdot p + p) \\\\\n\\\\\n&\\text{and since} \\\\\n&I = 3000, \\, \\alpha = 0.13, \\, \\beta = 20, \\, p = 0.05 \\\\\n\\\\\n&\\text{then} \\\\\nE(T) &= 3000 \\cdot (0.13 \\cdot 20 \\cdot 0.05 + 0.05) \\\\\n&= 540\n\\end{align*}\n\\]\nwhich proves that the expected number of \\(T\\) is not really close to 600 but rather 540 which is also the result of the simulation. Therefore we could use 54 covariates rather than 60 by the Peduzzi rule.\nWe note that this does not change the conclusions drawn from the simulation significantly.\nFinally, we note that the formula \\(\\alpha \\cdot \\beta \\cdot p + p\\) could be modified to get \\(T=600\\) by varying the coefficient \\(\\alpha=0.13\\) and/or \\(\\beta=20\\). If we change only \\(\\alpha\\) and keep \\(\\beta=20\\) we could use the result from above and with simple algebra\n\\[\n\\begin{align*}\nE(T)&= I (\\alpha \\cdot \\beta \\cdot p + p) \\\\\n\\\\\n&\\text{and since we want } E(T) = 600 \\\\\n&\\text{and that} \\\\\n&\\beta = 20, \\, I = 3000, \\, p = 0.05 \\\\\n\\\\\n\\text{then} \\\\\n600 &= 3000 \\cdot (\\alpha \\cdot 20 \\cdot 0.05 + 0.05) \\\\\n\\alpha &= \\frac{600 - 0.05 \\cdot 3000}{3000 \\cdot 20 \\cdot0.05} = \\frac{450}{3000} = 0.15\n\\end{align*}\n\\]\nand we simulate using \\(\\alpha = 0.15\\) to validate\n\nsimdr600.out &lt;- simdr(alpha = 0.15, seed = 1009)$stats\nsimdr600.out$`T`$sum\n\n[1] 593\n\n\n\nWe suggest that simdr be modified to use 0.15 instead of 0.13."
  },
  {
    "objectID": "app04_simdr.html#analyse-y-sum_i1i-y_i",
    "href": "app04_simdr.html#analyse-y-sum_i1i-y_i",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.2 Analyse \\(Y = \\sum_{i=1}^I Y_i\\)",
    "text": "D.2 Analyse \\(Y = \\sum_{i=1}^I Y_i\\)\nFirst we analyse \\(Y = \\sum Y_i\\) mathematically using the same notation as above.\n\\[\n\\begin{align*}\n&\\text{as defined in simdr} \\\\\nY_i &= 0.1 T_i + 0.1 \\frac{\\beta}{J} S_i \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y_i) &=0.1 E(T_i) + 0.1 \\frac{\\beta}{J} E(S_i) \\\\\n&= 0.01 E(P_i) + 0.01 \\frac{\\beta}{J} \\sum_{j=1}^J{E(H_{ij})} \\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\sum_{j=1}^J{p} \\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\frac{\\beta}{J} \\cdot J \\cdot p\\\\\n&= 0.01 \\cdot(\\alpha \\cdot \\beta \\cdot p + p) + 0.01 \\cdot \\beta \\cdot p \\\\\n\\\\\n&\\text{and given} \\\\\n&\\alpha = 0.13, \\, \\beta = 20, \\, p = 0.05 \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y_i) &= 0.01 \\cdot(0.13 \\cdot 20 \\cdot 0.05 + 0.05) + 0.01 \\cdot 20 \\cdot 0.05 \\\\\n&=0.0013 + 0.0005 + 0.01 \\\\\n&= 0.0118\n\\end{align*}\n\\]\nand since \\(Y = \\sum_{i=1}^I Y_i\\) then\n\\[\n\\begin{align*}\nY &= \\sum_{i=1}^I Y_i \\\\\nE(Y) &= \\sum_{i=1}^I E(Y_i) = I \\cdot E(Y_i) \\\\\n\\\\\n&\\text{and from above} \\\\\nE(Y_i) &= 0.0118 \\\\\n\\\\\n&\\text{therefore} \\\\\nE(Y) &= I \\cdot E(Y_i) = 3000 \\cdot 0.0118 = 35.4 \\\\\n&\\approx 35\n\\end{align*}\n\\]\nwhich mathematically proves the following\n\nWe simulated \\(Y_i\\) as a function of \\(T_i\\) and \\(\\sum_{k=1}^{ss}H_{i,k}\\), such that approximately 35 individuals had \\(Y=1\\).\n\nand the result we get from both simdr is 38, close enough.\n\nsimdr.out$Y$sum\n\n[1] 38\n\n\n\nThe mean of \\(\\sum_{k=1}^{ss}{H_{ik}}\\) was fixed at one, but when \\(ss\\) was set to 100, it ranged from 0.00 to 2.80.\n\nThat is exactly what we get.\n\nc(\"min\" = simdr.out$sumH$min, \"max\" = simdr.out$sumH$max)\n\nmin max \n0.0 2.8 \n\n\nthen\n\n\\(P(T = 1 \\mid H)\\) ranged from 0.041 to 0.468.\n\n\nThe results are different.\n\n\nc(\"min\" = simdr.out$probT$min, \"max\" = simdr.out$probT$max)\n\n       min        max \n0.03873516 0.41876534 \n\n\nProbably because the original simulation came up with \\(T=600\\) rather than \\(T=540\\) as discussed above. Indeed if we verify with the simulation with \\(T=600\\) the results are now reasonably close.\n\nc(\"min\" = simdr600.out$probT$min, \"max\" = simdr600.out$probT$max)\n\n       min        max \n0.03873516 0.47476534 \n\n\n\n\\(E(Y \\mid T, H)\\) ranged from 0.000 to 0.036.\n\nand the results are almost identical\n\nc(\"min\" = simdr.out$probY$min, \"max\" = simdr.out$probY$max)\n\n  min   max \n0.000 0.038"
  },
  {
    "objectID": "app04_simdr.html#functions",
    "href": "app04_simdr.html#functions",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.3 Functions",
    "text": "D.3 Functions\nThe function simdr is found on p. 127-128. See the script in the last section of this appendix. We just added some arguments and and output to facilitate the analysis. See the next section Analysis.\nThe Monte Carlo simulation is done with a non-parametric Monte Carlo function mc_standdr with the following script. See the section Simulation blow.\nNote that simdr was rewritten as function mc_standdr to\n\nfacilitate the analysis of the algorithm\nperform a Monte Carlo simulation with the MonteCarlo package\nseparate the data simulation from the measurement of estimates in 2 sub functions\n\nstanddr_sim: Simulate the data\nstanddr_est: Calculate the estimates using the simulated data from standdr_sim\n\n\nmc_standdr and simdr give exactly the same results for the simulatted data\n\nsimdr.out &lt;- simdr(seed = 1009)\nnew_simdr.out &lt;- standdr_sim(seed = 1009)\nstopifnot(identical(simdr.out$stats, new_simdr.out$stats))\n\nas well as the measurements\n\nnew_simdr.est &lt;- standdr_est(Y = new_simdr.out$data$Y,\n                             `T` = new_simdr.out$data$`T`,\n                             H = new_simdr.out$data$H)\nstopifnot(identical(simdr.out$est, new_simdr.est))"
  },
  {
    "objectID": "app04_simdr.html#scripts",
    "href": "app04_simdr.html#scripts",
    "title": "Appendix D — Doubly Robust Simulation",
    "section": "D.4 Scripts",
    "text": "D.4 Scripts\n\nD.4.1 simdr\n\n#' Doubly Robust Standardization Simulation\n#' \n#' Doubly robust standardization simulation.\n#' \n#' This is the function used in \\emph{Fundamentals of Causal Inference} by\n#' B. Brumback in section 6.3 of chapter 6, p.127-128. \\code{standdr_stats} is\n#' used in the output to give more statistics. Also the arguments \n#' \\code{beta = 0.13} and \\code{gamma = 20} were necessary to analyse the\n#' algorithm, they don't change anything.\n#'\n#' @param ss Number of covariates i.i.d with \\code{rbinom(n, size=1, prob=probH)}\n#' @param alpha coefficient used to compute the distribution of \\code{`T`}.\n#' @param beta coefficient used to compute the distribution of \\code{`T`}.\n#' @param seed Seed used for random number generation, default is \\code{NULL}.\n#'\n#' @return List of statistics for thesimulated data and estimates using\n#' different merhods.\n#'\n#' @examples\n#' \\dontrun{\n#' simdr()\n#' }\n#' @export\nsimdr &lt;- function(ss = 100, alpha = 0.13, beta = 20, seed = NULL) {\n  \n  set.seed(seed)\n  \n  # ss is the number of confounders\n  # i.e. the number of columns of H\n  H &lt;- matrix(0, 3000, ss)\n  # Let all components of H be independent Bernoulli variables with p=0.05\n  probH &lt;- rep(0.05, 3000)\n  for (i in 1:ss) {\n    H[, i] &lt;- rbinom(n = 3000, size = 1, prob = probH)\n  }\n  # Let the treatment depend on a function of H\n  sumH &lt;- apply(H, 1, sum) * beta / ss\n  # make sure P(T=1) is between 0 and 1, i.e. positivity assumption\n  probT &lt;- alpha * sumH + 0.05 * rnorm(n = 3000, mean = 1, sd = 0.1)\n  # validate the positivity assumption\n  stopifnot(probT &gt; 0, probT &lt; 1)\n  \n  `T` &lt;- rbinom(n = 3000, size = 1, prob = probT)\n  \n  # Generate the outcome depend on T and H\n  probY &lt;- 0.01 * `T` + 0.01 * sumH\n  # positivity assumption is not required for the outcome\n  # see intro to chapter 6 p. 99\n  stopifnot(probY &gt;= 0, probY &lt;= 1)\n  \n  Y &lt;- rbinom(n = 3000, size = 1, prob = probY)\n  \n  # put the simulated resuts in a list\n  stats &lt;- list(\"sumH\" = simdr_stats(sumH),\n              \"probT\" = simdr_stats(probT),\n              \"T\" = simdr_stats(`T`),\n              \"probY\" = simdr_stats(probY),\n              \"Y\" = simdr_stats(Y))\n  \n  # fit the exposure model\n  e &lt;- fitted(lm(`T` ~ H))\n  \n  # refit the exposure model using an incorrect logistic model\n  e2 &lt;- predict(glm(`T` ~ H, family = \"binomial\"), type = \"response\")\n  \n  # compute the weights\n  w0 &lt;- (1 - `T`) / (1 - e)\n  w1 &lt;- `T` / e\n  w02 &lt;- (1 - `T`) / (1 - e2)\n  w12 &lt;- T / e2\n  \n  # fit an overspecified (saturated) outcome model\n  mod.out &lt;- lm(Y ~ `T` * H)\n  \n  # Estimate the expected potential outcomes using the various methods\n  dat &lt;- data.frame(\"Y\" = Y, \"T\" = `T`)\n  dat0 &lt;- dat\n  dat0$`T` &lt;- 0\n  dat1 &lt;- dat\n  dat1$`T` &lt;- 1\n  \n  # the predicted data\n  preds0 &lt;- predict(mod.out, newdata = dat0)\n  preds1 &lt;- predict(mod.out, newdata = dat1)\n  \n  # calculate the estimates\n  EYT0 &lt;- mean(Y * (1 - `T`))\n  EYT1 &lt;- mean(Y * `T`)\n  EY0exp &lt;- weighted.mean(Y, w = w0)\n  EY1exp &lt;- weighted.mean(Y, w = w1)\n  EY0exp2 &lt;- weighted.mean(Y, w = w02)\n  EY1exp2 &lt;- weighted.mean(Y, w = w12)\n  EY0out &lt;- mean(preds0)\n  EY1out &lt;- mean(preds1)\n  EY0dr &lt;- mean(w0 * Y + preds0 * (`T` - e) / (1 - e))\n  EY1dr &lt;- mean(w1 * Y - preds1 * (`T` - e) / e)\n  \n  est &lt;- list(\n    \"EYT0\" = EYT0,\n    \"EYT1\" = EYT1,\n    \"EY0exp\" = EY0exp,\n    \"EY1exp\" = EY1exp,\n    \"EY0exp2\" = EY0exp2,\n    \"EY1exp2\" = EY1exp2,\n    \"EY0out\" = EY0out,\n    \"EY1out\" = EY1out,\n    \"EY0dr\" = EY0dr,\n    \"EY1dr\" = EY1dr\n    )\n  \n  list(\"stats\" = stats, \"est\" = est)\n}\n\n#' Compute statistics from \\code{simdr}. Sames as \\code{standdr_est}\n#'\n#' @param x Vector of numeric values.\n#'\n#' @return list of statistics: \\code{sun(x), mean(x), min(x), max(x)}.\n#' \n#' @seealso standdr_stats\n#'\n#' @examples\n#' simdr_stats(runif(20))\n#' @export\nsimdr_stats &lt;- function(x) {\n  list(\"sum\" = sum(x), \"mean\" = mean(x), \"min\" = min(x), \"max\" = max(x))\n}\n\n\n\nD.4.2 mc_standdr\n\n#' Monte Carlo Simulation of Doubly Robust Standardization\n#'\n#' @param ss Integer(). Number of covariates.\n#' @param nrep Number of Monte Carlo repetitions.\n#' @param width Width of interval. e.g. 0.95 will give interval c(0.025, 0.975).\n#' Default is 0.95.\n#' \n#' @seealso standdr_sim standdr_est\n#'\n#' @return Dataframe of results.\n#' @export\nmc_standdr &lt;- function(ss = c(40, 100), nrep = 1000, width = 0.95) {\n  stopifnot(all(ss &gt;= 1), nrep &gt;= 1, width &gt; 0, width &lt; 1)\n  \n  # We use alpha = 0.15 to match results with the books\n  ms_standdr_func &lt;- function(n = 3000, ss = 100, alpha = 0.13, beta = 20, \n                              probH = 0.05, seed = NULL) {\n    \n    # first we simulate the data\n    dat &lt;- standdr_sim(n = n, ss = ss, alpha = alpha, beta = beta, \n                       probH = probH, seed = seed)$data\n    # then output the estimates in a list\n    standdr_est(Y = dat$Y, `T` = dat$`T`, H = dat$H)\n  }\n  \n  params &lt;- list(\"ss\" = ss)\n  mc.out &lt;- MonteCarlo::MonteCarlo(func = ms_standdr_func,\n                                   nrep = nrep, param_list = params)\n  \n  # output results in a dataframe\n  out &lt;- suppressWarnings(MonteCarlo::MakeFrame(mc.out))\n  out %&gt;%\n    pivot_longer(cols = -ss, names_to = \"estimator\", values_to = \"value\") %&gt;%\n    group_by(ss, estimator) %&gt;%\n    summarize(n = n(), \n              mean = mean(value),\n              sd = sd(value),\n              lower = quantile(value, probs = (1 - width) / 2),\n              upper = quantile(value, probs = 1 - (1 - width) / 2)) %&gt;%\n    ungroup()\n}\n\n#' Data Simulation for Doubly Robust Standardization\n#'\n#' @param n Number of individuals/observations.\n#' @param ss Number of covariates.\n#' @param alpha coefficient used to compute the distribution of \\code{`T`}.\n#' @param beta coefficient used to compute the distribution of \\code{`T`}.\n#' @param probH probability of H.\n#' @param seed Seed value. default is \\code{NULL}.\n#'\n#' @return List with a dataframe of Y, T and H and summary statitics.\n#' @export\n#'\n#' @examples\n#' \\dontrun{\n#' standdr_sim()\n#' }\nstanddr_sim &lt;- function(n = 3000, ss = 100, alpha = 0.13, beta = 20, \n                        probH = 0.05, seed = NULL) {\n  set.seed(seed)\n  \n  # matrix of independent Bernoulli vector with prob = 0.05\n  # \"The columns of H were independent indicator variables each\n  #  with probability 0.05\"\n  H &lt;- cbind(replicate(n = ss, rbinom(n = n, size = 1, prob = probH)))\n  \n  # let the treatment depend on a function of H\n  # \"We simulated T  as indicator variables with probabilities that varied as\n  # a linear function  of H such that approximately 600 individuals had T=1\"\n  sumH &lt;- apply(H, MARGIN = 1, FUN = sum) * beta / ss\n  probT &lt;- alpha * sumH + probH * rnorm(n = n, mean = 1, sd = 0.1)\n  # validate the positivity assumption\n  stopifnot(probT &gt; 0, probT &lt; 1)\n  \n  `T` &lt;- rbinom(n = n, size = 1, prob = probT)\n  \n  \n  # generate the outcome depend on T and H\n  # \"We simulated Y as a function T ans sumH such hat approximatey 35 \n  # individuals had Y = 1\"\n  probY &lt;- 0.01 * `T` + 0.01 * sumH\n  # positivity assumption is not required for the outcome\n  # see intro to chapter 6 p. 99\n  stopifnot(probY &gt;= 0, probY &lt;= 1)\n  \n  Y &lt;- rbinom(n = n, size = 1, prob = probY)\n  \n  # put the data in a data.frame\n  df &lt;- data.frame(\"Y\" = Y, \"T\" = `T`, \"H\" = H)\n  \n  # output results in a list\n  list(\n    \"stats\" = list(\"sumH\" = standdr_stats(sumH),\n                 \"probT\" = standdr_stats(probT),\n                 \"T\" = standdr_stats(`T`),\n                 \"probY\" = standdr_stats(probY),\n                 \"Y\" = standdr_stats(Y)),\n    \"data\" = list(\"Y\" = Y, \"T\" = `T`, \"H\" = H)\n    )\n}\n\n\n#' Estimates from Doubly Robust Standardization Simulation\n#'\n#' @param Y Vector of outcomes\n#' @param `T` Vector of treatments\n#' @param H Matrix of covariates\n#'\n#' @return List of estimates\n#' @export\nstanddr_est &lt;- function(Y, `T`, H) {\n  \n  # fit the exposure model\n  e &lt;- fitted(lm(`T` ~ H))\n  \n  # refit the exposure model using an incorrect logistic model\n  e2 &lt;- predict(glm(`T` ~ H, family = \"binomial\"), type = \"response\")\n  \n  # compute the weights\n  w0 &lt;- (1 - `T`) / (1 - e)\n  w1 &lt;- `T` / e\n  w02 &lt;- (1 - `T`) / (1 - e2)\n  w12 &lt;- T / e2\n  \n  # fit an overspecified (saturated) outcome model\n  mod.out &lt;- lm(Y ~ `T` * H)\n  \n  # Estimate the expected potential outcomes using the various methods\n  dat &lt;- data.frame(\"Y\" = Y, \"T\" = `T`)\n  dat0 &lt;- dat\n  dat0$`T` &lt;- 0\n  dat1 &lt;- dat\n  dat1$`T` &lt;- 1\n  \n  # the predicted data\n  preds0 &lt;- predict(mod.out, newdata = dat0)\n  preds1 &lt;- predict(mod.out, newdata = dat1)\n  \n  # calculate the estimates\n  EYT0 &lt;- mean(Y * (1 - `T`))\n  EYT1 &lt;- mean(Y * `T`)\n  EY0exp &lt;- weighted.mean(Y, w = w0)\n  EY1exp &lt;- weighted.mean(Y, w = w1)\n  EY0exp2 &lt;- weighted.mean(Y, w = w02)\n  EY1exp2 &lt;- weighted.mean(Y, w = w12)\n  EY0out &lt;- mean(preds0)\n  EY1out &lt;- mean(preds1)\n  EY0dr &lt;- mean(w0 * Y + preds0 * (`T` - e) / (1 - e))\n  EY1dr &lt;- mean(w1 * Y - preds1 * (`T` - e) / e)\n  \n  list(\"EYT0\" = EYT0,\n       \"EYT1\" = EYT1,\n       \"EY0exp\" = EY0exp,\n       \"EY1exp\" = EY1exp,\n       \"EY0exp2\" = EY0exp2,\n       \"EY1exp2\" = EY1exp2,\n       \"EY0out\" = EY0out,\n       \"EY1out\" = EY1out,\n       \"EY0dr\" = EY0dr,\n       \"EY1dr\" = EY1dr)\n}\n\n#' Compute Statistics from \\code{standdr_sim}.\n#'\n#' @param x Vector of numeric values.\n#'\n#' @return list of statistics: \\code{sun(x), mean(x), min(x), max(x)}.\n#'\n#' @examples\n#' standdr_est(runif(20))\n#' @export\nstanddr_stats &lt;- function(x) {\n  list(\"sum\" = sum(x), \"mean\" = mean(x), \"min\" = min(x), \"max\" = max(x))\n}"
  }
]